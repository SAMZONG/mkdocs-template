{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\\\s\\\\u200b\\\\-]"},"docs":[{"location":"","text":"NDX \u4ea7\u54c1\u56e2\u961f \u00b6 NDX \u4ea7\u54c1\u56e2\u961f\u59cb\u7ec8\u81f4\u529b\u4e8e\u6253\u9020\u4e16\u754c\u7ea7\u4ea7\u54c1\uff0c\u5728\u4fdd\u8bc1 DCE\u3001DSM\u3001DMP\u3001\u4e00\u4f53\u673a\u53ca\u76f8\u5173\u4ea4\u4ed8\u9879\u76ee\u5e73\u7a33\u8fd0\u8425\u7684\u57fa\u7840\u4e0a\uff0c\u5bf9\u5168\u7403\u5f00\u6e90\u4e16\u754c\u4f17\u591a\u4ea7\u54c1\u8fdb\u884c\u4e86\u5927\u91cf\u8c03\u7814\uff0c\u535a\u53d6\u4f17\u957f\u517c\u6536\u5e76\u84c4\uff0c\u4ee5\u5927\u5bb9\u5668\u548c\u5927\u5fae\u670d\u52a1\u4e3a\u57fa\u7840\u6784\u5efa\u65b0\u4e00\u4ee3\u4e91\u539f\u751f\u5bb9\u5668\u5316\u5e73\u53f0 DaoCloud Enterprise 5.0\u3002 \u4ea7\u54c1\u56e2\u961f\u662f NDX \u90e8\u95e8\u7684\u7075\u9b42\uff0c\u901a\u8fc7\u5e02\u573a\u8c03\u7814\u4e86\u89e3\u7ade\u4e89\u683c\u5c40\u548c\u5ba2\u6237\u9700\u6c42\uff0c\u8d1f\u8d23\u5b9a\u4e49\u548c\u8bbe\u8ba1\u4ea7\u54c1\uff0c\u534f\u8c03\u552e\u524d\u552e\u540e\u652f\u6301\uff0c\u786e\u4fdd\u7814\u53d1/\u552e\u524d/\u4ea4\u4ed8\u7b49\u9879\u76ee\u5e73\u7a33\u8fd0\u8f6c\u3002 \u4e0b\u56fe\u4e3a 2022 \u5e74 Q2 \u4ea7\u54c1\u56e2\u961f\u7684\u67b6\u6784\u8bbe\u8ba1\u3002","title":"NDX \u4ea7\u54c1\u56e2\u961f"},{"location":"#ndx","text":"NDX \u4ea7\u54c1\u56e2\u961f\u59cb\u7ec8\u81f4\u529b\u4e8e\u6253\u9020\u4e16\u754c\u7ea7\u4ea7\u54c1\uff0c\u5728\u4fdd\u8bc1 DCE\u3001DSM\u3001DMP\u3001\u4e00\u4f53\u673a\u53ca\u76f8\u5173\u4ea4\u4ed8\u9879\u76ee\u5e73\u7a33\u8fd0\u8425\u7684\u57fa\u7840\u4e0a\uff0c\u5bf9\u5168\u7403\u5f00\u6e90\u4e16\u754c\u4f17\u591a\u4ea7\u54c1\u8fdb\u884c\u4e86\u5927\u91cf\u8c03\u7814\uff0c\u535a\u53d6\u4f17\u957f\u517c\u6536\u5e76\u84c4\uff0c\u4ee5\u5927\u5bb9\u5668\u548c\u5927\u5fae\u670d\u52a1\u4e3a\u57fa\u7840\u6784\u5efa\u65b0\u4e00\u4ee3\u4e91\u539f\u751f\u5bb9\u5668\u5316\u5e73\u53f0 DaoCloud Enterprise 5.0\u3002 \u4ea7\u54c1\u56e2\u961f\u662f NDX \u90e8\u95e8\u7684\u7075\u9b42\uff0c\u901a\u8fc7\u5e02\u573a\u8c03\u7814\u4e86\u89e3\u7ade\u4e89\u683c\u5c40\u548c\u5ba2\u6237\u9700\u6c42\uff0c\u8d1f\u8d23\u5b9a\u4e49\u548c\u8bbe\u8ba1\u4ea7\u54c1\uff0c\u534f\u8c03\u552e\u524d\u552e\u540e\u652f\u6301\uff0c\u786e\u4fdd\u7814\u53d1/\u552e\u524d/\u4ea4\u4ed8\u7b49\u9879\u76ee\u5e73\u7a33\u8fd0\u8f6c\u3002 \u4e0b\u56fe\u4e3a 2022 \u5e74 Q2 \u4ea7\u54c1\u56e2\u961f\u7684\u67b6\u6784\u8bbe\u8ba1\u3002","title":"NDX \u4ea7\u54c1\u56e2\u961f"},{"location":"SUMMARY/","text":"Summary \u00b6 NDX \u4ea7\u54c1\u56e2\u961f \u5927\u7eb2\u76ee\u5f55 \u00b6 DCE 5.0 \u5bb9\u5668\u7ba1\u7406 \u5168\u5c40\u7ba1\u7406 ClusterPedia\uff08\u5f00\u6e90\uff09 \u53ef\u89c2\u6d4b\u6027 HwameiStor\uff08\u5f00\u6e90\uff09 \u7b2c\u4e94\u4ee3\u524d\u7aef UX \u7f51\u7edc\uff08\u5f00\u6e90\uff09 \u7f51\u7edc\u76f8\u5173 \u670d\u52a1\u7f51\u683c Merbridge\uff08\u5f00\u6e90\uff09 \u4e2d\u95f4\u4ef6 \u5fae\u670d\u52a1\u5f15\u64ce \u5e94\u7528\u5de5\u4f5c\u53f0 Kube Grid \u4ea4\u4e92\u8bbe\u8ba1 \u4ea4\u4e92\u8bbe\u8ba1\u7ec4\u4ef6\u5e93 \u53ef\u89c6\u5316\u7ec4\u4ef6\u5e93 \u5168\u6d41\u7a0b\u8bbe\u8ba1\u6837\u677f\u623f \u652f\u63f4\u539f\u578b\u843d\u5730 \u73b0\u6709\u4ea7\u54c1\u548c\u9879\u76ee DCE 4.0 \u91cd\u70b9\u9879\u76ee Insight \u91cd\u70b9\u9879\u76ee \u4e00\u4f53\u673a/\u8fb9\u7f18 \u4ea4\u4ed8\u9879\u76ee\u652f\u6301 \u552e\u524d\u9879\u76ee\u652f\u6301 \u5f00\u653e\u4ea7\u54c1\u4f53\u7cfb \u4ea7\u54c1\u6587\u6863 \u4ea7\u54c1\u4e13\u5229 \u4ea7\u54c1\u6d41\u7a0b/\u6548\u7387\u89c4\u8303\u53ca\u5de5\u5177 \u5b66\u4e60\u57f9\u8bad \u4ea7\u54c1\u56e2\u961f\u5efa\u8bbe \u4ea7\u54c1\u8c03\u7814 CCE KubeSphere Tanzu Rancher Cilium \u5b58\u50a8\u8c03\u7814 Karmada","title":"Summary"},{"location":"SUMMARY/#summary","text":"NDX \u4ea7\u54c1\u56e2\u961f","title":"Summary"},{"location":"SUMMARY/#_1","text":"DCE 5.0 \u5bb9\u5668\u7ba1\u7406 \u5168\u5c40\u7ba1\u7406 ClusterPedia\uff08\u5f00\u6e90\uff09 \u53ef\u89c2\u6d4b\u6027 HwameiStor\uff08\u5f00\u6e90\uff09 \u7b2c\u4e94\u4ee3\u524d\u7aef UX \u7f51\u7edc\uff08\u5f00\u6e90\uff09 \u7f51\u7edc\u76f8\u5173 \u670d\u52a1\u7f51\u683c Merbridge\uff08\u5f00\u6e90\uff09 \u4e2d\u95f4\u4ef6 \u5fae\u670d\u52a1\u5f15\u64ce \u5e94\u7528\u5de5\u4f5c\u53f0 Kube Grid \u4ea4\u4e92\u8bbe\u8ba1 \u4ea4\u4e92\u8bbe\u8ba1\u7ec4\u4ef6\u5e93 \u53ef\u89c6\u5316\u7ec4\u4ef6\u5e93 \u5168\u6d41\u7a0b\u8bbe\u8ba1\u6837\u677f\u623f \u652f\u63f4\u539f\u578b\u843d\u5730 \u73b0\u6709\u4ea7\u54c1\u548c\u9879\u76ee DCE 4.0 \u91cd\u70b9\u9879\u76ee Insight \u91cd\u70b9\u9879\u76ee \u4e00\u4f53\u673a/\u8fb9\u7f18 \u4ea4\u4ed8\u9879\u76ee\u652f\u6301 \u552e\u524d\u9879\u76ee\u652f\u6301 \u5f00\u653e\u4ea7\u54c1\u4f53\u7cfb \u4ea7\u54c1\u6587\u6863 \u4ea7\u54c1\u4e13\u5229 \u4ea7\u54c1\u6d41\u7a0b/\u6548\u7387\u89c4\u8303\u53ca\u5de5\u5177 \u5b66\u4e60\u57f9\u8bad \u4ea7\u54c1\u56e2\u961f\u5efa\u8bbe \u4ea7\u54c1\u8c03\u7814 CCE KubeSphere Tanzu Rancher Cilium \u5b58\u50a8\u8c03\u7814 Karmada","title":"\u5927\u7eb2\u76ee\u5f55"},{"location":"dce5.0/","text":"DaoCloud Enterprise 5.0 \u00b6 DaoCloud Enterprise 5.0 \u7b80\u79f0\u4e3a DCE 5.0\uff0c\u662f\u300cDaoCloud \u9053\u5ba2\u300d\u96c6\u767e\u5bb6\u4e4b\u957f\u94f8\u5c31\u7684\u4e0b\u4e00\u4ee3\u4e91\u539f\u751f\u5bb9\u5668\u5316\u5e73\u53f0\u3002 \u76f8\u5173\u8d44\u6599\uff1a \u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5173\u952e\u91cc\u7a0b\u7891 \u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5f00\u53d1\u5411\u5bfc \u7b2c\u4e94\u4ee3\u4ea7\u54c1\u6700\u4f73\u5b9e\u8df5","title":"DaoCloud Enterprise 5.0"},{"location":"dce5.0/#daocloud-enterprise-50","text":"DaoCloud Enterprise 5.0 \u7b80\u79f0\u4e3a DCE 5.0\uff0c\u662f\u300cDaoCloud \u9053\u5ba2\u300d\u96c6\u767e\u5bb6\u4e4b\u957f\u94f8\u5c31\u7684\u4e0b\u4e00\u4ee3\u4e91\u539f\u751f\u5bb9\u5668\u5316\u5e73\u53f0\u3002 \u76f8\u5173\u8d44\u6599\uff1a \u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5173\u952e\u91cc\u7a0b\u7891 \u7b2c\u4e94\u4ee3\u4ea7\u54c1\u5f00\u53d1\u5411\u5bfc \u7b2c\u4e94\u4ee3\u4ea7\u54c1\u6700\u4f73\u5b9e\u8df5","title":"DaoCloud Enterprise 5.0"},{"location":"dce5.0/01kpanda/","text":"\u4e91\u5bb9\u5668\u7ba1\u7406 \u00b6 \u5bb9\u5668\u7ba1\u7406 repo \u4e91\u5bb9\u5668\u7ba1\u7406 (KPanda\uff09\u662f\u57fa\u4e8e Kubernetes \u5f00\u6e90\u6280\u672f\u6784\u5efa\u7684\u9762\u5411\u4e91\u539f\u751f\u5e94\u7528\u7684\u5bb9\u5668\u7ba1\u7406\u5e73\u53f0\uff0c\u57fa\u4e8e\u539f\u751f\u591a\u4e91\u67b6\u6784\uff0c\u89e3\u8026\u5e95\u5c42\u57fa\u7840\u8bbe\u65bd\u5e73\u53f0\uff0c\u5b9e\u73b0\u591a\u4e91\u4e0e\u591a\u96c6\u7fa4\u7edf\u4e00\u5316\u7ba1\u7406\uff0c\u7b80\u5316\u4f01\u4e1a\u7684\u5e94\u7528\u4e0a\u4e91\u6d41\u7a0b\uff0c\u964d\u4f4e\u8fd0\u7ef4\u7ba1\u7406\u548c\u4eba\u529b\u6210\u672c\u3002 \u4fbf\u6377\u521b\u5efa Kubernetes \u96c6\u7fa4\uff0c\u5e2e\u52a9\u4f01\u4e1a\u5feb\u901f\u642d\u5efa\u4f01\u4e1a\u7ea7\u7684\u5bb9\u5668\u4e91\u5e73\u53f0\u3002\u4e91\u5bb9\u5668\u7ba1\u7406\u7684\u4e3b\u8981\u529f\u80fd\u5982\u4e0b\uff1a \u96c6\u7fa4\u7ba1\u7406 \u96c6\u7fa4\u7684\u7edf\u4e00\u7eb3\u7ba1\uff0c\u652f\u6301\u6240\u6709\u7279\u5b9a\u7248\u672c\u8303\u56f4\u5185\u7684\u4efb\u610f Kubernetes \u96c6\u7fa4\u7eb3\u5165\u4e91\u5bb9\u5668\u7ba1\u7406\u7ba1\u7406\u8303\u56f4\uff0c\u5b9e\u73b0\u4e91\u4e0a\u4e91\u4e0b\u591a\u4e91&\u6df7\u5408\u4e91\u5bb9\u5668\u4e91\u5e73\u53f0\u7684\u7edf\u4e00\u7ba1\u7406\u3002 \u96c6\u7fa4\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u901a\u8fc7 Web \u754c\u9762\u5feb\u901f\u90e8\u7f72\u4f01\u4e1a\u7ea7\u7684 Kubernetes \u96c6\u7fa4\uff0c\u5feb\u901f\u642d\u5efa\u4f01\u4e1a\u7ea7\u5bb9\u5668\u4e91\u53f0\uff0c\u9002\u914d\u7269\u7406\u673a\u548c\u865a\u62df\u673a\u5e95\u5c42\u73af\u5883\u3002 \u4e00\u952e\u5f0f\u96c6\u7fa4\u5347\u7ea7\uff0c\u4e00\u952e\u5347\u7ea7 Kubernetes \u7248\u672c\uff0c\u7edf\u4e00\u7ba1\u7406\u7cfb\u7edf\u7ec4\u4ef6\u5347\u7ea7\u3002 \u96c6\u7fa4\u9ad8\u53ef\u7528\uff0c\u5185\u7f6e\u96c6\u7fa4\u5bb9\u707e\u3001\u5907\u4efd\u80fd\u529b\uff0c\u4fdd\u969c\u4e1a\u52a1\u7cfb\u7edf\u5728\u4e3b\u673a\u6545\u969c\u3001\u673a\u623f\u4e2d\u65ad\u3001\u81ea\u7136\u707e\u5bb3\u7b49\u60c5\u51b5\u4e0b\u53ef\u6062\u590d\uff0c\u63d0\u9ad8\u751f\u4ea7\u73af\u5883\u7684\u7a33\u5b9a\u6027\uff0c\u964d\u4f4e\u4e1a\u52a1\u4e2d\u65ad\u98ce\u9669\u3002 \u96c6\u7fa4\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u5b9e\u73b0\u81ea\u5efa\u4e91\u539f\u751f\u96c6\u7fa4\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002 \u5f00\u653e\u5f0f API \u80fd\u529b\uff0c\u63d0\u4f9b\u539f\u751f\u7684 Kubernetes OpenAPI \u80fd\u529b\u3002 \u5e94\u7528\u7ba1\u7406 \u4e00\u7ad9\u5f0f\u90e8\u7f72\uff0c\u89e3\u8026\u5e95\u5c42 Kubernetes \u5e73\u53f0\uff0c\u4e00\u7ad9\u5f0f\u90e8\u7f72\u548c\u8fd0\u7ef4\u4e1a\u52a1\u5e94\u7528\uff0c\u5b9e\u73b0\u5e94\u7528\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002 \u5e94\u7528\u8d1f\u8f7d\u7684\u6269\u7f29\u5bb9\uff0c\u901a\u8fc7\u754c\u9762\u53ef\u5b9e\u73b0\u5e94\u7528\u8d1f\u8f7d\u7684\u624b\u52a8/\u81ea\u52a8\u6269\u7f29\u5bb9\u80fd\u529b\uff0c\u81ea\u5b9a\u4e49\u6269\u7f29\u5bb9\u7b56\u7565\uff0c\u5e94\u5bf9\u6d41\u91cf\u9ad8\u5cf0\u3002 \u5e94\u7528\u7684\u5168\u751f\u547d\u5468\u671f\uff0c\u652f\u6301\u5e94\u7528\u67e5\u770b\uff0c\u66f4\u65b0\uff0c\u5220\u9664\uff0c\u56de\u6eda\uff0c\u65f6\u95f4\u67e5\u770b\u4ee5\u53ca\u5347\u7ea7\u7b49\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002 \u8de8\u96c6\u7fa4\u8d1f\u8f7d\u7edf\u4e00\u7ba1\u7406\u80fd\u529b\u3002 \u7b56\u7565\u7ba1\u7406 \u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u5236\u5b9a\u7f51\u7edc\u7b56\u7565\u3001\u914d\u989d\u7b56\u7565\u3001\u8d44\u6e90\u9650\u5236\u7b56\u7565\u3001\u707e\u5907\u7b56\u7565\u3001\u5b89\u5168\u7b56\u7565\u3002 \u7f51\u7edc\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u5236\u5b9a\u7f51\u7edc\u7b56\u7565\uff0c\u9650\u5b9a\u5bb9\u5668\u7ec4\u4e0e\u7f51\u7edc\u5e73\u4e0a\u7f51\u7edc\u201d\u5b9e\u4f53\u201c\u901a\u4fe1\u89c4\u5219 \u914d\u989d\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u8bbe\u5b9a\u914d\u989d\u7b56\u7565\uff0c\u9650\u5236\u96c6\u7fa4\u5185\u7684\u547d\u540d\u7a7a\u95f4\u7684\u8d44\u6e90\u4f7f\u7528 \u8d44\u6e90\u9650\u5236\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u8bbe\u5b9a\u8d44\u6e90\u9650\u5236\u7b56\u7565\uff0c\u7ea6\u675f\u5bf9\u5e94\u547d\u540d\u7a7a\u95f4\u5185\u5e94\u7528\u4f7f\u7528\u8d44\u6e90\u9650\u5236\u3002 \u707e\u5907\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u8bbe\u5b9a\u707e\u5907\u7b56\u7565\uff0c\u5b9e\u73b0\u4ee5\u547d\u540d\u7a7a\u95f4\u4e3a\u7ef4\u5ea6\u8fdb\u884c\u5bb9\u707e\u5907\u4efd\uff0c\u4fdd\u969c\u96c6\u7fa4\u7684\u5b89\u5168\u6027 \u5b89\u5168\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u8bbe\u5b9a\u707e\u5907\u7b56\u7565\uff0c\u4e3a Pod \u5b9a\u4e49\u4e0d\u540c\u7684\u9694\u79bb\u7ea7\u522b\u3002 \u4ea7\u54c1\u903b\u8f91\u67b6\u6784","title":"\u4e91\u5bb9\u5668\u7ba1\u7406"},{"location":"dce5.0/01kpanda/#_1","text":"\u5bb9\u5668\u7ba1\u7406 repo \u4e91\u5bb9\u5668\u7ba1\u7406 (KPanda\uff09\u662f\u57fa\u4e8e Kubernetes \u5f00\u6e90\u6280\u672f\u6784\u5efa\u7684\u9762\u5411\u4e91\u539f\u751f\u5e94\u7528\u7684\u5bb9\u5668\u7ba1\u7406\u5e73\u53f0\uff0c\u57fa\u4e8e\u539f\u751f\u591a\u4e91\u67b6\u6784\uff0c\u89e3\u8026\u5e95\u5c42\u57fa\u7840\u8bbe\u65bd\u5e73\u53f0\uff0c\u5b9e\u73b0\u591a\u4e91\u4e0e\u591a\u96c6\u7fa4\u7edf\u4e00\u5316\u7ba1\u7406\uff0c\u7b80\u5316\u4f01\u4e1a\u7684\u5e94\u7528\u4e0a\u4e91\u6d41\u7a0b\uff0c\u964d\u4f4e\u8fd0\u7ef4\u7ba1\u7406\u548c\u4eba\u529b\u6210\u672c\u3002 \u4fbf\u6377\u521b\u5efa Kubernetes \u96c6\u7fa4\uff0c\u5e2e\u52a9\u4f01\u4e1a\u5feb\u901f\u642d\u5efa\u4f01\u4e1a\u7ea7\u7684\u5bb9\u5668\u4e91\u5e73\u53f0\u3002\u4e91\u5bb9\u5668\u7ba1\u7406\u7684\u4e3b\u8981\u529f\u80fd\u5982\u4e0b\uff1a \u96c6\u7fa4\u7ba1\u7406 \u96c6\u7fa4\u7684\u7edf\u4e00\u7eb3\u7ba1\uff0c\u652f\u6301\u6240\u6709\u7279\u5b9a\u7248\u672c\u8303\u56f4\u5185\u7684\u4efb\u610f Kubernetes \u96c6\u7fa4\u7eb3\u5165\u4e91\u5bb9\u5668\u7ba1\u7406\u7ba1\u7406\u8303\u56f4\uff0c\u5b9e\u73b0\u4e91\u4e0a\u4e91\u4e0b\u591a\u4e91&\u6df7\u5408\u4e91\u5bb9\u5668\u4e91\u5e73\u53f0\u7684\u7edf\u4e00\u7ba1\u7406\u3002 \u96c6\u7fa4\u7684\u5feb\u901f\u90e8\u7f72\uff0c\u901a\u8fc7 Web \u754c\u9762\u5feb\u901f\u90e8\u7f72\u4f01\u4e1a\u7ea7\u7684 Kubernetes \u96c6\u7fa4\uff0c\u5feb\u901f\u642d\u5efa\u4f01\u4e1a\u7ea7\u5bb9\u5668\u4e91\u53f0\uff0c\u9002\u914d\u7269\u7406\u673a\u548c\u865a\u62df\u673a\u5e95\u5c42\u73af\u5883\u3002 \u4e00\u952e\u5f0f\u96c6\u7fa4\u5347\u7ea7\uff0c\u4e00\u952e\u5347\u7ea7 Kubernetes \u7248\u672c\uff0c\u7edf\u4e00\u7ba1\u7406\u7cfb\u7edf\u7ec4\u4ef6\u5347\u7ea7\u3002 \u96c6\u7fa4\u9ad8\u53ef\u7528\uff0c\u5185\u7f6e\u96c6\u7fa4\u5bb9\u707e\u3001\u5907\u4efd\u80fd\u529b\uff0c\u4fdd\u969c\u4e1a\u52a1\u7cfb\u7edf\u5728\u4e3b\u673a\u6545\u969c\u3001\u673a\u623f\u4e2d\u65ad\u3001\u81ea\u7136\u707e\u5bb3\u7b49\u60c5\u51b5\u4e0b\u53ef\u6062\u590d\uff0c\u63d0\u9ad8\u751f\u4ea7\u73af\u5883\u7684\u7a33\u5b9a\u6027\uff0c\u964d\u4f4e\u4e1a\u52a1\u4e2d\u65ad\u98ce\u9669\u3002 \u96c6\u7fa4\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u5b9e\u73b0\u81ea\u5efa\u4e91\u539f\u751f\u96c6\u7fa4\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002 \u5f00\u653e\u5f0f API \u80fd\u529b\uff0c\u63d0\u4f9b\u539f\u751f\u7684 Kubernetes OpenAPI \u80fd\u529b\u3002 \u5e94\u7528\u7ba1\u7406 \u4e00\u7ad9\u5f0f\u90e8\u7f72\uff0c\u89e3\u8026\u5e95\u5c42 Kubernetes \u5e73\u53f0\uff0c\u4e00\u7ad9\u5f0f\u90e8\u7f72\u548c\u8fd0\u7ef4\u4e1a\u52a1\u5e94\u7528\uff0c\u5b9e\u73b0\u5e94\u7528\u7684\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002 \u5e94\u7528\u8d1f\u8f7d\u7684\u6269\u7f29\u5bb9\uff0c\u901a\u8fc7\u754c\u9762\u53ef\u5b9e\u73b0\u5e94\u7528\u8d1f\u8f7d\u7684\u624b\u52a8/\u81ea\u52a8\u6269\u7f29\u5bb9\u80fd\u529b\uff0c\u81ea\u5b9a\u4e49\u6269\u7f29\u5bb9\u7b56\u7565\uff0c\u5e94\u5bf9\u6d41\u91cf\u9ad8\u5cf0\u3002 \u5e94\u7528\u7684\u5168\u751f\u547d\u5468\u671f\uff0c\u652f\u6301\u5e94\u7528\u67e5\u770b\uff0c\u66f4\u65b0\uff0c\u5220\u9664\uff0c\u56de\u6eda\uff0c\u65f6\u95f4\u67e5\u770b\u4ee5\u53ca\u5347\u7ea7\u7b49\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\u3002 \u8de8\u96c6\u7fa4\u8d1f\u8f7d\u7edf\u4e00\u7ba1\u7406\u80fd\u529b\u3002 \u7b56\u7565\u7ba1\u7406 \u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u5236\u5b9a\u7f51\u7edc\u7b56\u7565\u3001\u914d\u989d\u7b56\u7565\u3001\u8d44\u6e90\u9650\u5236\u7b56\u7565\u3001\u707e\u5907\u7b56\u7565\u3001\u5b89\u5168\u7b56\u7565\u3002 \u7f51\u7edc\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u5236\u5b9a\u7f51\u7edc\u7b56\u7565\uff0c\u9650\u5b9a\u5bb9\u5668\u7ec4\u4e0e\u7f51\u7edc\u5e73\u4e0a\u7f51\u7edc\u201d\u5b9e\u4f53\u201c\u901a\u4fe1\u89c4\u5219 \u914d\u989d\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u8bbe\u5b9a\u914d\u989d\u7b56\u7565\uff0c\u9650\u5236\u96c6\u7fa4\u5185\u7684\u547d\u540d\u7a7a\u95f4\u7684\u8d44\u6e90\u4f7f\u7528 \u8d44\u6e90\u9650\u5236\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u8bbe\u5b9a\u8d44\u6e90\u9650\u5236\u7b56\u7565\uff0c\u7ea6\u675f\u5bf9\u5e94\u547d\u540d\u7a7a\u95f4\u5185\u5e94\u7528\u4f7f\u7528\u8d44\u6e90\u9650\u5236\u3002 \u707e\u5907\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u8bbe\u5b9a\u707e\u5907\u7b56\u7565\uff0c\u5b9e\u73b0\u4ee5\u547d\u540d\u7a7a\u95f4\u4e3a\u7ef4\u5ea6\u8fdb\u884c\u5bb9\u707e\u5907\u4efd\uff0c\u4fdd\u969c\u96c6\u7fa4\u7684\u5b89\u5168\u6027 \u5b89\u5168\u7b56\u7565\uff0c\u652f\u6301\u4ee5\u547d\u540d\u7a7a\u95f4\u6216\u96c6\u7fa4\u7c92\u5ea6\u8bbe\u5b9a\u707e\u5907\u7b56\u7565\uff0c\u4e3a Pod \u5b9a\u4e49\u4e0d\u540c\u7684\u9694\u79bb\u7ea7\u522b\u3002 \u4ea7\u54c1\u903b\u8f91\u67b6\u6784","title":"\u4e91\u5bb9\u5668\u7ba1\u7406"},{"location":"dce5.0/02ghippo/","text":"\u5168\u5c40\u7ba1\u7406 \u00b6 \u5168\u5c40\u7ba1\u7406 repo \u5168\u5c40\u7ba1\u7406\u662f\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u7efc\u5408\u6027\u670d\u52a1\u6a21\u5757\uff0c\u5305\u542b\u7528\u6237\u4e0e\u8bbf\u95ee\u63a7\u5236\u3001\u4f01\u4e1a\u7a7a\u95f4\u3001\u5ba1\u8ba1\u65e5\u5fd7\u3001\u5e73\u53f0\u8bbe\u7f6e\u7b49\u57fa\u7840\u670d\u52a1\u529f\u80fd\u3002 \u7528\u6237\u4e0e\u8bbf\u95ee\u63a7\u5236 \u5e2e\u52a9\u7528\u6237\u5b89\u5168\u5730\u7ba1\u7406\u8d44\u6e90\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u901a\u8fc7\u7528\u6237\u4e0e\u8bbf\u95ee\u63a7\u5236\u521b\u5efa\u3001\u7ba1\u7406\u3001\u5220\u9664\u7528\u6237/\u7528\u6237\u7ec4\uff0c\u5e76\u7075\u6d3b\u914d\u7f6e\u7528\u6237/\u7528\u6237\u7ec4\u6743\u9650\uff0c\u5b8c\u6210\u7528\u6237\u804c\u80fd\u6743\u9650\u7684\u5212\u5206\u3002 \u4f01\u4e1a\u7a7a\u95f4 \u8fd9\u662f\u5177\u6709\u5c42\u7ea7\u7ed3\u6784\u548c\u8bbf\u95ee\u6743\u9650\u63a7\u5236\u7684\u8d44\u6e90\u9694\u79bb\u5355\u5143\uff0c\u7528\u6237\u53ef\u4ee5\u6309\u7167\u4f01\u4e1a\u5f00\u53d1\u73af\u5883\u3001\u90e8\u95e8\u7ed3\u6784\u7b49\u8bbe\u7f6e\u5c42\u7ea7\u7ed3\u6784\uff0c\u5e76\u63a7\u5236\u54ea\u4e9b\u4eba\u5bf9\u54ea\u4e9b\u8d44\u6e90\u5177\u6709\u8bbf\u95ee\u6743\u9650\u3002 \u5ba1\u8ba1\u65e5\u5fd7 \u63d0\u4f9b\u8d44\u6e90\u7684\u64cd\u4f5c\u8bb0\u5f55\uff0c\u5e2e\u52a9\u5feb\u901f\u5b9e\u73b0\u5b89\u5168\u5206\u6790\u3001\u8d44\u6e90\u53d8\u66f4\u3001\u95ee\u9898\u5b9a\u4f4d\u7b49\u3002 \u5e73\u53f0\u8bbe\u7f6e \u901a\u8fc7\u5e73\u53f0\u5b89\u5168\u7b56\u7565\u3001\u90ae\u4ef6\u670d\u52a1\u5668\u3001\u5916\u89c2\u5b9a\u5236\u7b49\u63d0\u9ad8\u7528\u6237\u4fe1\u606f\u7684\u5b89\u5168\u6027\u548c\u5e73\u53f0\u7684\u4e2a\u6027\u5316\u3002 \u529f\u80fd\u6846\u67b6 \u00b6 \u4ee5\u4e0b\u4e3a\u5168\u5c40\u7ba1\u7406\u7684\u529f\u80fd\u6846\u67b6\u56fe\u3002","title":"\u5168\u5c40\u7ba1\u7406"},{"location":"dce5.0/02ghippo/#_1","text":"\u5168\u5c40\u7ba1\u7406 repo \u5168\u5c40\u7ba1\u7406\u662f\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u7efc\u5408\u6027\u670d\u52a1\u6a21\u5757\uff0c\u5305\u542b\u7528\u6237\u4e0e\u8bbf\u95ee\u63a7\u5236\u3001\u4f01\u4e1a\u7a7a\u95f4\u3001\u5ba1\u8ba1\u65e5\u5fd7\u3001\u5e73\u53f0\u8bbe\u7f6e\u7b49\u57fa\u7840\u670d\u52a1\u529f\u80fd\u3002 \u7528\u6237\u4e0e\u8bbf\u95ee\u63a7\u5236 \u5e2e\u52a9\u7528\u6237\u5b89\u5168\u5730\u7ba1\u7406\u8d44\u6e90\u7684\u8bbf\u95ee\u6743\u9650\uff0c\u901a\u8fc7\u7528\u6237\u4e0e\u8bbf\u95ee\u63a7\u5236\u521b\u5efa\u3001\u7ba1\u7406\u3001\u5220\u9664\u7528\u6237/\u7528\u6237\u7ec4\uff0c\u5e76\u7075\u6d3b\u914d\u7f6e\u7528\u6237/\u7528\u6237\u7ec4\u6743\u9650\uff0c\u5b8c\u6210\u7528\u6237\u804c\u80fd\u6743\u9650\u7684\u5212\u5206\u3002 \u4f01\u4e1a\u7a7a\u95f4 \u8fd9\u662f\u5177\u6709\u5c42\u7ea7\u7ed3\u6784\u548c\u8bbf\u95ee\u6743\u9650\u63a7\u5236\u7684\u8d44\u6e90\u9694\u79bb\u5355\u5143\uff0c\u7528\u6237\u53ef\u4ee5\u6309\u7167\u4f01\u4e1a\u5f00\u53d1\u73af\u5883\u3001\u90e8\u95e8\u7ed3\u6784\u7b49\u8bbe\u7f6e\u5c42\u7ea7\u7ed3\u6784\uff0c\u5e76\u63a7\u5236\u54ea\u4e9b\u4eba\u5bf9\u54ea\u4e9b\u8d44\u6e90\u5177\u6709\u8bbf\u95ee\u6743\u9650\u3002 \u5ba1\u8ba1\u65e5\u5fd7 \u63d0\u4f9b\u8d44\u6e90\u7684\u64cd\u4f5c\u8bb0\u5f55\uff0c\u5e2e\u52a9\u5feb\u901f\u5b9e\u73b0\u5b89\u5168\u5206\u6790\u3001\u8d44\u6e90\u53d8\u66f4\u3001\u95ee\u9898\u5b9a\u4f4d\u7b49\u3002 \u5e73\u53f0\u8bbe\u7f6e \u901a\u8fc7\u5e73\u53f0\u5b89\u5168\u7b56\u7565\u3001\u90ae\u4ef6\u670d\u52a1\u5668\u3001\u5916\u89c2\u5b9a\u5236\u7b49\u63d0\u9ad8\u7528\u6237\u4fe1\u606f\u7684\u5b89\u5168\u6027\u548c\u5e73\u53f0\u7684\u4e2a\u6027\u5316\u3002","title":"\u5168\u5c40\u7ba1\u7406"},{"location":"dce5.0/02ghippo/#_2","text":"\u4ee5\u4e0b\u4e3a\u5168\u5c40\u7ba1\u7406\u7684\u529f\u80fd\u6846\u67b6\u56fe\u3002","title":"\u529f\u80fd\u6846\u67b6"},{"location":"dce5.0/03clusterpedia/","text":"\u96c6\u7fa4\u8d44\u6e90\u68c0\u7d22 ClusterPedia \u00b6 clusterpedia \u5b98\u7f51 clusterpedi repo Clusterpedia \u8fd9\u4e2a\u540d\u79f0\u501f\u9274\u81ea Wikipedia\uff0c\u662f\u591a\u96c6\u7fa4\u7684\u767e\u79d1\u5168\u4e66\uff0c\u5176\u6838\u5fc3\u7406\u5ff5\u662f\u6536\u96c6\u3001\u68c0\u7d22\u548c\u7b80\u5355\u63a7\u5236\u591a\u96c6\u7fa4\u8d44\u6e90\u3002 \u901a\u8fc7\u805a\u5408\u6536\u96c6\u591a\u96c6\u7fa4\u8d44\u6e90\uff0c\u5728\u517c\u5bb9 Kubernetes OpenAPI \u7684\u57fa\u7840\u4e0a\u989d\u5916\u63d0\u4f9b\u66f4\u52a0\u5f3a\u5927\u7684\u68c0\u7d22\u529f\u80fd\uff0c\u8ba9\u7528\u6237\u66f4\u65b9\u4fbf\u5feb\u6377\u5730\u5728\u591a\u96c6\u7fa4\u4e2d\u83b7\u53d6\u60f3\u8981\u7684\u4efb\u4f55\u8d44\u6e90\u3002 \u652f\u6301\u7684\u529f\u80fd\u5305\u62ec\uff1a \u652f\u6301\u590d\u6742\u7684\u68c0\u7d22\u6761\u4ef6\u3001\u8fc7\u6ee4\u6761\u4ef6\u3001\u6392\u5e8f\u3001\u5206\u9875\u7b49\u7b49 \u652f\u6301\u67e5\u8be2\u8d44\u6e90\u65f6\u8bf7\u6c42\u9644\u5e26\u5173\u7cfb\u8d44\u6e90 \u7edf\u4e00\u4e3b\u96c6\u7fa4\u548c\u591a\u96c6\u7fa4\u8d44\u6e90\u68c0\u7d22\u5165\u53e3 \u517c\u5bb9 kubernetes OpenAPI\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 kubectl \u8fdb\u884c\u591a\u96c6\u7fa4\u68c0\u7d22\uff0c\u800c\u65e0\u9700\u7b2c\u4e09\u65b9\u63d2\u4ef6\u6216\u8005\u5de5\u5177 \u517c\u5bb9\u6536\u96c6\u4e0d\u540c\u7248\u672c\u7684\u96c6\u7fa4\u8d44\u6e90\uff0c\u4e0d\u53d7\u4e3b\u96c6\u7fa4\u7248\u672c\u7ea6\u675f \u8d44\u6e90\u6536\u96c6\u9ad8\u6027\u80fd\uff0c\u4f4e\u5185\u5b58 \u6839\u636e\u96c6\u7fa4\u5f53\u524d\u7684\u5065\u5eb7\u72b6\u6001\uff0c\u81ea\u52a8\u5f00\u59cb/\u505c\u6b62\u8d44\u6e90\u6536\u96c6 \u63d2\u4ef6\u5316\u5b58\u50a8\u5c42\uff0c\u7528\u6237\u53ef\u4ee5\u6839\u636e\u81ea\u5df1\u9700\u6c42\u4f7f\u7528\u5176\u4ed6\u5b58\u50a8\u7ec4\u4ef6\u81ea\u5b9a\u4e49\u5b58\u50a8\u5c42 \u9ad8\u53ef\u7528","title":"\u96c6\u7fa4\u8d44\u6e90\u68c0\u7d22 ClusterPedia"},{"location":"dce5.0/03clusterpedia/#clusterpedia","text":"clusterpedia \u5b98\u7f51 clusterpedi repo Clusterpedia \u8fd9\u4e2a\u540d\u79f0\u501f\u9274\u81ea Wikipedia\uff0c\u662f\u591a\u96c6\u7fa4\u7684\u767e\u79d1\u5168\u4e66\uff0c\u5176\u6838\u5fc3\u7406\u5ff5\u662f\u6536\u96c6\u3001\u68c0\u7d22\u548c\u7b80\u5355\u63a7\u5236\u591a\u96c6\u7fa4\u8d44\u6e90\u3002 \u901a\u8fc7\u805a\u5408\u6536\u96c6\u591a\u96c6\u7fa4\u8d44\u6e90\uff0c\u5728\u517c\u5bb9 Kubernetes OpenAPI \u7684\u57fa\u7840\u4e0a\u989d\u5916\u63d0\u4f9b\u66f4\u52a0\u5f3a\u5927\u7684\u68c0\u7d22\u529f\u80fd\uff0c\u8ba9\u7528\u6237\u66f4\u65b9\u4fbf\u5feb\u6377\u5730\u5728\u591a\u96c6\u7fa4\u4e2d\u83b7\u53d6\u60f3\u8981\u7684\u4efb\u4f55\u8d44\u6e90\u3002 \u652f\u6301\u7684\u529f\u80fd\u5305\u62ec\uff1a \u652f\u6301\u590d\u6742\u7684\u68c0\u7d22\u6761\u4ef6\u3001\u8fc7\u6ee4\u6761\u4ef6\u3001\u6392\u5e8f\u3001\u5206\u9875\u7b49\u7b49 \u652f\u6301\u67e5\u8be2\u8d44\u6e90\u65f6\u8bf7\u6c42\u9644\u5e26\u5173\u7cfb\u8d44\u6e90 \u7edf\u4e00\u4e3b\u96c6\u7fa4\u548c\u591a\u96c6\u7fa4\u8d44\u6e90\u68c0\u7d22\u5165\u53e3 \u517c\u5bb9 kubernetes OpenAPI\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 kubectl \u8fdb\u884c\u591a\u96c6\u7fa4\u68c0\u7d22\uff0c\u800c\u65e0\u9700\u7b2c\u4e09\u65b9\u63d2\u4ef6\u6216\u8005\u5de5\u5177 \u517c\u5bb9\u6536\u96c6\u4e0d\u540c\u7248\u672c\u7684\u96c6\u7fa4\u8d44\u6e90\uff0c\u4e0d\u53d7\u4e3b\u96c6\u7fa4\u7248\u672c\u7ea6\u675f \u8d44\u6e90\u6536\u96c6\u9ad8\u6027\u80fd\uff0c\u4f4e\u5185\u5b58 \u6839\u636e\u96c6\u7fa4\u5f53\u524d\u7684\u5065\u5eb7\u72b6\u6001\uff0c\u81ea\u52a8\u5f00\u59cb/\u505c\u6b62\u8d44\u6e90\u6536\u96c6 \u63d2\u4ef6\u5316\u5b58\u50a8\u5c42\uff0c\u7528\u6237\u53ef\u4ee5\u6839\u636e\u81ea\u5df1\u9700\u6c42\u4f7f\u7528\u5176\u4ed6\u5b58\u50a8\u7ec4\u4ef6\u81ea\u5b9a\u4e49\u5b58\u50a8\u5c42 \u9ad8\u53ef\u7528","title":"\u96c6\u7fa4\u8d44\u6e90\u68c0\u7d22 ClusterPedia"},{"location":"dce5.0/04insight/","text":"\u53ef\u89c2\u6d4b\u6027 Insight \u00b6","title":"\u53ef\u89c2\u6d4b\u6027 Insight"},{"location":"dce5.0/04insight/#insight","text":"","title":"\u53ef\u89c2\u6d4b\u6027 Insight"},{"location":"dce5.0/05hwameistor/","text":"\u672c\u5730\u5b58\u50a8 HwameiStor \u00b6","title":"\u672c\u5730\u5b58\u50a8 HwameiStor"},{"location":"dce5.0/05hwameistor/#hwameistor","text":"","title":"\u672c\u5730\u5b58\u50a8 HwameiStor"},{"location":"dce5.0/06frontend-ux/","text":"\u524d\u7aef\u8bbe\u8ba1 UX \u00b6","title":"\u524d\u7aef\u8bbe\u8ba1 UX"},{"location":"dce5.0/06frontend-ux/#ux","text":"","title":"\u524d\u7aef\u8bbe\u8ba1 UX"},{"location":"dce5.0/07spiderpool/","text":"\u7f51\u7edc\u5f00\u6e90 \u00b6","title":"\u7f51\u7edc\u5f00\u6e90"},{"location":"dce5.0/07spiderpool/#_1","text":"","title":"\u7f51\u7edc\u5f00\u6e90"},{"location":"dce5.0/08spiderflat/","text":"\u7f51\u7edc\u76f8\u5173 \u00b6","title":"\u7f51\u7edc\u76f8\u5173"},{"location":"dce5.0/08spiderflat/#_1","text":"","title":"\u7f51\u7edc\u76f8\u5173"},{"location":"dce5.0/09mspider/","text":"\u670d\u52a1\u7f51\u683c \u00b6","title":"\u670d\u52a1\u7f51\u683c"},{"location":"dce5.0/09mspider/#_1","text":"","title":"\u670d\u52a1\u7f51\u683c"},{"location":"dce5.0/10merbridge/","text":"Merbridge \u00b6 \u98ce\u53e3\u4e4b\u4e0b\uff0c\u5189\u5189\u4e0a\u5347\u7684\u660e\u661f\u7cbe\u5de7\u9879\u76ee\u3002","title":"Merbridge"},{"location":"dce5.0/10merbridge/#merbridge","text":"\u98ce\u53e3\u4e4b\u4e0b\uff0c\u5189\u5189\u4e0a\u5347\u7684\u660e\u661f\u7cbe\u5de7\u9879\u76ee\u3002","title":"Merbridge"},{"location":"dce5.0/11midware/","text":"\u4e2d\u95f4\u4ef6 \u00b6","title":"\u4e2d\u95f4\u4ef6"},{"location":"dce5.0/11midware/#_1","text":"","title":"\u4e2d\u95f4\u4ef6"},{"location":"dce5.0/12skoala/","text":"\u5fae\u670d\u52a1\u5f15\u64ce \u00b6","title":"\u5fae\u670d\u52a1\u5f15\u64ce"},{"location":"dce5.0/12skoala/#_1","text":"","title":"\u5fae\u670d\u52a1\u5f15\u64ce"},{"location":"dce5.0/13appws/","text":"\u5e94\u7528\u5de5\u4f5c\u53f0 \u00b6","title":"\u5e94\u7528\u5de5\u4f5c\u53f0"},{"location":"dce5.0/13appws/#_1","text":"","title":"\u5e94\u7528\u5de5\u4f5c\u53f0"},{"location":"dce5.0/14kubegrid/","text":"Kube Grid \u00b6","title":"Kube Grid"},{"location":"dce5.0/14kubegrid/#kube-grid","text":"","title":"Kube Grid"},{"location":"design/","text":"\u4ea4\u4e92\u8bbe\u8ba1 \u00b6","title":"\u4ea4\u4e92\u8bbe\u8ba1"},{"location":"design/#_1","text":"","title":"\u4ea4\u4e92\u8bbe\u8ba1"},{"location":"design/interactive/","text":"\u4ea4\u4e92\u8bbe\u8ba1\u7ec4\u4ef6\u5e93 \u00b6","title":"\u4ea4\u4e92\u8bbe\u8ba1\u7ec4\u4ef6\u5e93"},{"location":"design/interactive/#_1","text":"","title":"\u4ea4\u4e92\u8bbe\u8ba1\u7ec4\u4ef6\u5e93"},{"location":"design/protype/","text":"\u652f\u63f4\u539f\u578b\u843d\u5730 \u00b6","title":"\u652f\u63f4\u539f\u578b\u843d\u5730"},{"location":"design/protype/#_1","text":"","title":"\u652f\u63f4\u539f\u578b\u843d\u5730"},{"location":"design/sample/","text":"\u8bbe\u8ba1\u6837\u677f\u623f \u00b6","title":"\u8bbe\u8ba1\u6837\u677f\u623f"},{"location":"design/sample/#_1","text":"","title":"\u8bbe\u8ba1\u6837\u677f\u623f"},{"location":"design/visible/","text":"\u53ef\u89c6\u5316\u7ec4\u4ef6\u5e93 \u00b6","title":"\u53ef\u89c6\u5316\u7ec4\u4ef6\u5e93"},{"location":"design/visible/#_1","text":"","title":"\u53ef\u89c6\u5316\u7ec4\u4ef6\u5e93"},{"location":"products/","text":"\u73b0\u6709\u4ea7\u54c1\u548c\u9879\u76ee \u00b6 \u534f\u8c03\u5185\u5916\u8d44\u6e90\uff0c\u786e\u4fdd\u73b0\u6709\u552e\u524d/\u4ea4\u4ed8\u9879\u76ee\u6b63\u5e38\u8fd0\u8f6c\u3002","title":"\u73b0\u6709\u4ea7\u54c1\u548c\u9879\u76ee"},{"location":"products/#_1","text":"\u534f\u8c03\u5185\u5916\u8d44\u6e90\uff0c\u786e\u4fdd\u73b0\u6709\u552e\u524d/\u4ea4\u4ed8\u9879\u76ee\u6b63\u5e38\u8fd0\u8f6c\u3002","title":"\u73b0\u6709\u4ea7\u54c1\u548c\u9879\u76ee"},{"location":"products/dce4.0/","text":"DCE 4.0 \u00b6 DCE 4.0 \u662f\u9886\u5148\u7684\u4e91\u539f\u751f\u5e94\u7528\u4e91\u5e73\u53f0\uff0c\u65e8\u5728\u52a9\u529b\u4f01\u4e1a\u5b8c\u6210\u65b0\u4e00\u4ee3\u4e92\u8054\u7f51\u6280\u672f\u9a71\u52a8\u4e0b\u7684\u6570\u5b57\u5316\u8f6c\u578b\uff0c\u5b9e\u73b0\u5168\u9762\u8f6f\u4ef6\u5b9a\u4e49\u7684\u6570\u636e\u4e2d\u5fc3\uff0c\u52a0\u901f\u4e1a\u52a1\u7684\u8fed\u4ee3\u4ea4\u4ed8\uff0c\u6ee1\u8db3\u4f01\u4e1a\u5feb\u901f\u53d8\u5316\u7684\u4e1a\u52a1\u9700\u6c42\u3002\u901a\u8fc7 DaoCloud Enterprise\uff0c\u4f01\u4e1a\u53ef\u5728\u5df2\u6709 IT \u57fa\u7840\u67b6\u6784\u4e4b\u4e0a\u5b9e\u73b0\u300cDevOps\u300d\u5f00\u53d1\u8fd0\u7ef4\u6a21\u5f0f\uff0c\u6807\u51c6\u5316\u5e94\u7528\u4ea4\u4ed8\u4e0e\u6d41\u7a0b\u5316\u8fd0\u7ef4\u7ba1\u63a7\uff0c\u83b7\u5f97\u5b89\u5168\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8fd0\u7ef4\u80fd\u529b\u3002\u4ee5\u9762\u5411\u4e92\u8054\u7f51\u300c\u654f\u6001 IT\u300d\u7684\u901f\u5ea6\u3001\u89c4\u6a21\u548c\u53ef\u9760\u6027\uff0c\u7ba1\u7406\u65e5\u65b0\u6708\u5f02\u7684\u73b0\u4ee3\u5316\u4f01\u4e1a\u5e94\u7528\u548c\u8f6f\u4ef6\u5b9a\u4e49\u7684\u6570\u636e\u4e2d\u5fc3\u3002 \u8be6\u60c5\u53c2\u9605\u300a DCE 4.0 \u4ea7\u54c1\u6587\u6863 \u300b","title":"DCE 4.0"},{"location":"products/dce4.0/#dce-40","text":"DCE 4.0 \u662f\u9886\u5148\u7684\u4e91\u539f\u751f\u5e94\u7528\u4e91\u5e73\u53f0\uff0c\u65e8\u5728\u52a9\u529b\u4f01\u4e1a\u5b8c\u6210\u65b0\u4e00\u4ee3\u4e92\u8054\u7f51\u6280\u672f\u9a71\u52a8\u4e0b\u7684\u6570\u5b57\u5316\u8f6c\u578b\uff0c\u5b9e\u73b0\u5168\u9762\u8f6f\u4ef6\u5b9a\u4e49\u7684\u6570\u636e\u4e2d\u5fc3\uff0c\u52a0\u901f\u4e1a\u52a1\u7684\u8fed\u4ee3\u4ea4\u4ed8\uff0c\u6ee1\u8db3\u4f01\u4e1a\u5feb\u901f\u53d8\u5316\u7684\u4e1a\u52a1\u9700\u6c42\u3002\u901a\u8fc7 DaoCloud Enterprise\uff0c\u4f01\u4e1a\u53ef\u5728\u5df2\u6709 IT \u57fa\u7840\u67b6\u6784\u4e4b\u4e0a\u5b9e\u73b0\u300cDevOps\u300d\u5f00\u53d1\u8fd0\u7ef4\u6a21\u5f0f\uff0c\u6807\u51c6\u5316\u5e94\u7528\u4ea4\u4ed8\u4e0e\u6d41\u7a0b\u5316\u8fd0\u7ef4\u7ba1\u63a7\uff0c\u83b7\u5f97\u5b89\u5168\u53ef\u9760\u7684\u81ea\u52a8\u5316\u8fd0\u7ef4\u80fd\u529b\u3002\u4ee5\u9762\u5411\u4e92\u8054\u7f51\u300c\u654f\u6001 IT\u300d\u7684\u901f\u5ea6\u3001\u89c4\u6a21\u548c\u53ef\u9760\u6027\uff0c\u7ba1\u7406\u65e5\u65b0\u6708\u5f02\u7684\u73b0\u4ee3\u5316\u4f01\u4e1a\u5e94\u7528\u548c\u8f6f\u4ef6\u5b9a\u4e49\u7684\u6570\u636e\u4e2d\u5fc3\u3002 \u8be6\u60c5\u53c2\u9605\u300a DCE 4.0 \u4ea7\u54c1\u6587\u6863 \u300b","title":"DCE 4.0"},{"location":"products/dcex/","text":"\u4e91\u539f\u751f\u4e00\u4f53\u673a \u00b6 \u4e91\u539f\u751f\u4e00\u4f53\u673a\u662f DaoCloud \u5168\u65b0\u63a8\u51fa\u4ee5\u5e94\u7528\u670d\u52a1\u4e3a\u4e2d\u5fc3\u7684\u4e00\u7ad9\u5f0f\u5bb9\u5668\u5e94\u7528\u4e91\u89e3\u51b3\u65b9\u6848\uff0c\u91c7\u7528\u667a\u80fd\u88f8\u91d1\u5c5e\u67b6\u6784 + \u5bb9\u5668\u5316\u5e73\u53f0\uff0c\u65e0\u865a\u62df\u5316\u635f\u8017\uff0c\u5c06\u8ba1\u7b97\u3001\u5b58\u50a8\u3001\u7f51\u7edc\u8d44\u6e90\u6c60\u5316\u540e\u5b9e\u73b0\u8f6f\u786c\u4ef6\u534f\u540c\u8c03\u5ea6\uff0c\u8f7b\u677e\u6a2a\u5411\u6269\u5c55\u3001\u5f39\u6027\u6269\u5bb9\uff0c\u517c\u5bb9\u73b0\u6709\u4f01\u4e1a\u7f51\u7edc\u89c4\u5212\uff0c\u80fd\u65e0\u7f1d\u63a5\u5165 VLAN\u3001SDN \u7f51\u7edc\uff0c\u7eb3\u7ba1\u73b0\u5b58\u8bbe\u5907\u642d\u5efa\u73b0\u4ee3\u5316\u6570\u636e\u4e2d\u5fc3\u3002 \u8be6\u60c5\u53c2\u9605\u300a \u4e91\u539f\u751f\u4e00\u4f53\u673a\u7528\u6237\u624b\u518c \u300b","title":"\u4e91\u539f\u751f\u4e00\u4f53\u673a"},{"location":"products/dcex/#_1","text":"\u4e91\u539f\u751f\u4e00\u4f53\u673a\u662f DaoCloud \u5168\u65b0\u63a8\u51fa\u4ee5\u5e94\u7528\u670d\u52a1\u4e3a\u4e2d\u5fc3\u7684\u4e00\u7ad9\u5f0f\u5bb9\u5668\u5e94\u7528\u4e91\u89e3\u51b3\u65b9\u6848\uff0c\u91c7\u7528\u667a\u80fd\u88f8\u91d1\u5c5e\u67b6\u6784 + \u5bb9\u5668\u5316\u5e73\u53f0\uff0c\u65e0\u865a\u62df\u5316\u635f\u8017\uff0c\u5c06\u8ba1\u7b97\u3001\u5b58\u50a8\u3001\u7f51\u7edc\u8d44\u6e90\u6c60\u5316\u540e\u5b9e\u73b0\u8f6f\u786c\u4ef6\u534f\u540c\u8c03\u5ea6\uff0c\u8f7b\u677e\u6a2a\u5411\u6269\u5c55\u3001\u5f39\u6027\u6269\u5bb9\uff0c\u517c\u5bb9\u73b0\u6709\u4f01\u4e1a\u7f51\u7edc\u89c4\u5212\uff0c\u80fd\u65e0\u7f1d\u63a5\u5165 VLAN\u3001SDN \u7f51\u7edc\uff0c\u7eb3\u7ba1\u73b0\u5b58\u8bbe\u5907\u642d\u5efa\u73b0\u4ee3\u5316\u6570\u636e\u4e2d\u5fc3\u3002 \u8be6\u60c5\u53c2\u9605\u300a \u4e91\u539f\u751f\u4e00\u4f53\u673a\u7528\u6237\u624b\u518c \u300b","title":"\u4e91\u539f\u751f\u4e00\u4f53\u673a"},{"location":"products/delivery/","text":"\u4ea4\u4ed8\u9879\u76ee\u652f\u6301 \u00b6","title":"\u4ea4\u4ed8\u9879\u76ee\u652f\u6301"},{"location":"products/delivery/#_1","text":"","title":"\u4ea4\u4ed8\u9879\u76ee\u652f\u6301"},{"location":"products/insight/","text":"Insight \u91cd\u70b9\u9879\u76ee\u652f\u6301 \u00b6","title":"Insight \u91cd\u70b9\u9879\u76ee\u652f\u6301"},{"location":"products/insight/#insight","text":"","title":"Insight \u91cd\u70b9\u9879\u76ee\u652f\u6301"},{"location":"products/pre-sales/","text":"\u552e\u524d\u9879\u76ee\u652f\u6301 \u00b6","title":"\u552e\u524d\u9879\u76ee\u652f\u6301"},{"location":"products/pre-sales/#_1","text":"","title":"\u552e\u524d\u9879\u76ee\u652f\u6301"},{"location":"scaffolds/tags/","text":"Tags \u00b6 Following is a list of relevant tags: HowTo \u00b6 \u6587\u6863\u7ad9\u4f7f\u7528\u8bf4\u660e","title":"Tags"},{"location":"scaffolds/tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"scaffolds/tags/#howto","text":"\u6587\u6863\u7ad9\u4f7f\u7528\u8bf4\u660e","title":"HowTo"},{"location":"support/","text":"\u6587\u6863\u53ca\u57f9\u8bad \u00b6 \u81f4\u529b\u4e8e\u6784\u5efa\u5f00\u653e\u4ea7\u54c1\u4f53\u7cfb\uff0c\u63d0\u9ad8\u4ea7\u54c1\u56e2\u961f\u8f6f\u5b9e\u529b\u3002","title":"\u6587\u6863\u53ca\u57f9\u8bad"},{"location":"support/#_1","text":"\u81f4\u529b\u4e8e\u6784\u5efa\u5f00\u653e\u4ea7\u54c1\u4f53\u7cfb\uff0c\u63d0\u9ad8\u4ea7\u54c1\u56e2\u961f\u8f6f\u5b9e\u529b\u3002","title":"\u6587\u6863\u53ca\u57f9\u8bad"},{"location":"support/01-mkdocs-material/","tags":["HowTo"],"text":"\u6587\u6863\u7ad9\u4f7f\u7528\u8bf4\u660e \u00b6 \u672c\u4ea7\u54c1\u6587\u6863\u7ad9\u91c7\u7528\u4e86 Mkdocs \u5f00\u53d1\u7684 Material \u98ce\u683c \u5efa\u7ad9 \u652f\u6301\u6587\u6863\u8bed\u6cd5\u4e3a Markdown \u6587\u6863\u7ad9\u7ed3\u6784 \u00b6 Important \u6587\u6863\u5185\u5bb9\u5168\u90e8\u5b58\u653e\u5728 docs/ \u76ee\u5f55\u4e0b\uff0c\u4fee\u6539\u5185\u5bb9\u4ec5\u7f16\u8f91 docs/ \u76ee\u5f55\u4e0b\u7684\u6587\u4ef6\u5373\u53ef docs \u251c\u2500\u2500 .pages.yaml # \u7ad9\u70b9\u9876\u90e8\u5bfc\u822a\u914d\u7f6e\u6587\u4ef6\uff0c\u63a7\u5236\u987a\u5e8f\u548c\u540d\u79f0\uff0c\u4e00\u822c\u4e0d\u589e\u52a0 \u251c\u2500\u2500 README.md # \u9ed8\u8ba4\u60c5\u51b5\u4e0b \u76ee\u5f55\u4e0b README.md \u4f5c\u4e3a default \u9875\u9762 \u251c\u2500\u2500 SUMMARY.md \u251c\u2500\u2500 dce5.0 # \u5b50\u6587\u4ef6\u5939\uff0c\u652f\u6301\u591a\u7ea7\u76ee\u5f55\uff0c\u81ea\u52a8\u68c0\u6d4b\u914d\u7f6e \u2502 \u251c\u2500\u2500 01kpanda.md # \u5b50\u6587\u4ef6\u81ea\u52a8\u68c0\u6d4b\uff0c\u53ef\u4ee5\u901a\u8fc7\u6587\u4ef6\u540d\u524d\u7f00\u6570\u5b57\u63a7\u5236\u6392\u5e8f \u2502 \u251c\u2500\u2500 02ghippo.md \u2502 \u251c\u2500\u2500 03clusterpedia.md \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 design \u2502 \u251c\u2500\u2500 .pages.yaml # \u53ef\u4ee5\u4e0d\u521b\u5efa \u5f53\u9700\u8981\u5bf9\u4e00\u4e2a\u76ee\u5f55\u8fdb\u884c\u7279\u6b8a\u914d\u7f6e\u65f6\u5f15\u5165.pages.yaml \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 images # \u56fe\u7247\u6587\u4ef6\u5939\uff0c\u4f7f\u7528\u76f8\u5bf9\u8def\u5f84\u5f15\u5165\u5373\u53ef \u2502 \u251c\u2500\u2500 ghippo.png \u2502 \u251c\u2500\u2500 icon.png \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 products \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 scaffolds # \u8fd9\u91cc\u5b58\u653e\u6a21\u677f\u6587\u4ef6\uff0c\u4e00\u822c\u4e0d\u9700\u8981\u4fee\u6539 \u2502 \u251c\u2500\u2500 .pages.yaml # \u7279\u6b8a\u5904\u7406\uff0c\u589e\u52a0 hide:true \u4e0d\u5c55\u793a\u5728\u9876\u5c42 nav \u2502 \u2514\u2500\u2500 tags.md \u251c\u2500\u2500 stylesheets # \u57fa\u4e8e\u4e3b\u9898\u7684\u81ea\u5b9a\u4e49\u6837\u5f0f\uff0c\u4e00\u822c\u4e0d\u9700\u8981\u4fee\u6539 \u2502 \u2514\u2500\u2500 extra.css \u251c\u2500\u2500 support \u2502 \u251c\u2500\u2500 01 -mkdocs-material.md \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 survey \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ... .pages.yaml \u4ecb\u7ecd \u00b6 \u901a\u8fc7\u5bf9\u6bcf\u4e2a\u6587\u4ef6\u4e0b\u7684\u7279\u5b9a\u5904\u7406\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b .pages.yaml \u4e3a\u7a7a\u5373\u53ef\uff0c\u5982\u9700\u8981\u7279\u6b8a\u5904\u7406\uff0c\u53ef\u4ee5\u5728\u6587\u6863\u6dfb\u52a0\u4e0b\u65b9\u53c2\u6570 title : Products # \u6587\u4ef6\u5939\u5c55\u793a\u7684\u6807\u9898 order : 1 # \u6587\u4ef6\u5939\u7684\u987a\u5e8f\uff0c\u6570\u5b57\u8d8a\u5c0f\u8d8a\u9760\u524d hide : false # \u662f\u5426\u9690\u85cf\uff0c\u9ed8\u8ba4\u4e0d\u9690\u85cf nav : # \u91c7\u7528\u81ea\u5b9a\u4e49\u5bfc\u822a - filename.md - filename2.md - ... nav \u7684\u914d\u7f6e\u65b9\u5f0f\uff0c\u8fd8\u6709\u66f4\u591a\u9ad8\u7ea7\u7528\u6cd5\uff0c\u53ef\u4ee5\u53c2\u8003\u63d2\u4ef6\u505a\u7684\u7684 Github \u4ecb\u7ecd \u4f20\u9001\u95e8 \u5982\u4f55\u6dfb\u52a0\u4e00\u7bc7\u6587\u6863 \u00b6 1. \u5728 `docs/` \u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u5939\uff0c\u5982 `dce5.0` 2. \u6216\u8005\u5728 `docs/` \u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\uff0c\u5982 `01kpanda.md` 3. \u5b8c\u6210\u5bf9\u6587\u6863\u7684\u7f16\u5199 4. \u63a8\u9001\u6b64\u53d8\u66f4\u5230 gitlab , \u4e00\u822c 2 \u5206\u949f\u540e\uff0c\u5176\u4ed6\u4eba\u5c31\u53ef\u4ee5\u5728\u6587\u6863\u7ad9\u4e0a\u770b\u5230 Front Matter \u00b6 \u901a\u8fc7\u5728\u6587\u6863\u7684\u5934\u90e8 \u589e\u52a0\u5bf9\u5e94\u7684 Front Matter\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u6587\u6863\u7684\u6807\u9898\u3001\u63cf\u8ff0\u3001\u6807\u7b7e\u3001\u989d\u5916\u7684\u914d\u7f6e\u7b49 \u672c\u6587\u6863\u91c7\u7528\u4e86 Tag \u7684\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u8fdb\u884c\u6587\u6863\u7684 \u5168\u7ad9\u5173\u8054\uff0c\u6bd4\u5982\uff1a tags : - HowTo \u8fd8\u53ef\u4ee5\u4f7f\u7528\u7684\u53c2\u6570\uff1a title: \u7528\u6765\u5b9a\u4e49\u6587\u6863\u7684\u6807\u9898\uff0c\u5982\u679c\u672a\u5b9a\u4e49 Mkdocs \u4f1a\u9ed8\u8ba4\u91c7\u7528 \u6587\u6863\u5185\u7b2c\u4e00\u4e2a # \u4f5c\u4e3a\u6807\u9898 Markdown \u8bed\u6cd5\u4ecb\u7ecd \u00b6 \u5728\u57fa\u7840\u7684 Markdown \u8bed\u6cd5\u4e4b\u5916\uff0c Material \u98ce\u683c\u652f\u6301\u4ee5\u4e0b\u8bed\u6cd5 Obsidian \u53ef\u4ee5\u548c Material \u6709\u5f88\u597d\u7684\u517c\u5bb9\uff0c\u5b89\u88c5\u5bf9\u5e94\u7684\u63d2\u4ef6\u5373\u53ef \u5feb\u6377\u4f20\u9001\u95e8 \u00b6 Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Video \u00b6 Inner Table \u00b6 C C++ #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; } Annotations \u00b6 Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Unordered list Ordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Table \u00b6 Left Center Right Method Description GET Fetch resource PUT Update resource DELETE Delete resource Method Description GET Fetch resource PUT Update resource DELETE Delete resource Method Description GET Fetch resource PUT Update resource DELETE Delete resource Charts \u00b6 graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! stateDiagram-v2 state fork_state <<fork>> [*] --> fork_state fork_state --> State2 fork_state --> State3 state join_state <<join>> State2 --> join_state State3 --> join_state join_state --> State4 State4 --> [*] classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() } erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2 Formatting \u00b6 Highlighting changes \u00b6 Text can be deleted and replacement text added . This can also be combined into one a single operation. Highlighting is also possible and comments can be added inline . Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content. Highlighting text \u00b6 This was marked This was inserted This was deleted Sub- and superscripts \u00b6 H 2 0 A T A Adding keyboard keys \u00b6 Ctrl + Alt + Del emojis \u00b6 Images \u00b6 image \u00b6 figcaption \u00b6 Image caption Lists \u00b6 Using unorder lists \u00b6 Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Using ordered lists \u00b6 Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu task list \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit Vestibulum convallis sit amet nisi a tincidunt In hac habitasse platea dictumst In scelerisque nibh non dolor mollis congue sed et metus Praesent sed risus massa Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque Lorem ipsum dolor sit amet, consectetur adipiscing elit. \u21a9 \u21a9 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. \u21a9 \u21a9","title":"\u6587\u6863\u7ad9\u4f7f\u7528\u8bf4\u660e"},{"location":"support/01-mkdocs-material/#_1","text":"\u672c\u4ea7\u54c1\u6587\u6863\u7ad9\u91c7\u7528\u4e86 Mkdocs \u5f00\u53d1\u7684 Material \u98ce\u683c \u5efa\u7ad9 \u652f\u6301\u6587\u6863\u8bed\u6cd5\u4e3a Markdown","title":"\u6587\u6863\u7ad9\u4f7f\u7528\u8bf4\u660e"},{"location":"support/01-mkdocs-material/#_2","text":"Important \u6587\u6863\u5185\u5bb9\u5168\u90e8\u5b58\u653e\u5728 docs/ \u76ee\u5f55\u4e0b\uff0c\u4fee\u6539\u5185\u5bb9\u4ec5\u7f16\u8f91 docs/ \u76ee\u5f55\u4e0b\u7684\u6587\u4ef6\u5373\u53ef docs \u251c\u2500\u2500 .pages.yaml # \u7ad9\u70b9\u9876\u90e8\u5bfc\u822a\u914d\u7f6e\u6587\u4ef6\uff0c\u63a7\u5236\u987a\u5e8f\u548c\u540d\u79f0\uff0c\u4e00\u822c\u4e0d\u589e\u52a0 \u251c\u2500\u2500 README.md # \u9ed8\u8ba4\u60c5\u51b5\u4e0b \u76ee\u5f55\u4e0b README.md \u4f5c\u4e3a default \u9875\u9762 \u251c\u2500\u2500 SUMMARY.md \u251c\u2500\u2500 dce5.0 # \u5b50\u6587\u4ef6\u5939\uff0c\u652f\u6301\u591a\u7ea7\u76ee\u5f55\uff0c\u81ea\u52a8\u68c0\u6d4b\u914d\u7f6e \u2502 \u251c\u2500\u2500 01kpanda.md # \u5b50\u6587\u4ef6\u81ea\u52a8\u68c0\u6d4b\uff0c\u53ef\u4ee5\u901a\u8fc7\u6587\u4ef6\u540d\u524d\u7f00\u6570\u5b57\u63a7\u5236\u6392\u5e8f \u2502 \u251c\u2500\u2500 02ghippo.md \u2502 \u251c\u2500\u2500 03clusterpedia.md \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 design \u2502 \u251c\u2500\u2500 .pages.yaml # \u53ef\u4ee5\u4e0d\u521b\u5efa \u5f53\u9700\u8981\u5bf9\u4e00\u4e2a\u76ee\u5f55\u8fdb\u884c\u7279\u6b8a\u914d\u7f6e\u65f6\u5f15\u5165.pages.yaml \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 images # \u56fe\u7247\u6587\u4ef6\u5939\uff0c\u4f7f\u7528\u76f8\u5bf9\u8def\u5f84\u5f15\u5165\u5373\u53ef \u2502 \u251c\u2500\u2500 ghippo.png \u2502 \u251c\u2500\u2500 icon.png \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 products \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 scaffolds # \u8fd9\u91cc\u5b58\u653e\u6a21\u677f\u6587\u4ef6\uff0c\u4e00\u822c\u4e0d\u9700\u8981\u4fee\u6539 \u2502 \u251c\u2500\u2500 .pages.yaml # \u7279\u6b8a\u5904\u7406\uff0c\u589e\u52a0 hide:true \u4e0d\u5c55\u793a\u5728\u9876\u5c42 nav \u2502 \u2514\u2500\u2500 tags.md \u251c\u2500\u2500 stylesheets # \u57fa\u4e8e\u4e3b\u9898\u7684\u81ea\u5b9a\u4e49\u6837\u5f0f\uff0c\u4e00\u822c\u4e0d\u9700\u8981\u4fee\u6539 \u2502 \u2514\u2500\u2500 extra.css \u251c\u2500\u2500 support \u2502 \u251c\u2500\u2500 01 -mkdocs-material.md \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 ... \u2514\u2500\u2500 survey \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ...","title":"\u6587\u6863\u7ad9\u7ed3\u6784"},{"location":"support/01-mkdocs-material/#pagesyaml","text":"\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u6587\u4ef6\u4e0b\u7684\u7279\u5b9a\u5904\u7406\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b .pages.yaml \u4e3a\u7a7a\u5373\u53ef\uff0c\u5982\u9700\u8981\u7279\u6b8a\u5904\u7406\uff0c\u53ef\u4ee5\u5728\u6587\u6863\u6dfb\u52a0\u4e0b\u65b9\u53c2\u6570 title : Products # \u6587\u4ef6\u5939\u5c55\u793a\u7684\u6807\u9898 order : 1 # \u6587\u4ef6\u5939\u7684\u987a\u5e8f\uff0c\u6570\u5b57\u8d8a\u5c0f\u8d8a\u9760\u524d hide : false # \u662f\u5426\u9690\u85cf\uff0c\u9ed8\u8ba4\u4e0d\u9690\u85cf nav : # \u91c7\u7528\u81ea\u5b9a\u4e49\u5bfc\u822a - filename.md - filename2.md - ... nav \u7684\u914d\u7f6e\u65b9\u5f0f\uff0c\u8fd8\u6709\u66f4\u591a\u9ad8\u7ea7\u7528\u6cd5\uff0c\u53ef\u4ee5\u53c2\u8003\u63d2\u4ef6\u505a\u7684\u7684 Github \u4ecb\u7ecd \u4f20\u9001\u95e8","title":".pages.yaml \u4ecb\u7ecd"},{"location":"support/01-mkdocs-material/#_3","text":"1. \u5728 `docs/` \u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\u5939\uff0c\u5982 `dce5.0` 2. \u6216\u8005\u5728 `docs/` \u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u6587\u4ef6\uff0c\u5982 `01kpanda.md` 3. \u5b8c\u6210\u5bf9\u6587\u6863\u7684\u7f16\u5199 4. \u63a8\u9001\u6b64\u53d8\u66f4\u5230 gitlab , \u4e00\u822c 2 \u5206\u949f\u540e\uff0c\u5176\u4ed6\u4eba\u5c31\u53ef\u4ee5\u5728\u6587\u6863\u7ad9\u4e0a\u770b\u5230","title":"\u5982\u4f55\u6dfb\u52a0\u4e00\u7bc7\u6587\u6863"},{"location":"support/01-mkdocs-material/#front-matter","text":"\u901a\u8fc7\u5728\u6587\u6863\u7684\u5934\u90e8 \u589e\u52a0\u5bf9\u5e94\u7684 Front Matter\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u6587\u6863\u7684\u6807\u9898\u3001\u63cf\u8ff0\u3001\u6807\u7b7e\u3001\u989d\u5916\u7684\u914d\u7f6e\u7b49 \u672c\u6587\u6863\u91c7\u7528\u4e86 Tag \u7684\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u8fdb\u884c\u6587\u6863\u7684 \u5168\u7ad9\u5173\u8054\uff0c\u6bd4\u5982\uff1a tags : - HowTo \u8fd8\u53ef\u4ee5\u4f7f\u7528\u7684\u53c2\u6570\uff1a title: \u7528\u6765\u5b9a\u4e49\u6587\u6863\u7684\u6807\u9898\uff0c\u5982\u679c\u672a\u5b9a\u4e49 Mkdocs \u4f1a\u9ed8\u8ba4\u91c7\u7528 \u6587\u6863\u5185\u7b2c\u4e00\u4e2a # \u4f5c\u4e3a\u6807\u9898","title":"Front Matter"},{"location":"support/01-mkdocs-material/#markdown","text":"\u5728\u57fa\u7840\u7684 Markdown \u8bed\u6cd5\u4e4b\u5916\uff0c Material \u98ce\u683c\u652f\u6301\u4ee5\u4e0b\u8bed\u6cd5 Obsidian \u53ef\u4ee5\u548c Material \u6709\u5f88\u597d\u7684\u517c\u5bb9\uff0c\u5b89\u88c5\u5bf9\u5e94\u7684\u63d2\u4ef6\u5373\u53ef","title":"Markdown \u8bed\u6cd5\u4ecb\u7ecd"},{"location":"support/01-mkdocs-material/#_4","text":"Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2","title":"\u5feb\u6377\u4f20\u9001\u95e8"},{"location":"support/01-mkdocs-material/#video","text":"","title":"Video"},{"location":"support/01-mkdocs-material/#inner-table","text":"C C++ #include <stdio.h> int main ( void ) { printf ( \"Hello world! \\n \" ); return 0 ; } #include <iostream> int main ( void ) { std :: cout << \"Hello world!\" << std :: endl ; return 0 ; }","title":"Inner Table"},{"location":"support/01-mkdocs-material/#annotations","text":"Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Unordered list Ordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Annotations"},{"location":"support/01-mkdocs-material/#table","text":"Left Center Right Method Description GET Fetch resource PUT Update resource DELETE Delete resource Method Description GET Fetch resource PUT Update resource DELETE Delete resource Method Description GET Fetch resource PUT Update resource DELETE Delete resource","title":"Table"},{"location":"support/01-mkdocs-material/#charts","text":"graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!]; sequenceDiagram Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! stateDiagram-v2 state fork_state <<fork>> [*] --> fork_state fork_state --> State2 fork_state --> State3 state join_state <<join>> State2 --> join_state State3 --> join_state join_state --> State4 State4 --> [*] classDiagram Person <|-- Student Person <|-- Professor Person : +String name Person : +String phoneNumber Person : +String emailAddress Person: +purchaseParkingPass() Address \"1\" <-- \"0..1\" Person:lives at class Student{ +int studentNumber +int averageMark +isEligibleToEnrol() +getSeminarsTaken() } class Professor{ +int salary } class Address{ +String street +String city +String state +int postalCode +String country -validate() +outputAsLabel() } erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses Lorem ipsum 1 dolor sit amet, consectetur adipiscing elit. 2","title":"Charts"},{"location":"support/01-mkdocs-material/#formatting","text":"","title":"Formatting"},{"location":"support/01-mkdocs-material/#emojis","text":"","title":"emojis"},{"location":"support/01-mkdocs-material/#images","text":"","title":"Images"},{"location":"support/01-mkdocs-material/#lists","text":"","title":"Lists"},{"location":"support/docs/","text":"\u4ea7\u54c1\u6587\u6863 \u00b6 \u76ee\u524d\u4f7f\u7528\u7684\u6587\u6863\u5de5\u5177\u6709\uff1a Honkit Hugo Docusaurus Confluence Wiki DCE 5.0 \u6587\u6863\u7ad9 \u00b6 \u6587\u6863\u7ad9\u6a21\u677f Kpanda \u6587\u6863\u7ad9 Ghippo \u6587\u6863\u7ad9 mSpider \u6587\u6863\u7ad9 Insight \u6587\u6863\u7ad9 Skoala \u6587\u6863\u7ad9 DCE 4.0 \u53ca\u4e00\u4f53\u673a\u4ea7\u54c1\u6587\u6863 \u00b6 DCE 4.0 \u4ea7\u54c1\u6587\u6863 DX-Insight \u7528\u6237\u624b\u518c \u81ea\u52a8\u6269\u5bb9\u7528\u6237\u624b\u518c DNS \u7528\u6237\u624b\u518c Parcel \u7f51\u7edc\u7528\u6237\u624b\u518c Serverless \u7528\u6237\u624b\u518c \u4e91\u539f\u751f\u4e00\u4f53\u673a\u7528\u6237\u624b\u518c DSM \u6587\u6863 \u00b6 DSM \u7528\u6237\u624b\u518c DMP \u6587\u6863 \u00b6 DMP 2.6 \u7528\u6237\u624b\u518c DMP PoC \u9a8c\u8bc1\u64cd\u4f5c\u6587\u6863 DMP PoC \u6d4b\u8bd5\u7528\u4f8b \u5f00\u6e90\u7ad9\u70b9 \u00b6 KLTS Clusterpedia Merbridge HwameiStor","title":"\u4ea7\u54c1\u6587\u6863"},{"location":"support/docs/#_1","text":"\u76ee\u524d\u4f7f\u7528\u7684\u6587\u6863\u5de5\u5177\u6709\uff1a Honkit Hugo Docusaurus Confluence Wiki","title":"\u4ea7\u54c1\u6587\u6863"},{"location":"support/docs/#dce-50","text":"\u6587\u6863\u7ad9\u6a21\u677f Kpanda \u6587\u6863\u7ad9 Ghippo \u6587\u6863\u7ad9 mSpider \u6587\u6863\u7ad9 Insight \u6587\u6863\u7ad9 Skoala \u6587\u6863\u7ad9","title":"DCE 5.0 \u6587\u6863\u7ad9"},{"location":"support/docs/#dce-40","text":"DCE 4.0 \u4ea7\u54c1\u6587\u6863 DX-Insight \u7528\u6237\u624b\u518c \u81ea\u52a8\u6269\u5bb9\u7528\u6237\u624b\u518c DNS \u7528\u6237\u624b\u518c Parcel \u7f51\u7edc\u7528\u6237\u624b\u518c Serverless \u7528\u6237\u624b\u518c \u4e91\u539f\u751f\u4e00\u4f53\u673a\u7528\u6237\u624b\u518c","title":"DCE 4.0 \u53ca\u4e00\u4f53\u673a\u4ea7\u54c1\u6587\u6863"},{"location":"support/docs/#dsm","text":"DSM \u7528\u6237\u624b\u518c","title":"DSM \u6587\u6863"},{"location":"support/docs/#dmp","text":"DMP 2.6 \u7528\u6237\u624b\u518c DMP PoC \u9a8c\u8bc1\u64cd\u4f5c\u6587\u6863 DMP PoC \u6d4b\u8bd5\u7528\u4f8b","title":"DMP \u6587\u6863"},{"location":"support/docs/#_2","text":"KLTS Clusterpedia Merbridge HwameiStor","title":"\u5f00\u6e90\u7ad9\u70b9"},{"location":"support/flow-tools/","text":"\u6d41\u7a0b\u548c\u5de5\u5177 \u00b6 \u8bbe\u8ba1\u4ea7\u54c1\u6d41\u7a0b\uff0c\u63d0\u9ad8\u7814\u53d1\u6548\u7387\uff0c\u8bbe\u5b9a\u4ea7\u54c1\u89c4\u8303\uff0c\u641c\u5bfb\u4f18\u8d28\u5de5\u5177\u3002","title":"\u6d41\u7a0b\u548c\u5de5\u5177"},{"location":"support/flow-tools/#_1","text":"\u8bbe\u8ba1\u4ea7\u54c1\u6d41\u7a0b\uff0c\u63d0\u9ad8\u7814\u53d1\u6548\u7387\uff0c\u8bbe\u5b9a\u4ea7\u54c1\u89c4\u8303\uff0c\u641c\u5bfb\u4f18\u8d28\u5de5\u5177\u3002","title":"\u6d41\u7a0b\u548c\u5de5\u5177"},{"location":"support/patent/","text":"\u4ea7\u54c1\u4e13\u5229 \u00b6 \u6d41\u7a0b\uff1a\u6559\u6388\u8bc4\u5ba1\u4e13\u5229 idea \u2192 \u4ea4\u5e95\u4e66\u64b0\u5199 \u2192 \u6559\u6388\u5ba1\u9605\u4ea4\u5e95\u4e66 \u2192 \u4ee3\u7406\u673a\u6784\u5bf9\u63a5 \u2192 \u56fd\u77e5\u5c40\u53d7\u7406 \u2192 \u521d\u5ba1\u5408\u683c\u540e\u516c\u5f00 \u2192 \u5b9e\u8d28\u5ba1\u67e5\u7b54\u590d \u2192 \u6388\u6743\u3002 \u53c2\u9605 \u4e13\u5229\u7533\u8bf7\u6fc0\u52b1\u8ba1\u5212 \u3002","title":"\u4ea7\u54c1\u4e13\u5229"},{"location":"support/patent/#_1","text":"\u6d41\u7a0b\uff1a\u6559\u6388\u8bc4\u5ba1\u4e13\u5229 idea \u2192 \u4ea4\u5e95\u4e66\u64b0\u5199 \u2192 \u6559\u6388\u5ba1\u9605\u4ea4\u5e95\u4e66 \u2192 \u4ee3\u7406\u673a\u6784\u5bf9\u63a5 \u2192 \u56fd\u77e5\u5c40\u53d7\u7406 \u2192 \u521d\u5ba1\u5408\u683c\u540e\u516c\u5f00 \u2192 \u5b9e\u8d28\u5ba1\u67e5\u7b54\u590d \u2192 \u6388\u6743\u3002 \u53c2\u9605 \u4e13\u5229\u7533\u8bf7\u6fc0\u52b1\u8ba1\u5212 \u3002","title":"\u4ea7\u54c1\u4e13\u5229"},{"location":"support/teambuilding/","text":"\u56e2\u961f\u5efa\u8bbe \u00b6 \u53cb\u597d\u534f\u4f5c\uff0c\u5171\u540c\u8fdb\u6b65\u3002","title":"\u56e2\u961f\u5efa\u8bbe"},{"location":"support/teambuilding/#_1","text":"\u53cb\u597d\u534f\u4f5c\uff0c\u5171\u540c\u8fdb\u6b65\u3002","title":"\u56e2\u961f\u5efa\u8bbe"},{"location":"support/training/","text":"\u5b66\u4e60\u57f9\u8bad \u00b6 \u5b66\u800c\u65f6\u4e60\u4e4b\uff0c\u4e0d\u4ea6\u4e50\u4e4e\u3002 06frontend-ux.md","title":"\u5b66\u4e60\u57f9\u8bad"},{"location":"support/training/#_1","text":"\u5b66\u800c\u65f6\u4e60\u4e4b\uff0c\u4e0d\u4ea6\u4e50\u4e4e\u3002 06frontend-ux.md","title":"\u5b66\u4e60\u57f9\u8bad"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/","text":"DaoCloud CKA \u8003\u8bd5\u4ecb\u7ecd \u00b6 Info \u6ce8\u518c\u65f6\u5fc5\u987b\u4f7f\u7528\u516c\u53f8\u90ae\u7bb1\u5730\u5740\uff0c\u8003\u8bd5\u901a\u8fc7\u540e\uff0c\u5bfc\u51fa\u7535\u5b50\u53d1\u7968\u8fdb\u884c\u62a5\u9500 \u628a\u8003\u8bd5\u8bc1\u4e66\u4e0a\u4f20\u81f3 wiki \uff0c\u5e76\u5728\u62a5\u9500\u65f6\u9644\u4e0a\u76f8\u5173\u94fe\u63a5\uff0c\u65b9\u80fd\u6b63\u5e38\u62a5\u9500 \u5982\u679c\u5458\u5de5\u81ea\u62a5\u9500\u4e4b\u65e5\u8d77\u4e00\u5e74\u5185\u79bb\u804c\uff0c\u9700\u8981\u9000\u8fd8\u5bf9\u5e94\u62a5\u9500\u8d39\u7528\u7ed9\u516c \u62a5\u540d \u00b6 \u6ce8\u518c\uff1a\u5730\u5740\uff1ahttps://identity.linuxfoundation.org/user/login\uff0c\u6ce8\u518c\u8bf7\u4f7f\u7528\u516c\u53f8\u90ae\u4ef6\uff0c\u7136\u540e\u8003\u8bd5\u901a\u8fc7\u540e\uff0c\u5bfc\u51fa\u7535\u5b50\u53d1\u7968\u8fdb\u884c\u62a5\u9500 \u62a5\u540d\uff1ahttps://www.cncf.io/certification/CKA/\uff0c *\u70b9\u51fb\u201c REGISTER FOR THE EXAM \u201d\uff0c\u8fdb\u5165\u8d2d\u4e70\u754c\u9762* \u5728Coupons(\u4f18\u60e0\u5238)\u8f93\u5165DCUBEOFFER \u80fd\u51cf\u5c1148\u5200,\u7136\u540e\u586b\u5199\u957f\u5f97\u548c\u4ed8\u6b3e\u4fe1\u606f(\u9700\u8981visa\u6216\u522b\u7684\u80fd\u652f\u4ed8\u6ca1\u6709\u7684\u94f6\u884c\u5361\u6216\u4fe1\u7528\u5361) \u62a5\u540d\u5b8c\u6210\u540e\u4f1a\u770b\u5230\u5982\u4e0b\u754c\u9762https://portal.linuxfoundation.org/portal (\u8fd9\u4e2a\u662f\u8003\u8bd5\u8fc7\u7684\u754c\u9762\uff0c\u53ef\u80fd\u4f1a\u6709\u4e00\u4e9b\u4e0d\u4e00\u6837)\uff0c\u70b9\u51fb\"checklist\"\u68c0\u67e5\u4f60\u7684\u8bbe\u5907\uff0c \u4e0a\u56fe\u53f3\u8fb9\"schedule Exam\"\uff0c\u9009\u62e9\u4f60\u7684\u8003\u8bd5\u5730\u65b9\u548c\u65f6\u95f4\u548c\u65f6\u533a \u2013-\u53ef\u4ee5\u9009\u62e9\u5728\u7ebf\u8003\u8bd5\u548c\u4e2d\u56fd\u533a\u7684\u73b0\u573a\u8003\u8bd5\uff0c\u5982\u679c\u6539\u65f6\u95f4\u9700\u8981\u63d0\u524d24\u5c0f\u65f6 \u516c\u53f8\u8d44\u6599 \u00b6 \u8ba4\u8bc1\u8003\u8bd5\u4ecb\u7ecd https://dwiki.daocloud.io/pages/viewpage.action?pageId=13060398 \u5df2\u901a\u8fc7\u8003\u8bd5\u7684\u524d\u8f88 https://dwiki.daocloud.io/pages/viewpage.action?pageId=16104222 CKA \u57f9\u8bad\u6587\u6863 https://dwiki.daocloud.io/pages/viewpage.action?pageId=55337980","title":"DaoCloud CKA \u8003\u8bd5\u4ecb\u7ecd"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/#daocloud-cka","text":"Info \u6ce8\u518c\u65f6\u5fc5\u987b\u4f7f\u7528\u516c\u53f8\u90ae\u7bb1\u5730\u5740\uff0c\u8003\u8bd5\u901a\u8fc7\u540e\uff0c\u5bfc\u51fa\u7535\u5b50\u53d1\u7968\u8fdb\u884c\u62a5\u9500 \u628a\u8003\u8bd5\u8bc1\u4e66\u4e0a\u4f20\u81f3 wiki \uff0c\u5e76\u5728\u62a5\u9500\u65f6\u9644\u4e0a\u76f8\u5173\u94fe\u63a5\uff0c\u65b9\u80fd\u6b63\u5e38\u62a5\u9500 \u5982\u679c\u5458\u5de5\u81ea\u62a5\u9500\u4e4b\u65e5\u8d77\u4e00\u5e74\u5185\u79bb\u804c\uff0c\u9700\u8981\u9000\u8fd8\u5bf9\u5e94\u62a5\u9500\u8d39\u7528\u7ed9\u516c","title":"DaoCloud CKA \u8003\u8bd5\u4ecb\u7ecd"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/#_1","text":"\u6ce8\u518c\uff1a\u5730\u5740\uff1ahttps://identity.linuxfoundation.org/user/login\uff0c\u6ce8\u518c\u8bf7\u4f7f\u7528\u516c\u53f8\u90ae\u4ef6\uff0c\u7136\u540e\u8003\u8bd5\u901a\u8fc7\u540e\uff0c\u5bfc\u51fa\u7535\u5b50\u53d1\u7968\u8fdb\u884c\u62a5\u9500 \u62a5\u540d\uff1ahttps://www.cncf.io/certification/CKA/\uff0c *\u70b9\u51fb\u201c REGISTER FOR THE EXAM \u201d\uff0c\u8fdb\u5165\u8d2d\u4e70\u754c\u9762* \u5728Coupons(\u4f18\u60e0\u5238)\u8f93\u5165DCUBEOFFER \u80fd\u51cf\u5c1148\u5200,\u7136\u540e\u586b\u5199\u957f\u5f97\u548c\u4ed8\u6b3e\u4fe1\u606f(\u9700\u8981visa\u6216\u522b\u7684\u80fd\u652f\u4ed8\u6ca1\u6709\u7684\u94f6\u884c\u5361\u6216\u4fe1\u7528\u5361) \u62a5\u540d\u5b8c\u6210\u540e\u4f1a\u770b\u5230\u5982\u4e0b\u754c\u9762https://portal.linuxfoundation.org/portal (\u8fd9\u4e2a\u662f\u8003\u8bd5\u8fc7\u7684\u754c\u9762\uff0c\u53ef\u80fd\u4f1a\u6709\u4e00\u4e9b\u4e0d\u4e00\u6837)\uff0c\u70b9\u51fb\"checklist\"\u68c0\u67e5\u4f60\u7684\u8bbe\u5907\uff0c \u4e0a\u56fe\u53f3\u8fb9\"schedule Exam\"\uff0c\u9009\u62e9\u4f60\u7684\u8003\u8bd5\u5730\u65b9\u548c\u65f6\u95f4\u548c\u65f6\u533a \u2013-\u53ef\u4ee5\u9009\u62e9\u5728\u7ebf\u8003\u8bd5\u548c\u4e2d\u56fd\u533a\u7684\u73b0\u573a\u8003\u8bd5\uff0c\u5982\u679c\u6539\u65f6\u95f4\u9700\u8981\u63d0\u524d24\u5c0f\u65f6","title":"\u62a5\u540d"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/#_2","text":"\u8ba4\u8bc1\u8003\u8bd5\u4ecb\u7ecd https://dwiki.daocloud.io/pages/viewpage.action?pageId=13060398 \u5df2\u901a\u8fc7\u8003\u8bd5\u7684\u524d\u8f88 https://dwiki.daocloud.io/pages/viewpage.action?pageId=16104222 CKA \u57f9\u8bad\u6587\u6863 https://dwiki.daocloud.io/pages/viewpage.action?pageId=55337980","title":"\u516c\u53f8\u8d44\u6599"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/","text":"CKA \u5907\u8003\u6307\u5357 \u00b6 \u8003\u524d\u5fc5\u61c2 \u00b6 \u9700\u8981\u79d1\u5b66\u4e0a\u7f51\uff0cCKA\u8003\u8bd5\u65f6\u9700\u8981\u68c0\u67e5\u4e0a\u6700\u4f4e 500Kbps \u4e0b\u8f7d\u548c 256Kbps \u4e0a\u4f20 \u7b14\u8bb0\u672c\u7535\u8111\u81ea\u5e26\u6444\u50cf\u5934\u5373\u53ef \u9700\u8981\u5b89\u88c5 Innovative Exams Screensharing \u8c37\u6b4c\u6d4f\u89c8\u5668\u63d2\u4ef6 CKA\u76ee\u524d\u5df2\u7ecf\u63a8\u51fa\u4e2d\u6587\u8003\u8bd5\u670d\u52a1\uff0c\u5e76\u4e14\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u652f\u4ed8\u5b9d\u8d2d\u4e70\uff0c\u5e76\u5f00\u5177\u53d1\u7968\u3002 \u62a5\u540d\u5730\u5740\uff1a https://training.linuxfoundation.cn/certificate/details/1 \u4e2d\u6587\u62a5\u540d\u8003\u8bd5\u8d39\u7528\u4e3a2088\u5143\u4eba\u6c11\u5e01 CKA\u73b0\u6709\u4ee5\u4e0b\u8003\u8bd5\u65b9\u5f0f\u53ef\u4f9b\u9009\u62e9\uff1a \u82f1\u6587\u76d1\u8003\u5b98\u2013\u7ebf\u4e0a\u8003\u8bd5 (\u8003\u8bd5\u7f16\u53f7\uff1aCKA)\uff0c \u4e2d\u6587\u76d1\u8003\u5b98\u2013\u7ebf\u4e0a\u8003\u8bd5(\u8003\u8bd5\u7f16\u53f7\uff1aCKA-CN)\u3002 \u53ef\u9009\u62e9\u8fdc\u7a0b\u6216\u8005\u662f\u5728\u8003\u70b9\u8fdb\u884c\u8003\u8bd5\uff08\u8003\u70b9\u7684\u7f51\u7edc\u60c5\u51b5\u4e5f\u4e00\u822c\uff0c\u5e76\u65e0\u592a\u5927\u533a\u522b\uff09 => \u5efa\u8bae\u8fdc\u7a0b \u8003\u524d\u6ce8\u610f\u4e8b\u9879 \u00b6 CKA\u8ba4\u8bc1\u8003\u8bd5\u62a5\u540d \u00b6 CKA\u76ee\u524d\u5df2\u7ecf\u63a8\u51fa\u4e2d\u6587\u8003\u8bd5\u670d\u52a1\uff0c\u5e76\u4e14\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u652f\u4ed8\u5b9d\u8d2d\u4e70\uff0c\u5e76\u5f00\u5177\u53d1\u7968\u3002 \u62a5\u540d\u5730\u5740\uff1a https://training.linuxfoundation.cn/certificate/details/1 \u4e2d\u6587\u62a5\u540d\u8003\u8bd5\u8d39\u7528\u4e3a2088\u5143\u4eba\u6c11\u5e01\u3002 CKA\u73b0\u6709\u4ee5\u4e0b\u8003\u8bd5\u65b9\u5f0f\u53ef\u4f9b\u9009\u62e9\uff1a \u82f1\u6587\u76d1\u8003\u5b98\u2013\u7ebf\u4e0a\u8003\u8bd5 (\u8003\u8bd5\u7f16\u53f7\uff1aCKA)\uff0c \u4e2d\u6587\u76d1\u8003\u5b98\u2013\u7ebf\u4e0a\u8003\u8bd5(\u8003\u8bd5\u7f16\u53f7\uff1aCKA-CN)\u3002\u53ef\u9009\u62e9\u8fdc\u7a0b\u6216\u8005\u662f\u5728\u8003\u70b9\u8fdb\u884c\u8003\u8bd5\uff08\u8003\u70b9\u7684\u7f51\u7edc\u60c5\u51b5\u4e5f\u4e00\u822c\uff0c\u5e76\u65e0\u592a\u5927\u533a\u522b\uff09\u3002 CKA\u8ba4\u8bc1\u8003\u8bd5\u51c6\u5907 \u00b6 \u4ee5\u4e2d\u6587\u8003\u8bd5\u4e3a\u4f8b\uff0c\u9700\u8981\u505a\u597d\u4ee5\u4e0b\u51c6\u5907\uff1a \u5148\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\uff0c\u51c6\u5907\u8eab\u4efd\u8bc1\u53ca\u6709\u672c\u4eba\u7b7e\u540d\u7684\u4fe1\u7528\u5361\uff08\u5883\u5185\u5883\u5916\u90fd\u53ef\u4ee5\uff09 \u7ea6\u8003\uff08\u6709\u6548\u671f\u4e00\u5e74\uff0c\u6ce8\u610f\u65f6\u533a\u95ee\u9898\u3002\u8003\u8bd5\u524d24\u5c0f\u65f6\u53ef\u4fee\u6539\uff09 \u73af\u5883\u68c0\u6d4b\uff08 WebDelivery Compatibility Check \uff0c\u9700\u8981\u63d0\u524d\u8c03\u8bd5\u597d\u7f51\u7edc\u3001\u5b89\u88c5\u63d2\u4ef6\uff09\u7b49\u5982\u4e0b\u4e8b\u9879\u3002 \u6240\u6709\u51c6\u5907\u9879\u5747\u901a\u8fc7\u540e\u624d\u53ef\u6b63\u5f0f\u53c2\u52a0\u8003\u8bd5\u3002 CKA\u8ba4\u8bc1\u8003\u8bd5\u7cfb\u7edf\u8981\u6c42 \u00b6 \u8003\u8bd5\u8981\u6c42\u4f7f\u7528chrome\u6216\u5176\u4ed6chromium\u5185\u6838\u7684\u6d4f\u89c8\u5668\uff0c\u5b89\u88c5innovactive exams screensharing\u63d2\u4ef6\uff0c\u5e76\u6253\u5f00\u7b2c\u4e09\u65b9cookie\u3002\u5efa\u8bae\u4f7f\u7528\u8f83\u65b0\u7248\u672c\u7684chrome\u3002 \u9700\u8981\u540c\u6b65\u97f3\u9891\u3001\u89c6\u9891\u548c\u684c\u9762\uff0c\u5bf9\u7f51\u7edc\u5e26\u5bbd\u53ca\u7a33\u5b9a\u6027\u6709\u8f83\u9ad8\u8981\u6c42\u3002\u5efa\u8bae\u8c03\u8bd5\u597d\u7f51\u7edc\uff08\u672c\u4eba\u4e3a\u4e86\u8003\u8bd5\u4e13\u95e8\u6362\u4e86\u5343\u5146\u8def\u7531\u5668 _ \uff09\u3002\u5e76\u4e14\u5982\u679c\u662f\u5171\u7528\u7f51\u7edc\uff0c\u63d0\u524d\u548c\u4ed6\u4eba\u6253\u58f0\u62db\u547c\uff0c\u4e0d\u8981\u770b\u89c6\u9891\u6216\u8005\u4e0b\u8f7d\u5927\u6587\u4ef6\u3002 \u63d0\u524d\u51c6\u5907\u597d\u9ea6\u514b\u98ce\u7b49\u8bbe\u5907\uff0c\u786e\u4fdd\u80fd\u91c7\u96c6\u5230\u8003\u8bd5\u73af\u5883\u58f0\u97f3\u3002\u7b14\u8bb0\u672c\u7535\u8111\u7528\u81ea\u5e26\u7684\u5373\u53ef\u3002 \u63d0\u524d\u51c6\u5907\u597d\u6444\u50cf\u5934\u7b49\u8bbe\u5907\uff0c\u786e\u4fdd\u80fd\u91c7\u96c6\u5230\u8003\u8bd5\u73af\u5883\u753b\u9762\uff0c\u76d1\u8003\u5b98\u4f1a\u8981\u6c42\u79fb\u52a8\u6444\u50cf\u5934\u67e5\u770b\u684c\u9762\u3001\u684c\u5e95\u4ee5\u53ca\u5468\u56f4\u73af\u5883\u3002\u7b14\u8bb0\u672c\u7535\u8111\u7528\u81ea\u5e26\u7684\u5373\u53ef\u3002 CKA\u8ba4\u8bc1\u8003\u8bd5\u73af\u5883\u51c6\u5907 \u00b6 \u6574\u6d01\u7684\u684c\u9762 \u684c\u9762\u4e0d\u80fd\u6709\u7eb8\u3001\u7b14\u3001\u7535\u5b50\u8bbe\u5907\u6216\u5176\u4ed6\u6742\u7269\u3002\u53ef\u4ee5\u559d\u6c34\uff0c\u4e0d\u80fd\u5403\u4e1c\u897f\uff0c\u996e\u7528\u6c34\u4e0d\u80fd\u6709\u6807\u8bc6\u3002 \u684c\u5e95\u4e0d\u80fd\u6709\u7eb8\u3001\u5783\u573e\u6876\u6216\u5176\u4ed6\u6742\u7269 \u5e72\u51c0\u7684\u5899\u58c1 \u5899\u58c1\u4e0a\u4e0d\u80fd\u8d34\u6709\u7eb8\u6216\u6253\u5370\u7269\u3002\u5982\u679c\u6709\u5728\u8003\u8bd5\u5f00\u59cb\u524d\u4f1a\u88ab\u8981\u6c42\u79fb\u9664 \u53ef\u4ee5\u6709\u753b\u4f5c\u6216\u8005\u5899\u58c1\u88c5\u9970 \u5149\u7ebf \u8981\u6c42\u5149\u7ebf\u5145\u8db3\uff0c\u80fd\u770b\u6e05\u8003\u751f\u7684\u8138\u3001\u624b\u548c\u5468\u56f4\u73af\u5883 \u8003\u751f\u8eab\u540e\u6ca1\u6709\u660e\u4eae\u7684\u706f\u5149\u6216\u8005\u7a97\u6237 \u5176\u4ed6 \u8003\u8bd5\u671f\u95f4\u8003\u751f\u5fc5\u987b\u7559\u5728\u6444\u50cf\u5934\u8303\u56f4\u5185 \u8003\u8bd5\u73af\u5883\u5e94\u5c3d\u53ef\u80fd\u5b89\u9759\uff0c\u907f\u514d\u5496\u5561\u5385\u3001\u5f00\u653e\u5f0f\u529e\u516c\u573a\u6240\u7b49 \u62a5\u540d\u5b8c\u6210 \u00b6 \u5b8c\u6210\u62a5\u540d\uff0c\u7ea6\u5b9a\u8003\u8bd5\u65f6\u95f4\u540e\u4f60\u4f1a\u6536\u5230\u8003\u8bd5\u5b98\u65b9\u90ae\u4ef6\uff0c\u81ea\u5df1\u8981\u4ed4\u7ec6\u9605\u8bfb\u3002 \u7f51\u7edc\u4e00\u5b9a\u8981\u5feb\uff0c\u4e00\u822c\u53ef\u4ee5\u9009\u62e9\u5468\u672b\u65e9\u4e0a4,5\u70b9\uff0c\u8fd9\u4e2a\u65f6\u5019\u5916\u56fd\u7f51\u7ad9\u53ef\u80fd\u4e0d\u5361\uff0c\u8003\u8bd5\u65f6\u95f4\u4e09\u4e2a\u5c0f\u65f6\u3002 \u63d0\u524d\u51c6\u5907\u597d\u62a4\u7167\uff0c\u6e05\u7406\u597d\u684c\u9762\uff0c\u8003\u8bd5\u8fc7\u7a0b\u4e5f\u5f88\u4e25\u683c\uff08\u6211\u8003\u8bd5\u5f53\u5929\u786e\u8ba4\u4e2a\u4eba\u4fe1\u606f\u52a0\u6e05\u7406\u684c\u9762\u82b1\u4e8650\u5206\u949f\uff0c\u5f53\u7136\u4f1a\u987a\u5ef6\u65f6\u95f4\uff0c\u4f46\u662f\u4e0d\u80fd\u786e\u4fdd\u7f51\u7edc\u60c5\u51b5\uff01\uff01\uff09 \u8003\u8bd5\u65f6\u53ea\u80fd\u6253\u5f00\u4e00\u4e2akubeenetes.io\u5b98\u7f51\u6587\u6863\u9875\u7b7e \u8003\u8bd5\u7ed3\u675f\u540e\u4f1a\u81ea\u52a8\u8bc4\u5206\uff0c36\u5c0f\u65f6\u5185\u4f1a\u6536\u5230\u90ae\u4ef6\u544a\u77e5\u8003\u8bd5\u7ed3\u679c\u662f\u5426\u901a\u8fc7\u3002 CKA\u8ba4\u8bc1\u6709\u6548\u671f\u4e3a3\u5e74\u30023\u5e74\u4e4b\u540e\u6839\u636e\u4e2a\u4eba\u9700\u8981\u51b3\u5b9a\u662f\u5426\u91cd\u8003\u3002 \u5b66\u4e60\u8d44\u6599 \u00b6 \u53c2\u8003\u6587\u6863: https://www.cnblogs.com/miketwais/p/CKA.html https://mp.weixin.qq.com/s/pK-_nTAQoqNwNVHhiqCEjQ (\u975e\u5e38\u8be6\u7ec6) \u5b66\u4e60\u89c6\u9891\uff1a https://www.bilibili.com/video/BV1S7411m7vM","title":"CKA \u5907\u8003\u6307\u5357"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#cka","text":"","title":"CKA \u5907\u8003\u6307\u5357"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#_1","text":"\u9700\u8981\u79d1\u5b66\u4e0a\u7f51\uff0cCKA\u8003\u8bd5\u65f6\u9700\u8981\u68c0\u67e5\u4e0a\u6700\u4f4e 500Kbps \u4e0b\u8f7d\u548c 256Kbps \u4e0a\u4f20 \u7b14\u8bb0\u672c\u7535\u8111\u81ea\u5e26\u6444\u50cf\u5934\u5373\u53ef \u9700\u8981\u5b89\u88c5 Innovative Exams Screensharing \u8c37\u6b4c\u6d4f\u89c8\u5668\u63d2\u4ef6 CKA\u76ee\u524d\u5df2\u7ecf\u63a8\u51fa\u4e2d\u6587\u8003\u8bd5\u670d\u52a1\uff0c\u5e76\u4e14\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u652f\u4ed8\u5b9d\u8d2d\u4e70\uff0c\u5e76\u5f00\u5177\u53d1\u7968\u3002 \u62a5\u540d\u5730\u5740\uff1a https://training.linuxfoundation.cn/certificate/details/1 \u4e2d\u6587\u62a5\u540d\u8003\u8bd5\u8d39\u7528\u4e3a2088\u5143\u4eba\u6c11\u5e01 CKA\u73b0\u6709\u4ee5\u4e0b\u8003\u8bd5\u65b9\u5f0f\u53ef\u4f9b\u9009\u62e9\uff1a \u82f1\u6587\u76d1\u8003\u5b98\u2013\u7ebf\u4e0a\u8003\u8bd5 (\u8003\u8bd5\u7f16\u53f7\uff1aCKA)\uff0c \u4e2d\u6587\u76d1\u8003\u5b98\u2013\u7ebf\u4e0a\u8003\u8bd5(\u8003\u8bd5\u7f16\u53f7\uff1aCKA-CN)\u3002 \u53ef\u9009\u62e9\u8fdc\u7a0b\u6216\u8005\u662f\u5728\u8003\u70b9\u8fdb\u884c\u8003\u8bd5\uff08\u8003\u70b9\u7684\u7f51\u7edc\u60c5\u51b5\u4e5f\u4e00\u822c\uff0c\u5e76\u65e0\u592a\u5927\u533a\u522b\uff09 => \u5efa\u8bae\u8fdc\u7a0b","title":"\u8003\u524d\u5fc5\u61c2"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#_2","text":"","title":"\u8003\u524d\u6ce8\u610f\u4e8b\u9879"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#cka_1","text":"CKA\u76ee\u524d\u5df2\u7ecf\u63a8\u51fa\u4e2d\u6587\u8003\u8bd5\u670d\u52a1\uff0c\u5e76\u4e14\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u652f\u4ed8\u5b9d\u8d2d\u4e70\uff0c\u5e76\u5f00\u5177\u53d1\u7968\u3002 \u62a5\u540d\u5730\u5740\uff1a https://training.linuxfoundation.cn/certificate/details/1 \u4e2d\u6587\u62a5\u540d\u8003\u8bd5\u8d39\u7528\u4e3a2088\u5143\u4eba\u6c11\u5e01\u3002 CKA\u73b0\u6709\u4ee5\u4e0b\u8003\u8bd5\u65b9\u5f0f\u53ef\u4f9b\u9009\u62e9\uff1a \u82f1\u6587\u76d1\u8003\u5b98\u2013\u7ebf\u4e0a\u8003\u8bd5 (\u8003\u8bd5\u7f16\u53f7\uff1aCKA)\uff0c \u4e2d\u6587\u76d1\u8003\u5b98\u2013\u7ebf\u4e0a\u8003\u8bd5(\u8003\u8bd5\u7f16\u53f7\uff1aCKA-CN)\u3002\u53ef\u9009\u62e9\u8fdc\u7a0b\u6216\u8005\u662f\u5728\u8003\u70b9\u8fdb\u884c\u8003\u8bd5\uff08\u8003\u70b9\u7684\u7f51\u7edc\u60c5\u51b5\u4e5f\u4e00\u822c\uff0c\u5e76\u65e0\u592a\u5927\u533a\u522b\uff09\u3002","title":"CKA\u8ba4\u8bc1\u8003\u8bd5\u62a5\u540d"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#cka_2","text":"\u4ee5\u4e2d\u6587\u8003\u8bd5\u4e3a\u4f8b\uff0c\u9700\u8981\u505a\u597d\u4ee5\u4e0b\u51c6\u5907\uff1a \u5148\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\uff0c\u51c6\u5907\u8eab\u4efd\u8bc1\u53ca\u6709\u672c\u4eba\u7b7e\u540d\u7684\u4fe1\u7528\u5361\uff08\u5883\u5185\u5883\u5916\u90fd\u53ef\u4ee5\uff09 \u7ea6\u8003\uff08\u6709\u6548\u671f\u4e00\u5e74\uff0c\u6ce8\u610f\u65f6\u533a\u95ee\u9898\u3002\u8003\u8bd5\u524d24\u5c0f\u65f6\u53ef\u4fee\u6539\uff09 \u73af\u5883\u68c0\u6d4b\uff08 WebDelivery Compatibility Check \uff0c\u9700\u8981\u63d0\u524d\u8c03\u8bd5\u597d\u7f51\u7edc\u3001\u5b89\u88c5\u63d2\u4ef6\uff09\u7b49\u5982\u4e0b\u4e8b\u9879\u3002 \u6240\u6709\u51c6\u5907\u9879\u5747\u901a\u8fc7\u540e\u624d\u53ef\u6b63\u5f0f\u53c2\u52a0\u8003\u8bd5\u3002","title":"CKA\u8ba4\u8bc1\u8003\u8bd5\u51c6\u5907"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#cka_3","text":"\u8003\u8bd5\u8981\u6c42\u4f7f\u7528chrome\u6216\u5176\u4ed6chromium\u5185\u6838\u7684\u6d4f\u89c8\u5668\uff0c\u5b89\u88c5innovactive exams screensharing\u63d2\u4ef6\uff0c\u5e76\u6253\u5f00\u7b2c\u4e09\u65b9cookie\u3002\u5efa\u8bae\u4f7f\u7528\u8f83\u65b0\u7248\u672c\u7684chrome\u3002 \u9700\u8981\u540c\u6b65\u97f3\u9891\u3001\u89c6\u9891\u548c\u684c\u9762\uff0c\u5bf9\u7f51\u7edc\u5e26\u5bbd\u53ca\u7a33\u5b9a\u6027\u6709\u8f83\u9ad8\u8981\u6c42\u3002\u5efa\u8bae\u8c03\u8bd5\u597d\u7f51\u7edc\uff08\u672c\u4eba\u4e3a\u4e86\u8003\u8bd5\u4e13\u95e8\u6362\u4e86\u5343\u5146\u8def\u7531\u5668 _ \uff09\u3002\u5e76\u4e14\u5982\u679c\u662f\u5171\u7528\u7f51\u7edc\uff0c\u63d0\u524d\u548c\u4ed6\u4eba\u6253\u58f0\u62db\u547c\uff0c\u4e0d\u8981\u770b\u89c6\u9891\u6216\u8005\u4e0b\u8f7d\u5927\u6587\u4ef6\u3002 \u63d0\u524d\u51c6\u5907\u597d\u9ea6\u514b\u98ce\u7b49\u8bbe\u5907\uff0c\u786e\u4fdd\u80fd\u91c7\u96c6\u5230\u8003\u8bd5\u73af\u5883\u58f0\u97f3\u3002\u7b14\u8bb0\u672c\u7535\u8111\u7528\u81ea\u5e26\u7684\u5373\u53ef\u3002 \u63d0\u524d\u51c6\u5907\u597d\u6444\u50cf\u5934\u7b49\u8bbe\u5907\uff0c\u786e\u4fdd\u80fd\u91c7\u96c6\u5230\u8003\u8bd5\u73af\u5883\u753b\u9762\uff0c\u76d1\u8003\u5b98\u4f1a\u8981\u6c42\u79fb\u52a8\u6444\u50cf\u5934\u67e5\u770b\u684c\u9762\u3001\u684c\u5e95\u4ee5\u53ca\u5468\u56f4\u73af\u5883\u3002\u7b14\u8bb0\u672c\u7535\u8111\u7528\u81ea\u5e26\u7684\u5373\u53ef\u3002","title":"CKA\u8ba4\u8bc1\u8003\u8bd5\u7cfb\u7edf\u8981\u6c42"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#cka_4","text":"\u6574\u6d01\u7684\u684c\u9762 \u684c\u9762\u4e0d\u80fd\u6709\u7eb8\u3001\u7b14\u3001\u7535\u5b50\u8bbe\u5907\u6216\u5176\u4ed6\u6742\u7269\u3002\u53ef\u4ee5\u559d\u6c34\uff0c\u4e0d\u80fd\u5403\u4e1c\u897f\uff0c\u996e\u7528\u6c34\u4e0d\u80fd\u6709\u6807\u8bc6\u3002 \u684c\u5e95\u4e0d\u80fd\u6709\u7eb8\u3001\u5783\u573e\u6876\u6216\u5176\u4ed6\u6742\u7269 \u5e72\u51c0\u7684\u5899\u58c1 \u5899\u58c1\u4e0a\u4e0d\u80fd\u8d34\u6709\u7eb8\u6216\u6253\u5370\u7269\u3002\u5982\u679c\u6709\u5728\u8003\u8bd5\u5f00\u59cb\u524d\u4f1a\u88ab\u8981\u6c42\u79fb\u9664 \u53ef\u4ee5\u6709\u753b\u4f5c\u6216\u8005\u5899\u58c1\u88c5\u9970 \u5149\u7ebf \u8981\u6c42\u5149\u7ebf\u5145\u8db3\uff0c\u80fd\u770b\u6e05\u8003\u751f\u7684\u8138\u3001\u624b\u548c\u5468\u56f4\u73af\u5883 \u8003\u751f\u8eab\u540e\u6ca1\u6709\u660e\u4eae\u7684\u706f\u5149\u6216\u8005\u7a97\u6237 \u5176\u4ed6 \u8003\u8bd5\u671f\u95f4\u8003\u751f\u5fc5\u987b\u7559\u5728\u6444\u50cf\u5934\u8303\u56f4\u5185 \u8003\u8bd5\u73af\u5883\u5e94\u5c3d\u53ef\u80fd\u5b89\u9759\uff0c\u907f\u514d\u5496\u5561\u5385\u3001\u5f00\u653e\u5f0f\u529e\u516c\u573a\u6240\u7b49","title":"CKA\u8ba4\u8bc1\u8003\u8bd5\u73af\u5883\u51c6\u5907"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#_3","text":"\u5b8c\u6210\u62a5\u540d\uff0c\u7ea6\u5b9a\u8003\u8bd5\u65f6\u95f4\u540e\u4f60\u4f1a\u6536\u5230\u8003\u8bd5\u5b98\u65b9\u90ae\u4ef6\uff0c\u81ea\u5df1\u8981\u4ed4\u7ec6\u9605\u8bfb\u3002 \u7f51\u7edc\u4e00\u5b9a\u8981\u5feb\uff0c\u4e00\u822c\u53ef\u4ee5\u9009\u62e9\u5468\u672b\u65e9\u4e0a4,5\u70b9\uff0c\u8fd9\u4e2a\u65f6\u5019\u5916\u56fd\u7f51\u7ad9\u53ef\u80fd\u4e0d\u5361\uff0c\u8003\u8bd5\u65f6\u95f4\u4e09\u4e2a\u5c0f\u65f6\u3002 \u63d0\u524d\u51c6\u5907\u597d\u62a4\u7167\uff0c\u6e05\u7406\u597d\u684c\u9762\uff0c\u8003\u8bd5\u8fc7\u7a0b\u4e5f\u5f88\u4e25\u683c\uff08\u6211\u8003\u8bd5\u5f53\u5929\u786e\u8ba4\u4e2a\u4eba\u4fe1\u606f\u52a0\u6e05\u7406\u684c\u9762\u82b1\u4e8650\u5206\u949f\uff0c\u5f53\u7136\u4f1a\u987a\u5ef6\u65f6\u95f4\uff0c\u4f46\u662f\u4e0d\u80fd\u786e\u4fdd\u7f51\u7edc\u60c5\u51b5\uff01\uff01\uff09 \u8003\u8bd5\u65f6\u53ea\u80fd\u6253\u5f00\u4e00\u4e2akubeenetes.io\u5b98\u7f51\u6587\u6863\u9875\u7b7e \u8003\u8bd5\u7ed3\u675f\u540e\u4f1a\u81ea\u52a8\u8bc4\u5206\uff0c36\u5c0f\u65f6\u5185\u4f1a\u6536\u5230\u90ae\u4ef6\u544a\u77e5\u8003\u8bd5\u7ed3\u679c\u662f\u5426\u901a\u8fc7\u3002 CKA\u8ba4\u8bc1\u6709\u6548\u671f\u4e3a3\u5e74\u30023\u5e74\u4e4b\u540e\u6839\u636e\u4e2a\u4eba\u9700\u8981\u51b3\u5b9a\u662f\u5426\u91cd\u8003\u3002","title":"\u62a5\u540d\u5b8c\u6210"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/00-CKA%E8%80%83%E8%AF%95%E6%8C%87%E5%8D%97/#_4","text":"\u53c2\u8003\u6587\u6863: https://www.cnblogs.com/miketwais/p/CKA.html https://mp.weixin.qq.com/s/pK-_nTAQoqNwNVHhiqCEjQ (\u975e\u5e38\u8be6\u7ec6) \u5b66\u4e60\u89c6\u9891\uff1a https://www.bilibili.com/video/BV1S7411m7vM","title":"\u5b66\u4e60\u8d44\u6599"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/","text":"CKA\u8bd5\u9898\u6536\u96c6-01 \u00b6 \u8003\u524d\u51c6\u5907\u5de5\u4f5c \u00b6 \u9700\u8981\u79d1\u5b66\u4e0a\u7f51\uff0cCKA\u8003\u8bd5\u65f6\u9700\u8981\u68c0\u67e5\u4e0a\u6700\u4f4e500Kbps\u4e0b\u8f7d\u548c256Kbps\u4e0a\u4f20\uff0c \u7b14\u8bb0\u672c\u7535\u8111\u81ea\u5e26\u6444\u50cf\u5934\u5373\u53ef \u9700\u8981\u5b89\u88c5 Innovative Exams Screensharing \u8c37\u6b4c\u6d4f\u89c8\u5668\u63d2\u4ef6 \u591a\u505aCKA\u771f\u9898\uff0c\u57fa\u672c\u4e0a\u8003\u8bd5\u5c31\u662f\u53c2\u6570\u6709\u53d8\u52a8\uff0c\u90fd\u662f\u539f\u9898 \u91cd\u70b9\u91cd\u70b9\u91cd\u70b9 \u8003\u8bd5\u65f6\u53ef\u4ee5\u770b\u5b98\u65b9\u6587\u6863\uff0c\u53ef\u4ee5\u63d0\u524d\u505a\u6210\u4e66\u7b7e,\u8003\u8bd5\u65f6\u90fd\u6709\u76f4\u63a5\u70b9\u51fb\u5373\u53ef\uff0c\u53ea\u80fd\u6253\u5f00\u4e00\u4e2a\u9875\u7b7e\u3002 \u82f1\u6587\u8003\u8bd5\u8bd5\u9898 \u00b6 \u8bd5\u98981 \u00b6 **Set configuration context $kubectl config use-context k8s. Monitor the logs of Pod foobar and Extract log lines corresponding to error unable-to-access-website . Write them to /opt/KULM00612/foobar.** \u7ffb\u8bd1\uff1a\u8bbe\u7f6e\u914d\u7f6e\u4e0a\u4e0b\u6587 $kubectl config use context k8s\uff0c\u76d1\u63a7Pod foobar\u7684\u65e5\u5fd7\uff0c\u5e76\u63d0\u53d6\u9519\u8bef\u201cunable-to-access-website\u201d\u5bf9\u5e94\u7684\u65e5\u5fd7\u884c\u3002\u628a\u5b83\u4eec\u5199\u5230/opt/KULM00612/foobar\u3002 \u89e3\u6790\uff1a\u5c31\u662f\u770b\u4e0b\u4e00\u4e2apod\u4e2d\u7684\u65e5\u5fd7\uff0c\u628a\u6ee1\u8db3\u6761\u4ef6\u7684\u65e5\u5fd7\u884c\u4fdd\u5b58\u5728\u67d0\u4e00\u6587\u4ef6\u4e2d \u89e3\u7b54\uff1a \u9996\u5148\u5207\u6362\u4e0b\u4e0a\u4e0b\u6587k8s\u73af\u5883 kubectl config use context k8s [ root@k8s-node1 ~ ] # kubectl logs nginx|grep \"unable-to-access-website\" > /tmp/task.txt \u8bd5\u98982 \u00b6 ** Set configuration context $kubectl config use - context k8s . List all PVs sorted by capacity , saving the full kubectl output to /opt/ KUCC0006 / my_volumes . Use kubectl own functionally for sorting the output , and do not manipulate it any further ** \u7ffb\u8bd1\uff1a\u8bbe\u7f6e\u914d\u7f6e\u4e0a\u4e0b\u6587 $kubectl config use context k8s \u3002\u5217\u51fa\u6309\u5bb9\u91cf\u6392\u5e8f\u7684\u6240\u6709 PV \uff0c\u5c06\u5b8c\u6574\u7684 kubectl\u8f93\u51fa\u4fdd\u5b58\u5230 / opt / KUCC0006 / my_volumes \u3002\u5728\u529f\u80fd\u4e0a\u4f7f\u7528 kubectl \u672c\u8eab\u5bf9\u8f93\u51fa\u8fdb\u884c\u6392\u5e8f\uff0c\u4e0d\u8981\u518d\u5bf9\u5176\u8fdb\u884c\u4efb\u4f55\u64cd\u4f5c \u89e3\u6790\uff1a pv\u6392\u5e8f \u89e3\u7b54\uff1a [ root @k8s - node1 pv ] # kubectl get pv -A --sort-by={.spec.capacity.sotrge} > /opt/cka-tak.txt \u7136\u540e\u5728\u9a8c\u8bc1\u4e0b\u662f\u5426\u6587\u4ef6\u4e2d\u6709\u6570\u636e\u3002 [ root @k8s - node1 pv ] # cat /opt/cka-tak.txt NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfspv1 1 Gi RWO Recycle Available mynfs 4 m12s [ root @k8s - node1 pv ] # \u8bd5\u98983 \u00b6 **Set configuration context $kubectl config use-context k8s. Ensure a single instance of Pod nginx is running on each node of the Kubernetes cluster where nginx also represents the image name which has to be used. Do no override any taints currently in place. Use Daemonset to complete this task and use ds.kusc00612 as Daemonset name** \u7ffb\u8bd1\uff1a\u8bbe\u7f6e\u914d\u7f6e\u4e0a\u4e0b\u6587 $kubectl config use context k8s\u3002\u786e\u4fdd\u5728Kubernetes\u96c6\u7fa4\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884cPod nginx\u7684\u5355\u4e2a\u5b9e\u4f8b\uff0c\u5176\u4e2dnginx\u8fd8\u8868\u793a\u5fc5\u987b\u4f7f\u7528\u7684\u6620\u50cf\u540d\u79f0\u3002\u4e0d\u8981\u8986\u76d6\u5f53\u524d\u5b58\u5728\u7684\u4efb\u4f55\u6c61\u70b9\u3002\u4f7f\u7528Daemonset\u5b8c\u6210\u6b64\u4efb\u52a1\u5e76\u4f7f\u7528ds.kusc00612\u4f5c\u4e3a\u5b88\u62a4\u8fdb\u7a0b\u540d\u79f0 \u89e3\u7b54\uff1a \u53ef\u53c2\u7167\u5b98\u7f51\u7684Daemonset\u8fdb\u884c\u4fee\u6539 https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/ [ root@k8s-node1 ~ ] # vim Daemonset.ayml apiVersion: apps/v1 kind: DaemonSet metadata: name: ds.kusc00612 labels: k8s-app: ds.kusc00612 spec: selector: matchLabels: name: ds.kusc00612 template: metadata: labels: name: ds.kusc00612 spec: containers: - name: nginx image: nginx apply\u52a0\u8f7dyaml\u6587\u4ef6\u9a8c\u8bc1\u662f\u5426\u6b63\u5e38 [ root@k8s-node1 ~ ] # kubectl apply -f Daemonset.ayml daemonset.apps/ds.kusc00612 created [ root@k8s-node1 ~ ] # kubectl get daemonset NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ds.kusc00612 1 1 1 1 1 <none> 2m14s \u8bd5\u98984 \u00b6 Set configuration context $ kubectl config use - context k8s Perform the following tasks : Add an init container to lumpy - koala ( which has been defined in spec file / opt / kucc00100 / pod - specKUCC00612 . yaml ). The init container should create an empty file named / workdir / calm . txt . If / workdir / calm . txt is not detected , the Pod should exit . Once the spec file has been updated with the init container definition , the Pod should be created \u7ffb\u8bd1\uff1a\u6267\u884c\u4ee5\u4e0b\u4efb\u52a1\uff1a\u5c06 init \u5bb9\u5668\u6dfb\u52a0\u5230 lumpy - koala \uff08\u5df2\u5728\u6587\u4ef6 / opt / kucc00100 / pod - specKUCC00612 . yaml\u4e2d\u5b9a\u4e49 \uff09\u3002 init \u5bb9\u5668\u5e94\u8be5\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a / workdir / calm . txt\u7684\u7a7a\u6587\u4ef6 . \u5982\u679c / workdir / calm . txt\u672a\u68c0\u6d4b\u5230 \uff0c Pod \u5e94\u9000\u51fa\u3002\u4e00\u65e6\u7528 init \u5bb9\u5668\u5b9a\u4e49\u66f4\u65b0\u4e86 spec \u6587\u4ef6\uff0c\u5c31\u5e94\u8be5\u521b\u5efa Pod \u89e3\u6790\uff1a\u8fd9\u9053\u9898\u5728 / opt / kucc00100 / pod - specKUCC00612 . yaml\u8def\u5f84\u4e0b\u5df2\u7ecf\u6709\u5199\u597d\u7684Yaml\u4e86 \uff0c\u4f46\u662f\u8fd8\u672a\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u8be5\u5bf9\u8c61\u3002\u6240\u4ee5\u4f60\u4e0a\u53bb\u6700\u597d\u5148 kubectl get po | grep pod \u540d\u5b57\u3002\u53d1\u73b0\u96c6\u7fa4\u8fd8\u6ca1\u6709\u8be5 pod \u3002\u6240\u4ee5\u4f60\u5c31\u5148\u6539\u4e0b\u8fd9\u4e2a Yaml , \u7136\u540e apply . \u5148\u521b\u5efaInitcontainer , \u7136\u540e\u5728\u91cc\u9762\u521b\u5efa\u6587\u4ef6\uff0c / workdir \u76ee\u5f55\u660e\u663e\u662f\u4e2a\u6302\u8f7d\u8fdb\u5ea6\u7684\u76ee\u5f55\uff0c\u9898\u76ee\u6ca1\u89c4\u5b9a\uff0c\u4f60\u5c31\u5b9a\u4e49 empDir \u7c7b\u578b\u3002\u8fd9\u8fb9\u8fd8\u8981\u7528\u5230 liveness \u68c0\u67e5 \u89e3\u7b54\uff1a \u53ef\u53c2\u7167\u5b98\u7f51 : https : //kubernetes.io/docs/concepts/workloads/pods/init-containers/#using-init-containers https : //kubernetes.io/docs/concepts/storage/volumes/#emptydir https : //kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ [ root@ k8s - node1 ~] # vim init - pod . yaml apiVersion : v1 kind : Pod metadata : name : myapp - pod labels : app : myapp spec : containers : - name : myapp - container image : busybox : 1.28 command : [ ' sh ' , ' - c ' , ' echo The app is running ! && sleep 3600 ' ] initContainers : - name : init - myservice image : busybox : 1.28 command : [ ' sh ' , ' - c ' , \"touch /workdir/calm.txt\" ] volumeMounts : - mountPath : / workdir name : cache - volume volumes : - name : cache - volume emptyDir : {} [ root@ k8s - node1 ~] # kubectl apply - f init - pod . yaml pod / myapp - pod created \u8bd5\u98985 \u00b6 Set configuration context $kubectl config use - context k8s . Create a pod named kucc6 with a single container for each of the following images running inside ( there may be between 1 and 4 images specified ) :nginx + redis + memcached + consul \u3002 \u7ffb\u8bd1\uff1a\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a kucc6\u7684pod \uff0c\u5176\u4e2d\u5305\u542b\u8fd0\u884c\u5728\u5176\u4e2d\u7684\u4ee5\u4e0b\u6620\u50cf\u7684\u5355\u4e2a\u5bb9\u5668\uff08\u53ef\u80fd\u4f1a\u6307\u5b9a 1 \u5230 4 \u4e2a\u6620\u50cf\uff09\uff1a nginx + redis + memcached + consur \u89e3\u7b54\uff1a \u53ef\u53c2\u7167\u5b98\u7f51 :https:/ / kubernetes . io / zh / docs / concepts / scheduling - eviction / assign - pod - node / [ root @k8s - node1 ~] # vim images.yaml apiVersion : v1 kind : Pod metadata : name : kucc6 labels : env : kucc6 spec : containers : - name : nginx image : nginx - name : redis image : redis - name : memcached image : memcached - name : consul image : consul \u9a8c\u8bc1\uff1a [ root @k8s - node1 ~] # kubectl apply -f images.yaml pod / kucc6 created \u8bd5\u98986 \u00b6 Set configuration context $kubectl config use - context k8s Schedule a Pod as follows : Name : nginxkusc00612 Image : nginx Node selector : disk = ssd \u7ffb\u8bd1\uff1a\u521b\u5efa Pod \uff0c\u540d\u5b57\u4e3a nginx \uff0c\u955c\u50cf\u4e3a nginx \uff0c\u90e8\u7f72\u5230 label disk = ssd\u7684node\u4e0a \u89e3\u6790\uff1a pod\u8c03\u5ea6\u5230\u6307\u5b9a\u8282\u70b9 \uff0c Nodeselector \u89e3\u7b54\uff1a \u5b98\u7f51\u641c\u7d22 nodeselector\u627eyaml\u6587\u4ef6 , \u53c2\u8003\u6587\u6863 https : // kubernetes . io / zh / docs / concepts / scheduling - eviction / assign - pod - node / #\u9996\u5148\u67e5\u8be2\u4e0bnode\u8282\u70b9\u4e0a\u9762\u7684\u6807\u7b7e\uff0c\u662f\u5426\u6709\u5bf9\u5e94\u5f97lable\u6807\u7b7e\u7136\u540e\u5728\u64cd\u4f5c [ root @k8s - node1 ~] # kubectl get nodes --show-labels [ root @k8s - node1 ~] # vim nodeSelector.yaml apiVersion : v1 kind : Pod metadata : name : nginxkusc00612 labels : env : test spec : containers : - name : nginx image : nginx imagePullPolicy : IfNotPresent nodeSelector : disktype : ssd [ root @k8s - node1 ~] # kubectl apply -f nodeSelector.yaml pod / nginxkusc00612 created [ root @k8s - node1 ~] # kubectl get pods nginxkusc00612 NAME READY STATUS RESTARTS AGE nginxkusc00612 1 / 1 Running 0 3 m \u8bd5\u98987 \u00b6 Set configuration context $kubectl config use - context k8s . Create a deployment as follows : Name : nginxapp Using container nginx with version 1 . 11 . 9 - alpine . The deployment should contain 3 replicas . Next , deploy the app with new version 1 . 12 . 0 - alpine by performing a rolling update and record that update . Finally , rollback that update to the previous version 1 . 11 . 9 - alpine . \u89e3\u6790\uff1a\u90e8\u7f72 deploy , \u7136\u540e\u4fee\u6539\u8fdb\u955c\u50cf\uff08\u6eda\u52a8\u66f4\u65b0\uff09\uff0c\u7136\u540e\u56de\u6eda\u4e0a\u4e00\u7248\u672c \u5b98\u7f51\u641c\u7d22 Deployment \u89e3\u7b54\uff1a https : // kubernetes . io / zh / docs / concepts / workloads / controllers / deployment / [ root @k8s - node1 ~] # kubectl create deployment nginxapp --image=nginx:1.11.9-alpine deployment . apps / nginxapp created [ root @k8s - node1 ~] # kubectl scale deployment nginxapp --replicas=3 deployment . apps / nginxapp scaled [ root @k8s - node1 ~] # kubectl set image deployment/nginxapp nginx=nginx:1.12.0-alpine --record=true deployment . apps / nginxapp image updated [ root @k8s - node1 ~] # kubectl rollout undo deployment.apps/nginxapp \u8bd5\u98988 \u00b6 Set configuration context $kubectl config use - context k8s Create and configure the service front - endservice so it \u2019 s accessible through NodePort / ClusterIp and routes to the existing pod named nginxkusc00612 \u89e3\u6790\uff1a\u521b\u5efa service \uff0c\u6307\u5b9a\u540e\u7aef\u5230\u5df2\u6709 pod : nginxkusc00612 \u89e3\u7b54\uff1a https : // kubernetes . io / zh / docs / tasks / access - application - cluster / connecting - frontend - backend / #\u7531\u4e8e\u6ca1\u6709\u9a8c\u8bc1\u6210\u529f\uff0c\u5f85\u540e\u7eed\u5728\u9a8c\u8bc1\u3002\u627e\u4e0d\u5230Endpoints [ root @k8s - node1 ~] # cat service.yaml apiVersion : v1 kind : Service metadata : name : pod - service spec : selector : app : front - end type : NodePort ports : - protocol : TCP port : 80 targetPort : http \u8bd5\u98989 \u00b6 Set configuration context $kubectl config use-context k8s Create a Pod as follows: Name: jenkins Using image: jenkins In a new Kubernetes namespace named pro-test \u89e3\u6790\uff1a\u5728\u65b0\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efajenkins\u7684pod \u89e3\u7b54\uff1a \u9996\u5148\u67e5\u8be2\u662f\u5426\u6709pro-test\u540d\u79f0\u7a7a\u95f4 [root@k8s-node1 ~]# kubectl get namespaces pro-test \u7136\u540e\u5728\u521b\u5efa [root@k8s-node1 ~]# kubectl run jenkins --image=jenkins --namespace=pro-test \u7136\u540e\u5728\u9a8c\u8bc1\uff1a [root@k8s-node1 ~]# kubectl get pods -n pro-test \u8bd5\u989810 \u00b6 Set configuration context $kubectl config use-context k8s Create a deployment spec file that will: Launch 7 replicas of the redis image with the label : app_enb_stage=dev Deployment name: kual00612 Save a copy of this spec file to /opt/KUAL00612/deploy_spec.yaml (or .json) When you are done,clean up(delete) any new k8s API objects that you produced during this task \u7ffb\u8bd1\uff1a\u8bbe\u7f6e\u914d\u7f6e\u4e0a\u4e0b\u6587$kubectl config use context k8s\u521b\u5efa\u4e00\u4e2a\u90e8\u7f72\u89c4\u8303\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5c06\uff1a\u542f\u52a8redis\u6620\u50cf\u76847\u4e2a\u526f\u672c\uff0c\u6807\u7b7e\u4e3a\uff1aapp_enb_stage=dev deployment name:kual00612\u5c06\u6b64\u89c4\u8303\u6587\u4ef6\u7684\u526f\u672c\u4fdd\u5b58\u5230/opt/kual00612/deploy_spec.yaml\uff08\u6216.json\uff09\u5b8c\u6210\u540e\uff0c\u6e05\u7406\uff08\u5220\u9664\uff09\u60a8\u5728\u6b64\u4efb\u52a1\u671f\u95f4\u751f\u6210\u7684\u4efb\u4f55\u65b0\u7684k8sapi\u5bf9\u8c61 \u89e3\u6790\uff1a\u521b\u5efa7\u526f\u672c\u7684redis\u7684deploy\uff0c\u6307\u660e\u6807\u7b7e\uff0c\u7136\u540e\u628ayaml\u4fdd\u5b58\u5728\u6307\u5b9a\u4f4d\u7f6e \u89e3\u7b54\uff1a https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ [root@k8s-node1 ~]# vim ReplicaSet.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kual00612 labels: app_enb_stage: dev spec: replicas: 7 selector: matchLabels: app: kual00612 template: metadata: labels: app: kual00612 spec: containers: - name: redis image: redis \u9a8c\u8bc1\u662f\u5426\u6b63\u5e38\uff1a [root@k8s-node1 ~]# kubectl get pods |grep kual00612|grep Running|wc -l \u7136\u540e\u5728\u628ayaml\u590d\u5236\u5230/opt/KUAL00612/deploy_spec.yaml cat ReplicaSet.yaml > /opt/KUAL00612/deploy_spec.yaml \u8bd5\u989811 \u00b6 Set configuration context $kubectl config use - context k8s Create a file / opt / KUCC00612 / kucc00612 . txt that lists all pods that implement Service foo in Namespace production . The format of the file should be one pod name per line . \u89e3\u6790\uff1a\u6ee1\u8db3 foo service\u9009\u62e9\u89c4\u5219\u7684pod \uff0c\u5e76\u628a\u540d\u5b57\u5199\u5165\u67d0\u4e2a\u6587\u4ef6 kubecet get svc - n production -- show - lables | grep foo kubectl get pods - nccod45 - l name = foo | grep - v NAME | awk '{print $1}' >> / opt / KUCC00302 / kucc00302 . txt \u8bd5\u989812 \u00b6 Set configuration context $kubectl config use - context k8s Create a Kubernetes Secret as follows: Name: super - secret credential: blob , Create a Pod named pod - secrets - via - file using the redis image which mounts a secret named super - secret at / secrets . Create a second Pod named pod - secretsvia - env using the redis image , which exports credential as \u7ffb\u8bd1\uff1a\u521b\u5efa\u4e00\u4e2a Kubernetes Secret \uff0c\u5982\u4e0b\u6240\u793a\uff1a Name: super Secret credential: blob \uff0c\u4f7f\u7528 redis\u6620\u50cf\u521b\u5efa\u4e00\u4e2a\u540d\u4e3aPod secrets\u7684Pod \uff0c\u8be5\u6620\u50cf\u5728 / secrets\u5904\u6302\u8f7d\u4e00\u4e2a\u540d\u4e3asuper Secret\u7684\u673a\u5bc6 \u3002\u4f7f\u7528 redis\u6620\u50cf\u521b\u5efa\u7b2c\u4e8c\u4e2a\u540d\u4e3aPod secretsvia env\u7684Pod \uff0c\u5b83\u5c06\u51ed\u8bc1\u5bfc\u51fa\u4e3a\u51ed\u8bc1 \u89e3\u6790\uff1a\u521b\u5efa secret \uff0c\u5e76\u5728 pod\u4e2d\u901a\u8fc7Volume\u548c\u73af\u5883\u53d8\u91cf\u4f7f\u7528\u8be5secret \u89e3\u7b54\uff1a https: //kubernetes.io/zh/docs/concepts/configuration/secret/ [ root @k8s - node1 ~ ] # echo blob | base64 YmxvYgo = \u8bd5\u989813 \u00b6 Set configuration context $kubectl config use-context k8s Create a pod as follows: Name: nonpersistent-redis Container image: redis Named-volume with name: cache-control Mount path : /data/redis It should launch in the pre-prod namespace and the volume MUST NOT be persistent. \u89e3\u6790\uff1a\u521b\u5efa\u4e00\u4e2apod\uff0c\u5e76\u6302\u8f7dvolume \u89e3\u7b54\uff1a https://kubernetes.io/zh/docs/concepts/storage/volumes/ [root@k8s-node1 ~]# cat volumes.yaml apiVersion: v1 kind: Pod metadata: name: nonpersistent-redis spec: containers: - image: redis name: nonpersistent-redis volumeMounts: - mountPath: /data/redis name: cache-control volumes: - name: cache-control emptyDir: {} \u8bd5\u989814 \u00b6 Set configuration context $kubectl config use - context k8s Scale the deployment webserver to 6 pods \u89e3\u6790\uff1a\u6269\u7f29\u5bb9 \u89e3\u7b54\uff1a https : // kubernetes . io / docs / reference / generated / kubectl / kubectl - commands #scale [ root @k8s - node1 ~] # kubectl scale --replicas=6 webserver \u8bd5\u989815 \u00b6 Set configuration context $kubectl config use - context k8s Check to see how many nodes are ready ( not including nodes tainted NoSchedule ) and write the number to / opt / nodenum . \u89e3\u6790\uff1a\u6709\u591a\u5c11\u8282\u70b9\u662f ready\u72b6\u6001\u7684 \uff0c\u4e0d\u5305\u542b\u88ab\u6253\u4e86 NoSchedule\u6c61\u70b9\u7684\u8282\u70b9 kubectl describe nodes ` kubectl get nodes | grep Ready | awk '{print $1}' ` | grep Taints | grep - vc NoSchedule > / opt / nodenum \u8bd5\u989816 \u00b6 Set configuration context $kubectl config use - context k8s Create a deployment as follows : Name : nginxdns Exposed via a service : nginx - dns Ensure that the service & pod are accessible via their respective DNS records The container ( s ) within any Pod ( s ) running as a part of this deployment should use the nginx image . Next , use the utility nslookup to look up the DNS records of the service & pod and write the output to /opt/se rvice . dns and /opt/ pod . dns respectively . Ensure you use the busybox : 1 . 28 image ( or earlier ) for any testing , an the latest release has an upstream bug which impacts the use of nslookup \u89e3\u6790\uff1a\u521b\u5efa service\u548cdeployment \uff0c\u7136\u540e\u89e3\u6790 service\u7684dns\u548cpod\u7684dns \uff0c\u5e76\u628a\u89e3\u6790\u8bb0\u5f55\u4fdd\u5b58\u5230\u6307\u5b9a\u6587\u4ef6 \u89e3\u7b54\uff1a \u53c2\u8003\u7f51\u5740 :https:/ / kubernetes . io / docs / tasks / access - application - cluster / connecting - frontend - backend / https : // kubernetes . io / zh / docs / concepts / workloads / pods / init - containers / [ root @k8s - node1 dns ] # cat deployment.yaml apiVersion : apps / v1 kind : Deployment metadata : name : backend spec : selector : matchLabels : app : nginxdns replicas : 1 template : metadata : labels : app : nginxdns spec : containers : - name : nginx image : nginx ports : - name : http containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginxdns spec : selector : app : nginxdns ports : - protocol : TCP port : 80 targetPort : http --- apiVersion : v1 kind : Pod metadata : name : busybox - test labels : app : busybox - test spec : containers : - name : myapp - container image : busybox : 1 . 28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] [ root @k8s - node1 dns ] # kubectl exec -ti busybox-test -- nslookup nginxdns #\u89e3\u6790svc\u5730\u5740 >/opt/service.dns kubectl exec - ti busybox - test -- nslookup 10 . 244 . 1 . 52 > /opt/ pod . dns #Pod ip\u89e3\u6790 \u8bd5\u989817 \u00b6 No configuration context change required for this item Create a snapshot of the etcd instance running at https : // 127 . 0 . 0 . 1 : 2379 saving the snapshot to the file path /data/ backup / etcd - snapshot . db The etcd instance is running etcd version 3 . 2 . 18 The following TLS certificates / key are supplied for connecting to the server with etcdctl CA certificate : /opt/ KUCM0612 / ca . crt Client certificate : /opt/ KUCM00612 / etcdclient . crt Client key : /opt/ KUCM00612 / etcd - client . key \u89e3\u7b54\uff1a https : // kubernetes . io / zh / docs / tasks / administer - cluster / configure - upgrade - etcd / \u5907\u4efd etcd ETCDCTL_API = 3 etcdctl -- endpoints = https : // 127 . 0 . 0 . 1 : 2379 \\ -- cacert = /etc/ kubernetes / pki / etcd / ca . crt -- cert = /etc/ kubernetes / pki / etcd / peer . crt -- key = /etc/ kubernetes / pki / etcd / peer . key \\ snapshot save /etc/ kubernetes / pki / etcd / etcd - snapshot - test . db \u9a8c\u8bc1 etcd TCDCTL_API = 3 etcdctl -- write - out = table snapshot status /etc/ kubernetes / pki / etcd / etcd - snapshot - test . db \u8fd8\u539f etcd ETCDCTL_API = 3 etcdctl -- endpoints = https : // 127 . 0 . 0 . 1 : 2379 snapshot restore \\ -- cacert = /etc/ kubernetes / pki / etcd / ca . crt -- cert = /etc/ kubernetes / pki / etcd / peer . crt -- key = /etc/ kubernetes / pki / etcd / peer . key \\ /etc/ kubernetes / pki / etcd / etcd - snapshot - test . db \u7136\u540e\u5728\u91cd\u542f\u4e0b etcd\u4e4b\u540e \uff0c\u5728\u770b\u4e0b nodes\u548cpods \u8bd5\u989818 \u00b6 Set configuration context $kubectl config use - context ek8s Set the node labelled with name = ek8s - node - 1 as unavailable and reschedule all the pods running on it . \u89e3\u6790\uff1a\u5c06\u6807\u7b7e\u672a name = ek8s - node - 1 \u8bbe\u7f6e\u6210\u4e0d\u53ef\u7528\u4e14\u628a\u8fd9\u4e2a\u8282\u70b9\u4e0a\u9762\u7684 pod\u8c03\u5ea6\u5230\u5176\u4ed6\u8282\u70b9\u4e0a\u53bb \u3002\u5176\u5b9e\u5c31\u662f\u4f7f\u7528 kubectl drain\u547d\u4ee4 kubectl cordon ek8s - node - 1 #\u5148\u9694\u79bb kubectl drain ek8s - node - 1 -- ignore - daemonsets -- delete - local - data -- force \u5b8c\u6210\u540e\u4e00\u5b9a\u8981\u901a\u8fc7 get nodes \u52a0\u4ee5\u786e\u8ba4 \u8bd5\u989819 \u00b6 Set configuration context $kubectl config use - context wk8s A Kubernetes worker node , labelled with name = wk8s - node - 0 is in state NotReady . Investigate why this is the case , and perform any appropriate steps to bring the node to a Ready state , Ensuring that any changes are made permanent . Hints: You can ssh to the failed node using $ssh wk8s - node - 0. You can assume elevated privileges on the node with the following command $sudo - i \u9898\u76ee\u89e3\u6790\uff1a wk8s - node - 0 \u662f NotReady\u72b6\u6001 \uff0c\u4f60\u9700\u8981\u5904\u7406\u4e0b\uff0c\u4f7f\u5176\u53d8\u4e3a ready \uff0c\u522b\u66f4\u6539\u9700\u8981\u6c38\u4e45\u6027 \u8bd5\u989820 \u00b6 21 \u3001 Set configuration context $kubectl config use - context wk8s Configure the kubelet system managed service , on the node labelled with name = wk8s - node - 1 , to Launch a Pod containing a single container of image nginx named myservice automatically . Any spec files required should be placed in the / etc / kubernetes / manifests directory on the node . Hints: You can ssh to the failed node using $ssh wk8snode - 1. You can assume elevated privileges on the node with the following command $sudo - i \u8bd5\u989821 \u00b6 \u8fd9\u9898\u662f\u7ed9\u4f60\u4e24\u4e2a\u8282\u70b9 \uff0c master1\u548cnode1 \uff0c \u548c\u4e00\u4e2aadmin . conf\u6587\u4ef6 \uff0c \u7136\u540e\u8ba9\u4f60\u5728\u8fd9\u4e24\u4e2a\u8282\u70b9\u4e0a\u90e8\u7f72\u96c6\u7fa4 \u3002 \u89e3\u6790 \uff1a \u8bd5\u989822 \u00b6 Set configuration context $kubectl configuse - context bk8s Given a partially - functioning Kubernetes cluser , identify symptoms of failure on the cluter . Determine the node , the failing service and take actions to bring up the failed service and restore the health of the cluser . Ensure that any changes are made permanently . The worker node in this cluster is labelled with name = bk8s - node - 0 Hints: You can ssh to the relevant nodes using $ssh $ ( NODE ) where $ ( NODE ) is one of bk8s - master - 0 or bk8s - node - 0. You can assume elevated privileges on any node in the cluster with the following command: $ sudo - i . \u89e3\u6790\uff1a\u8fd9\u9898\u7684\u610f\u601d\u662f\uff0c\u6709\u4e2a\u96c6\u7fa4\u90e8\u5206\u529f\u80fd\u51fa\u73b0\u95ee\u9898\uff0c\u9700\u8981\u4f60\u53bb\u4fee\u4e00\u4e0b\uff0c\u9700\u8981\u662f\u6c38\u4e45\u6027\u7684\u4fee\u590d\u3002 \u8bd5\u989823 \u00b6 Set configuration context $kubectl config use - context hk8s Create a persistent volume with name appconfig of capacity 1 Gi and access mode ReadWriteMany . The type of volume is hostPath and its locationis / srv / app - config CKA\u6a21\u62df\u9898 \u00b6 \u8bd5\u98981 \u00b6 Task weight: 1 % You have access to multiple clusters from your main terminal through kubectl contexts. Write all those context names into /opt/course/1/contexts. Next write a command to display the current context into /opt/course/1/context_default_kubectl.sh, the command should use kubectl. Finally write a second command doing the same thing into /opt/course/1/context_default_no_kubectl.sh, but without the use of kubectl. \u8bd5\u98982 \u00b6 Task weight: 3 % Use context: kubectl config use-context k8s-c1-H Create a single Pod of image httpd:2.4.41-alpine in Namespace default. The Pod should be named pod1 and the container should be named pod1-container. This Pod should only be scheduled on a master node, do not add new labels any nodes. Shortly write the reason on why Pods are by default not scheduled on master nodes into /opt/course/2/master_schedule_reason . \u8bd5\u98983 \u00b6 Use context: kubectl config use-context k8s-c1-H There are two Pods named o3db-* in Namespace project-c13. C13 management asked you to scale the Pods down to one replica to save resources. Record the action. \u8bd5\u98984 \u00b6 Task weight: 1% Use context: kubectl config use-context k8s-c1-H There are two Pods named o3db-* in Namespace project-c13. C13 management asked you to scale the Pods down to one replica to save resources. Record the action. \u8bd5\u98985 \u00b6 Task weight: 4 % Use context: kubectl config use - context k8s - c1 - H Do the following in Namespace default . Create a single Pod named ready - if - service - ready of image nginx: 1.16 . 1 - alpine . Configure a LivenessProbe which simply runs true . Also configure a ReadinessProbe which does check if the url http: //service-am-i-ready:80 is reachable, you can use wget -T2 -O- http://service-am-i-ready:80 for this. Start the Pod and confirm it isn't ready because of the ReadinessProbe. Create a second Pod named am - i - ready of image nginx: 1.16 . 1 - alpine with label id: cross - server - ready . The already existing Service service - am - i - ready should now have that second Pod as endpoint . Now the first Pod should be in ready state , confirm that . \u8bd5\u98986 \u00b6 Task weight: 1 % Use context: kubectl config use-context k8s-c1-H There are various Pods in all namespaces. Write a command into /opt/course/5/find_pods.sh which lists all Pods sorted by their AGE ( metadata.creationTimestamp ) . Write a second command into /opt/course/5/find_pods_uid.sh which lists all Pods sorted by field metadata.uid. Use kubectl sorting for both commands. \u8bd5\u98987 \u00b6 Task weight : 8 % Use context : kubectl config use - context k8s - c1 - H Create a new PersistentVolume named safari - pv . It should have a capacity of 2 Gi , accessMode ReadWriteOnce , hostPath / Volumes / Data and no storageClassName defined . Next create a new PersistentVolumeClaim in Namespace project - tiger named safari - pvc . It should request 2 Gi storage , accessMode ReadWriteOnce and should not define a storageClassName . The PVC should bound to the PV correctly . Finally create a new Deployment safari in Namespace project - tiger which mounts that volume at / tmp / safari - data . The Pods of that Deployment should be of image httpd : 2.4.41 - alpine . \u8bd5\u98988 \u00b6 Task weight : 2 % Use context : kubectl config use - context k8s - c1 - H Ssh into the master node with ssh cluster1 - master1 . Check how the master components kubelet , kube - apiserver , kube - scheduler , kube - controller - manager and etcd are started / installed on the master node . Also find out the name of the DNS application and how it ' s started / installed on the master node . Write your findings into file / opt / course / 8 / master - components . txt . The file should be structured like : # / opt / course / 8 / master - components . txt kubelet : [ TYPE ] kube - apiserver : [ TYPE ] kube - scheduler : [ TYPE ] kube - controller - manager : [ TYPE ] etcd : [ TYPE ] dns : [ TYPE ] [ NAME ] Choices of [ TYPE ] are : not - installed , process , static - pod , pod \u8bd5\u98989 \u00b6 Task weight : 5 % Use context : kubectl config use - context k8s - c2 - AC Ssh into the master node with ssh cluster2 - master1 . Temporarily stop the kube - scheduler , this means in a way that you can start it again afterwards . Create a single Pod named manual - schedule of image httpd : 2.4 - alpine , confirm its started but not scheduled on any node . Now you ' re the scheduler and have all its power , manually schedule that Pod on node cluster2 - master1 . Make sure it ' s running . Start the kube - scheduler again and confirm its running correctly by creating a second Pod named manual - schedule2 of image httpd : 2.4 - alpine and check if it ' s running on cluster2 - worker1 . \u8bd5\u989810 \u00b6 Use context: kubectl config use-context k8s-c1-H Create a new ServiceAccount processor in Namespace project-hamster. Create a Role and RoleBinding, both named processor as well. These should allow the new SA to only create Secrets and ConfigMaps in that Namespace. \u8bd5\u989811 \u00b6 Use context: kubectl config use - context k8s - c1 - H Use Namespace project - tiger for the following . Create a DaemonSet named ds - important with image httpd: 2.4 - alpine and labels id = ds - important and uuid = 18426 a0b - 5 f59 - 4e10 - 923 f - c0e078e82462 . The Pods it creates should request 10 millicore cpu and 10 megabytes memory . The Pods of that DaemonSet should run on all nodes . \u8bd5\u989812 \u00b6 Use context : kubectl config use - context k8s - c1 - H Use Namespace project - tiger for the following . Create a Deployment named deploy - important with label id = very - important ( the pods should also have this label ) and 3 replicas . It should contain two containers , the first named container1 with image nginx : 1.17.6 - alpine and the second one named container2 with image kubernetes / pause . There should be only ever one Pod of that Deployment running on one worker node . We have two worker nodes : cluster1 - worker1 and cluster1 - worker2 . Because the Deployment has three replicas the result should be that on both nodes one Pod is running . The third Pod won ' t be scheduled , unless a new worker node will be added . In a way we kind of simulate the behaviour of a DaemonSet here , but using a Deployment and a fixed number of replicas . \u8bd5\u989813 \u00b6 Create a Pod named multi - container - playground in Namespace default with three containers , named c1 , c2 and c3 . There should be a volume attached to that Pod and mounted into every container , but the volume shouldn ' t be persisted or shared with other Pods . Container c1 should be of image nginx : 1.17.6 - alpine and have the name of the node where its Pod is running on value available as environment variable MY_NODE_NAME . Container c2 should be of image busybox : 1.31.1 and write the output of the date command every second in the shared volume into file date . log . You can use while true ; do date >> / your / vol / path / date . log ; sleep 1 ; done for this . Container c3 should be of image busybox : 1.31.1 and constantly write the content of file date . log from the shared volume to stdout . You can use tail - f / your / vol / path / date . log for this . Check the logs of container c3 to confirm correct setup . \u8bd5\u989814 \u00b6 Use context : kubectl config use - context k8s - c1 - H You ' re ask to find out following information about the cluster k8s - c1 - H : How many master nodes are available ? How many worker nodes are available ? What is the Service CIDR ? Which Networking ( or CNI Plugin ) is configured and where is its config file ? Which suffix will static pods have that run on cluster1 - worker1 ? Write your answers into file / opt / course / 14 / cluster - info , structured like this : # / opt / course / 14 / cluster - info 1 : [ ANSWER ] 2 : [ ANSWER ] 3 : [ ANSWER ] 4 : [ ANSWER ] 5 : [ ANSWER ] \u8bd5\u989815 \u00b6 Use context: kubectl config use-context k8s-c2-AC Write a command into /opt/course/15/cluster_events.sh which shows the latest events in the whole cluster, ordered by time. Use kubectl for it. Now kill the kube-proxy Pod running on node cluster2-worker1 and write the events this caused into /opt/course/15/pod_kill.log. Finally kill the containerd container of the kube-proxy Pod on node cluster2-worker1 and write the events into /opt/course/15/container_kill.log. Do you notice differences in the events both actions caused? \u8bd5\u989816 \u00b6 Use context: kubectl config use-context k8s-c1-H Create a new Namespace called cka-master. Write the names of all namespaced Kubernetes resources (like Pod, Secret, ConfigMap...) into /opt/course/16/resources.txt. Find the project-* Namespace with the highest number of Roles defined in it and write its name and amount of Roles into /opt/course/16/crowded-namespace.txt. \u8bd5\u989817 \u00b6 Use context: kubectl config use-context k8s-c1-H In Namespace project-tiger create a Pod named tigers-reunite of image httpd:2.4.41-alpine with labels pod = container and container = pod. Find out on which node the Pod is scheduled. Ssh into that node and find the containerd container belonging to that Pod. Using command crictl: Write the ID of the container and the info.runtimeType into /opt/course/17/pod-container.txt Write the logs of the container into /opt/course/17/pod-container.log \u8bd5\u989818 \u00b6 Use context: kubectl config use-context k8s-c3-CCC There seems to be an issue with the kubelet not running on cluster3-worker1. Fix it and confirm that cluster3 has node cluster3-worker1 available in Ready state afterwards. Schedule a Pod on cluster3-worker1. Write the reason of the is issue into /opt/course/18/reason.txt. \u8bd5\u989819 \u00b6 this task can only be solved if questions 18 or 20 have been successfully implemented and the k8s - c3 - CCC cluster has a functioning worker node Use context : kubectl config use - context k8s - c3 - CCC Do the following in a new Namespace secret . Create a Pod named secret - pod of image busybox : 1.31.1 which should keep running for some time , it should be able to run on master nodes as well . There is an existing Secret located at / opt / course / 19 / secret1 . yaml , create it in the secret Namespace and mount it readonly into the Pod at / tmp / secret1 . Create a new Secret in Namespace secret called secret2 which should contain user = user1 and pass = 1234. These entries should be available inside the Pod ' s container as environment variables APP_USER and APP_PASS . Confirm everything is working . \u8bd5\u989820 \u00b6 Your coworker said node cluster3 - worker2 is running an older Kubernetes version and is not even part of the cluster . Update kubectl and kubeadm to the version that ' s running on cluster3 - master1 . Then add this node to the cluster , you can use kubeadm for this . \u8bd5\u989821 \u00b6 Use context: kubectl config use-context k8s-c3-CCC Create a Static Pod named my-static-pod in Namespace default on cluster3-master1. It should be of image nginx:1.16-alpine and have resource requests for 10m CPU and 20Mi memory. Then create a NodePort Service named static-pod-service which exposes that static Pod on port 80 and check if it has Endpoints and if its reachable through the cluster3-master1 internal IP address. You can connect to the internal node IPs from your main terminal. \u8bd5\u989822 \u00b6 Use context: kubectl config use-context k8s-c2-AC Check how long the kube-apiserver server certificate is valid on cluster2-master1. Do this with openssl or cfssl. Write the exipiration date into /opt/course/22/expiration. Also run the correct kubeadm command to list the expiration dates and confirm both methods show the same date. Write the correct kubeadm command that would renew the apiserver server certificate into /opt/course/22/kubeadm-renew-certs.sh. \u8bd5\u989823 \u00b6 Use context: kubectl config use-context k8s-c2-AC Node cluster2-worker1 has been added to the cluster using kubeadm and TLS bootstrapping. Find the \"Issuer\" and \"Extended Key Usage\" values of the cluster2-worker1: kubelet client certificate, the one used for outgoing connections to the kube-apiserver. kubelet server certificate, the one used for incoming connections from the kube-apiserver. Write the information into file /opt/course/23/certificate-info.txt. Compare the \"Issuer\" and \"Extended Key Usage\" fields of both certificates and make sense of these. \u8bd5\u989824 \u00b6 Use context: kubectl config use-context k8s-c1-H There was a security incident where an intruder was able to access the whole cluster from a single hacked backend Pod. To prevent this create a NetworkPolicy called np-backend in Namespace project-snake. It should allow the backend-* Pods only to: connect to db1-* Pods on port 1111 connect to db2-* Pods on port 2222 Use the app label of Pods in your policy. After implementation, connections from backend-* Pods to vault-* Pods on port 3333 should for example no longer work. \u8bd5\u989825 \u00b6 Use context: kubectl config use - context k8s - c3 - CCC Make a backup of etcd running on cluster3 - master1 and save it on the master node at / tmp / etcd - backup . db . Then create a Pod of your kind in the cluster . Finally restore the backup , confirm the cluster is still working and that the created Pod is no longer with us . \u8bd5\u989826 \u00b6 Use context: kubectl config use-context k8s-c1-H Check all available Pods in the Namespace project-c13 and find the names of those that would probably be terminated first if the Nodes run out of resources ( cpu or memory ) to schedule all Pods. Write the Pod names into /opt/course/e1/pods-not-stable.txt. \u8bd5\u989827 \u00b6 Use context: kubectl config use-context k8s-c1-H There is an existing ServiceAccount secret-reader in Namespace project-hamster. Create a Pod of image curlimages/curl:7.65.3 named tmp-api-contact which uses this ServiceAccount. Make sure the container keeps running. Exec into the Pod and use curl to access the Kubernetes Api of that cluster manually, listing all available secrets. You can ignore insecure https connection. Write the command ( s ) for this into file /opt/course/e4/list-secrets. \u8bd5\u989826 \u00b6 Preview Question 1 Use context : kubectl config use - context k8s - c2 - AC The cluster admin asked you to find out the following information about etcd running on cluster2 - master1 : Server private key location Server certificate expiration date Is client certificate authentication endabled Write these information into / opt / course / p1 / etcd - info . txt Finally you ' re asked to save an etcd snapshot at / etc / etcd - snapshot . db on cluster2 - master1 and display its status . \u8bd5\u989827 \u00b6 Preview Question 2 Use context : kubectl config use - context k8s - c1 - H You 're asked to confirm that kube-proxy is running correctly on all nodes. For this perform the following in Namespace project-hamster: Create a new Pod named p2 - pod with two containers , one of image nginx : 1.21.3 - alpine and one of image busybox : 1.31 . Make sure the busybox container keeps running for some time . Create a new Service named p2 - service which exposes that Pod internally in the cluster on port 3000 -> 80. Find the kube - proxy container on all nodes cluster1 - master1 , cluster1 - worker1 and cluster1 - worker2 and make sure that it 's using iptables. Use command crictl for this. Write the iptables rules of all nodes belonging the created Service p2 - service into file / opt / course / p2 / iptables . txt . Finally delete the Service and confirm that the iptables rules are gone from all nodes . \u8bd5\u989826 \u00b6 Preview Question 3 Use context: kubectl config use-context k8s-c2-AC Create a Pod named check-ip in Namespace default using image httpd:2.4.41-alpine. Expose it on port 80 as a ClusterIP Service named check-ip-service. Remember/output the IP of that Service. Change the Service CIDR to 11 .96.0.0/12 for the cluster. Then create a second Service named check-ip-service2 pointing to the same Pod to check if your settings did take effect. Finally check if the IP of the first Service has changed. CKA\u771f\u547d\u9898 \u00b6 \u6ce8\u610f\u70b9 \u5982\u679c\u9047\u5230etcd\u8fd8\u539f\u7684\u540e\u548c\u5347\u7ea7k8s\u6700\u597d\u8fd9\u9898\u6700\u540e\u5728\u505a\u5427 \u5347\u7ea7k8s\u4e5f\u6700\u540e\u5728\u505a\u5427\uff0c\u907f\u514d\u6574\u4e2ak8s\u5f02\u5e38\uff0c\u5bfc\u81f4\u5176\u4ed6\u7684\u9898\u76ee\u65e0\u6cd5\u8fdb\u884c\u4e86\uff0c\u6211\u7b2c\u4e00\u6b21\u8003\u6302\u4e86\uff0c\u5bfc\u81f4\u540e\u9762\u7684\u9898\u76ee\u65e0\u6cd5\u505a\u3002 \u53c2\u8003\u6587\u7ae0: https://blog.csdn.net/u011127242/category_10823035.html?spm=1001.2014.3001.5482 \u9898\u76ee1 \u9700\u8981\u91cd\u70b9\u8bb0\u5f55 \u521b\u5efa\u540d\u79f0 deployment-clusterrole \u7684 ClusterRole \u8be5\u89d2\u8272\u5177\u5907\u521b\u5efa Deployment\u3001Statefulset\u3001Daemonset \u7684\u6743\u9650 \u5728\u547d\u540d\u7a7a\u95f4 app-team1 \u4e2d\u521b\u5efa\u540d\u79f0\u4e3a cicd-token \u7684 ServiceAccount \u7ed1\u5b9a ClusterRole \u5230 ServiceAccount\uff0c\u4e14\u9650\u5b9a\u547d\u540d\u7a7a\u95f4\u4e3a app-team1 kubectl create ns app-team1 kubectl create serviceaccount cicd-token -n app-team1 kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployment,statefulset,daemonset kubectl -n app-team1 create rolebinding cicd-clusterrole --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token \u9898\u76ee2 \u9700\u8981\u91cd\u70b9\u8bb0\u5f55 \u8bbe\u7f6e ek8s - node -1 \u8282\u70b9\u4e3a\u4e0d\u53ef\u7528 \u91cd\u65b0\u8c03\u5ea6\u8be5\u8282\u70b9\u4e0a\u7684\u6240\u6709 pod kubectl cordon ek8s - node -1 kubectl drain ek8s - node -1 -- ignore - daemonsets -- delete - local - data -- force \u9898\u76ee3 \u5347\u7ea7 master \u8282\u70b9\u4e3a1.20.1 \u5347\u7ea7\u524d\u786e\u4fdddrain master \u8282\u70b9 \u4e0d\u8981\u5347\u7ea7worker node \u3001\u5bb9\u5668 manager\u3001 etcd\u3001 CNI\u63d2\u4ef6\u3001DNS \u7b49\u5185\u5bb9 https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ kubectl get nodes ssh mk8s-master-0 kubectl cordon mk8s-master-0 kubectl drain mk8s-master-0 --ignore-daemonsets apt-mark unhold kubeadm kubectl kubelet apt-get update && apt-get install -y kubeadm = 1 .20.1-00 kubelet = 1 .20.1-00 kubectl = 1 .20.1-00 apt-mark hold kubeadm kubectl kubelet kubeadm upgrade plan kubeadm upgrade apply v1.20.1 --etcd-upgrade = false // kubectl rollout undo deployment coredns -n kube-system ,\u6709\u4e9b\u5927\u4f6c\u5efa\u8baerollout coredns\uff0c\u7b14\u8005\u8003\u8bd5\u7684\u65f6\u5019\u6ca1\u6709rollover kubectl uncordon mk8s-master-0 \u9898\u76ee4 \u5907\u4efd https://127.0.0.1:2379 \u4e0a\u7684 etcd \u6570\u636e\u5230 /var/lib/backup/etcd-snapshot.db \u4f7f\u7528\u4e4b\u524d\u7684\u6587\u4ef6 /data/backup/etcd-snapshot-previous.db \u8fd8\u539f etcd \u4f7f\u7528\u6307\u5b9a\u7684 ca.crt \u3001 etcd-client.crt \u3001etcd-client.key \u5907\u4efdetc,\u9700\u8981\u6307\u5b9a\u8bc1\u4e66 ETCDCTL_API = 3 etcdctl --endpoints = https://127.0.0.1:2379 --cacert = /etc/kubernetes/pki/etcd/ca.crt --cert = /etc/kubernetes/pki/etcd/peer.crt --key = /etc/kubernetes/pki/etcd/peer.key snapshot save /var/lib/bacp/etcd-snapshot.db \u9a8c\u8bc1etcd\u5907\u4efd\u662f\u5426\u6b63\u786e ETCDCTL_API = 3 etcdctl --write-out = table snapshot status etcd-snapshot.db --cacert = /etc/kubernetes/pki/etcd/ca.crt --cert = /etc/kubernetes/pki/etcd/peer.crt --key = /etc/kubernetes/pki/etcd/peer.key \u6062\u590dectd\u96c6\u7fa4 ETCDCTL_API = 3 etcdctl --write-out = table snapshot restore etcd-snapshot.db --cacert = /etc/kubernetes/pki/etcd/ca.crt --cert = /etc/kubernetes/pki/etcd/peer.crt --key = /etc/kubernetes/pki/etcd/peer.key \u9898\u76ee5 \u9700\u8981\u91cd\u70b9\u8bb0\u5f55 \u62f7\u8d1d services-networking/network-policies \u4e2d\u7684\u6848\u4f8b\uff0c\u5220\u6389\u4e0d\u5fc5\u8981\u7684\u90e8\u5206 \u8bbe\u7f6e\u7f51\u7edc\u7b56\u7565\u6240\u5c5e\u7684 ns \u4e3a fubar\uff0c\u7aef\u53e3\u4e3a 80 \u8bbe\u7f6e namespaceSelector \u4e3a\u6e90ns my-app \u7684labels https://kubernetes.io/docs/concepts/services-networking/network-policies/ [ root@k8s-node1 ~ ] # vim NetworkPolicy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-port-from-namespace namespace: fubar spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: my-app-key: my-app-value - podSelector: matchLabels: {} ports: - protocol: TCP port: 80 \u9898\u76ee6 <\u9700\u8981\u91cd\u70b9\u6ce8\u610f\u70b9> \u91cd\u65b0\u914d\u7f6e\u5df2\u6709\u7684 deployment front - end \uff0c\u6dfb\u52a0\u4e00\u4e2a\u540d\u79f0\u4e3a http \u7684\u7aef\u53e3\uff0c\u66b4\u9732 80 / TCP \u521b\u5efa\u540d\u79f0\u4e3a front - end - svc \u7684 service \uff0c\u66b4\u9732\u5bb9\u5668\u7684 http \u7aef\u53e3 \u914d\u7f6e service \u7684\u7c7b\u522b\u4e3a NodePort 1 \uff09 edit front - end \uff0c\u5728 containers \u4e2d\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9 kubectl edit deployment front - end ports : - name : http protocol : TCP containerPort : 80 https : // kubernetes . io / docs / reference / generated / kubectl / kubectl - commands #expose \u53c2\u8003 kubectl expose deployment\uff0c\u7136\u540e\u5728\u4f7f\u7528-h\u67e5\u770b\u5e2e\u52a9\u4fe1\u606f\u3002 2 \uff09 [ root @k8s - node1 ~] # kubectl expose deployment kual00612 --port=80 --target-port=80 --type=NodePort --name=front-end-svc \u9898\u76ee7 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Ingress \u8d44\u6e90\uff0c\u540d\u79f0 ping \uff0c\u547d\u540d\u7a7a\u95f4 ing - internal \u4f7f\u7528 / hello \u8def\u5f84\u66b4\u9732\u670d\u52a1 hello \u7684 5678 \u7aef\u53e3 https : //kubernetes.io/docs/concepts/services-networking/ingress/ [ root@ k8s - node1 ~] # vim ingress . yaml apiVersion : networking . k8s . io / v1 kind : Ingress metadata : name : ping namespace : ing - internal annotations : nginx . ingress . kubernetes . io / rewrite - target : / spec : rules : - http : paths : - path : / hello pathType : Prefix backend : service : name : test port : number : 5678 \u9898\u76ee8 \u6269\u5bb9 deployment guestbook \u4e3a 6\u4e2apod kubectl scale deployment --replicas = 6 guestbook \u6216\u8005\u4f7f\u7528 kubectl edit deployment guestbook #\u627e\u5230replicas \u8bbe\u7f6e\u4e3a6\uff0c\u5728\u4fdd\u5b58 \u9898\u76ee9 \u521b\u5efa pod \u540d\u79f0 nginx - kusc0041 \uff0c\u955c\u50cf nginx \u8c03\u5ea6\u8be5 pod \u5230 disk = ssd \u7684\u8282\u70b9\u4e0a https : // kubernetes . io / docs / concepts / scheduling - eviction / assign - pod - node / apiVersion : v1 kind : Pod metadata : name : nginx - kusc0041 spec : containers : - name : nginx image : nginx nodeSelector : disk : ssd \u9898\u76ee10 \u68c0\u67e5\u6709\u591a\u5c11\u8282\u70b9\u5df2\u51c6\u5907\u5c31\u7eea\uff08\u4e0d\u5305\u62ec nodes tainted Noschedule \uff09\uff0c\u5e76\u5c06\u7f16\u53f7\u5199\u5165 / opt / kusco0402 / kusco0402 . txt [ root @k8s - node1 ~] # kubectl get nodes|grep -v NAME|wc -l 2 [ root @k8s - node1 ~] # kubectl describe nodes |grep NoSchedule|wc -l 0 pods\u6570\u51cf\u53bbNoSchedule\u67e5\u8be2\u7684\u6570 \uff0c\u5199\u5165\u5230\u6307\u5b9a\u6587\u4ef6\u4e2d echo 2 > /opt/ kusco0402 / kusco0402 . txt \u9898\u76ee11 \u521b\u5efa\u540d\u79f0\u4e3a kucc1 \u7684 pod pod \u4e2d\u8fd0\u884c nginx \u548c redis \u4e24\u4e2a\u793a\u4f8b [ root @k8s - node1 ~] # more create-pods.yaml apiVersion : v1 kind : Pod metadata : name : nginx - to - redis spec : containers : - name : redis image : redis - name : nginx image : nginx \u9898\u76ee12 \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a app - config \u7684 PV \uff0c PV\u7684\u5bb9\u91cf\u4e3a2Gi \uff0c\u8bbf\u95ee\u6a21\u5f0f\u4e3a ReadWriteMany \uff0c volume \u7684\u7c7b\u578b\u4e3a hostPath pv \u6620\u5c04\u7684 hostPath \u4e3a /srv/ app - config \u76ee\u5f55 \u76f4\u63a5\u4ece\u5b98\u65b9\u62f7\u8d1d\u5408\u9002\u7684\u6848\u4f8b\uff0c\u4fee\u6539\u53c2\u6570\uff0c\u7136\u540e\u8bbe\u7f6e hostPath \u4e3a /srv/ app - config \u5373\u53ef https : // kubernetes . io / zh / docs / tasks / configure - pod - container / configure - persistent - volume - storage / #create-a-persistentvolume [ root @k8s - node1 ~] # more create-pv.yaml apiVersion : v1 kind : PersistentVolume metadata : name : app - config - 1 labels : type : local spec : capacity : storage : 2 Gi accessModes : - ReadWriteOnce hostPath : path : \"/srv/app-config\" \u9898\u76ee13 \u4f7f\u7528\u6307\u5b9a storageclass csi-hostpath-sc \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a pv-volume \u7684 pvc\uff0c\u5bb9\u91cf\u4e3a 10Mi \u521b\u5efa\u540d\u79f0\u4e3a web-server \u7684pod\uff0c\u5c06 nginx \u5bb9\u5668\u7684 /usr/share/nginx/html \u76ee\u5f55\u4f7f\u7528\u8be5 pvc \u6302\u8f7d \u5c06\u4e0a\u8ff0 pvc \u7684\u5927\u5c0f\u4ece 10Mi \u66f4\u65b0\u4e3a 70Mi\uff0c\u5e76\u8bb0\u5f55\u672c\u6b21\u53d8\u66f4 https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/ [ root@k8s-node1 ~ ] # cat create-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pv-volume spec: storageClassName: csi-hostpath-sc accessModes: - ReadWriteOnce resources: requests: storage: 3Mi [ root@k8s-node1 ~ ] # cat pvc-mount-pods.yaml apiVersion: v1 kind: Pod metadata: name: task-pv-pod spec: volumes: - name: my-pvc persistentVolumeClaim: claimName: pv-volume containers: - name: web-server image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: my-pvc kubectl edit pvc pv-volume --record #--record\u662f\u8bb0\u5f55\u5728\u6ce8\u89e3\u4e2d \u9898\u76ee15 \u6dfb\u52a0 sidecar \u5bb9\u5668\u5e76\u8f93\u51fa\u65e5\u5fd7 \u6dfb\u52a0\u4e00\u4e2a sidecar \u5bb9\u5668 ( \u4f7f\u7528busybox \u955c\u50cf ) \u5230\u5df2\u6709\u7684 pod 11 - factor - app \u4e2d \u786e\u4fdd sidecar \u5bb9\u5668\u80fd\u591f\u8f93\u51fa / var / log / 11 - factor - app . log \u7684\u4fe1\u606f \u4f7f\u7528 volume \u6302\u8f7d / var / log \u76ee\u5f55 \uff0c \u786e\u4fdd sidecar \u80fd\u8bbf\u95ee 11 - factor - app . log \u6587\u4ef6 \u901a\u8fc7 kubectl get pod - o yaml \u7684\u65b9\u6cd5\u5907\u4efd\u539f\u59cb pod \u4fe1\u606f \uff0c \u5220\u9664\u65e7\u7684pod 11 - factor - app copy \u4e00\u4efd\u65b0 yaml \u6587\u4ef6 \uff0c \u6dfb\u52a0 \u4e00\u4e2a\u540d\u79f0\u4e3a sidecar \u7684\u5bb9\u5668 \u65b0\u5efa emptyDir \u7684\u5377 \uff0c \u786e\u4fdd\u4e24\u4e2a\u5bb9\u5668\u90fd\u6302\u8f7d\u4e86 / var / log \u76ee\u5f55 \u65b0\u5efa\u542b\u6709 sidecar \u7684 pod \uff0c \u5e76\u901a\u8fc7 kubectl logs \u9a8c\u8bc1 https : //kubernetes.io/zh/docs/concepts/cluster-administration/logging/ \u9898\u76ee16 \u8282\u70b9 wk8s-node-0 \u72b6\u6001\u4e3a NotReady\uff0c\u67e5\u770b\u539f\u56e0\u5e76\u6062\u590d\u5176\u72b6\u6001\u4e3a Ready \u786e\u4fdd\u64cd\u4f5c\u4e3a\u6301\u4e45\u7684 \u89e3\u6790\uff1a \u901a\u8fc7 get nodes \u67e5\u770b\u5f02\u5e38\u8282\u70b9\uff0c\u767b\u5f55\u8282\u70b9\u67e5\u770b kubelet \u7b49\u7ec4\u4ef6\u7684 status \u5e76\u5224\u65ad\u539f\u56e0 \u542f\u52a8 kubelet \u5e76 enable kubelet \u5373\u53ef kubectl get nodes ssh wk8s-node-0 sudo -i systemctl status kubelet systemctl enable kubelet systemctl restart kubelet systemctl status kubelet \u518d\u6b21 get nodes\uff0c \u786e\u4fdd\u8282\u70b9\u6062\u590d Ready \u72b6\u6001 \u9898\u76ee17 \u67e5\u627e label \u4e3a name = cpu - loader \u7684 pod , \u7b5b\u9009\u51fa cpu \u8d1f\u8f7d\u6700\u9ad8\u7684\u90a3\u4e2a pod \uff0c\u5e76\u5c06\u540d\u79f0 \u8ffd\u52a0 \u5230 / opt / KUTR00401 / KUTR00401 . txt \u89e3\u6790\uff1a \u4f7f\u7528 top\u547d\u4ee4 \uff0c\u7ed3\u5408 - l label_key = label_value \u548c -- sort = cpu \u8fc7\u6ee4\u51fa\u76ee\u6807\u5373\u53ef kubectl top pod - l name = cpu - loader - A -- sort - by = cpu echo podName >> / opt / KUTR00401 / KUTR00401 . txt \u9700\u8981\u91cd\u70b9\u6ce8\u610f\u7684\u9898 \u00b6 \u6dfb\u52a0 sidecar \u5bb9\u5668\u5e76\u8f93\u51fa\u65e5\u5fd7 \u6dfb\u52a0\u4e00\u4e2a sidecar \u5bb9\u5668 ( \u4f7f\u7528busybox \u955c\u50cf ) \u5230\u5df2\u6709\u7684 pod 11 -factor-app \u4e2d \u786e\u4fdd sidecar \u5bb9\u5668\u80fd\u591f\u8f93\u51fa /var/log/11-factor-app.log \u7684\u4fe1\u606f \u4f7f\u7528 volume \u6302\u8f7d /var/log \u76ee\u5f55\uff0c\u786e\u4fdd sidecar \u80fd\u8bbf\u95ee 11 -factor-app.log \u6587\u4ef6 \u89e3\u7b54\uff1a https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/ \u5148\u7528kubectl get podname -o yaml > podname.yaml \u83b7\u53d6\u5230yaml\u6587\u4ef6\uff0c\u7136\u540e\u5220\u9664\u65e7\u7684 pod\uff1b \u518d\u91cd\u65b0 copy \u4e00\u4e2a\u65b0\u7684 yaml \u6dfb\u52a0 sidecar \u5bb9\u5668\uff0c\u5e76\u5728\u4e24\u4e2a\u5bb9\u5668\u4e2d\u90fd\u6302\u8f7d emtpyDir \u5230 /var/log\u76ee\u5f55\uff0c\u6700\u540e\u901a\u8fc7apply \u751f\u6210\u5e26 sidecar \u7684 pod\uff1b pod \u6b63\u5e38\u62c9\u8d77\u540e,\u901a\u8fc7 kubectl logs 11 -factor-app sidecar \u786e\u8ba4\u80fd\u6b63\u5e38\u8f93\u5165\u65e5\u5fd7\u5373\u53ef #\u5148\u5907\u4efd [ root@k8s-node1 ~ ] # kubectl get pods 11-factor-app -o yaml > 11-factor-app.ayml #\u7136\u540e\u5728\u5220\u9664\uff0c\u4f7f\u7528\u5b98\u65b9\u7684\u6a21\u677f\u8fdb\u884c\u4fee\u6539\u5373\u53ef\u3002 [ root@k8s-node1 ~ ] # more sidecar.yaml apiVersion: v1 kind: Pod metadata: name: 11 -factor-app spec: containers: - name: 11 -factor-app image: busybox args: - /bin/sh - -c - > i = 0 ; while true ; do echo \" $( date ) INFO $i \" >> /var/log/11-factor-app.log ; i = $(( i+1 )) ; sleep 1 ; done volumeMounts: - name: varlog mountPath: /var/log - name: sidecar image: busybox args: [ /bin/sh, -c, 'tail -n+1 -f /var/log/11-factor-app.log' ] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} \u521b\u5efa\u548c\u4f7f\u7528 PVC \u4f7f\u7528\u6307\u5b9a storageclass csi - hostpath - sc \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a pv - volume \u7684 pvc \uff0c\u5bb9\u91cf\u4e3a 10 Mi \u521b\u5efa\u540d\u79f0\u4e3a web - server \u7684 pod \uff0c\u5c06 nginx \u5bb9\u5668\u7684 /usr/s hare / nginx / html \u76ee\u5f55\u4f7f\u7528\u8be5 pvc \u6302\u8f7d \u5c06\u4e0a\u8ff0 pvc \u7684\u5927\u5c0f\u4ece 10 Mi \u66f4\u65b0\u4e3a 70 Mi \uff0c\u5e76\u8bb0\u5f55\u672c\u6b21\u53d8\u66f4 \u9996\u5148\u8981\u786e\u8ba4\u662f\u5426\u8fdb\u9519\u4e86 k8s\u96c6\u7fa4\u4e2d \uff0c\u7136\u540e\u5728\u67e5\u770b StorageClass \u662f\u5426\u6709\u5bf9\u5e94\u7684 [ root @k8s - node1 newpvc ] # more createpvc.yaml apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pv - volume - to2 spec : accessModes : - ReadWriteOnce volumeMode : Filesystem resources : requests : storage : 10 Mi storageClassName : cis - hostpath - cs - to2 [ root @k8s - node1 newpvc ] # kubectl get pvc #\u5982\u679c\u6ca1\u7ed1\u5b9a\u7684\u8bdd\uff0c\u770b\u4e0b NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pv - volume - to2 Bound task - pv - volume - to2 1 Gi RWO cis - hostpath - cs - to2 31 m [ root @k8s - node1 newpvc ] # more newpvcmount.yaml apiVersion : v1 kind : Pod metadata : name : web - server - to2 spec : containers : - name : nginx image : nginx volumeMounts : - mountPath : \"/usr/share/nginx/html\" name : mypd volumes : - name : mypd persistentVolumeClaim : claimName : pv - volume - to2 kubectl edit pvc pv - volume - to2 -- record #\u7531\u4e8enfs\u548c\u672c\u5730\u5377\u4e0d\u652f\u6301\u52a8\u6001\u8c03\u6574\uff0c\u6545\u5c31\u6ca1\u505a\u3002 \u6309\u8981\u6c42\u521b\u5efa Ingress \u8d44\u6e90 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Ingress \u8d44\u6e90\uff0c\u540d\u79f0 ping\uff0c\u547d\u540d\u7a7a\u95f4 ing-internal \u4f7f\u7528 /hello \u8def\u5f84\u66b4\u9732\u670d\u52a1 hello \u7684 5678 \u7aef\u53e3\uff0c #\u89e3\u7b54\u521b\u5efaingress\u4e4b\u540e\u65e0\u6cd5\u83b7\u53d6ip\u5730\u5740 CKA\u8003\u8bd5 K8s \u5b98\u65b9\u9875\u7b7e \u00b6 \u4f7f\u7528\u8bf4\u660e \u4f7f\u7528\u590d\u5236\u4e4b\u540e\u5199\u5165\u6587\u4ef6txt\u6587\u4ef6\u4e2d\uff0c\u540e\u7f00\u4fee\u6539\u6210html,\u7136\u540e\u5728\u5bfc\u5165\u4e66\u7b7e\u4e2d\u5373\u53ef \u89c1 https://www.jianshu.com/p/a743860b13fe \u53c2\u8003\u6587\u6863\uff1a https://www.cloudcared.cn/3138.html","title":"CKA\u8bd5\u9898\u6536\u96c6-01"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#cka-01","text":"","title":"CKA\u8bd5\u9898\u6536\u96c6-01"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#_1","text":"\u9700\u8981\u79d1\u5b66\u4e0a\u7f51\uff0cCKA\u8003\u8bd5\u65f6\u9700\u8981\u68c0\u67e5\u4e0a\u6700\u4f4e500Kbps\u4e0b\u8f7d\u548c256Kbps\u4e0a\u4f20\uff0c \u7b14\u8bb0\u672c\u7535\u8111\u81ea\u5e26\u6444\u50cf\u5934\u5373\u53ef \u9700\u8981\u5b89\u88c5 Innovative Exams Screensharing \u8c37\u6b4c\u6d4f\u89c8\u5668\u63d2\u4ef6 \u591a\u505aCKA\u771f\u9898\uff0c\u57fa\u672c\u4e0a\u8003\u8bd5\u5c31\u662f\u53c2\u6570\u6709\u53d8\u52a8\uff0c\u90fd\u662f\u539f\u9898 \u91cd\u70b9\u91cd\u70b9\u91cd\u70b9 \u8003\u8bd5\u65f6\u53ef\u4ee5\u770b\u5b98\u65b9\u6587\u6863\uff0c\u53ef\u4ee5\u63d0\u524d\u505a\u6210\u4e66\u7b7e,\u8003\u8bd5\u65f6\u90fd\u6709\u76f4\u63a5\u70b9\u51fb\u5373\u53ef\uff0c\u53ea\u80fd\u6253\u5f00\u4e00\u4e2a\u9875\u7b7e\u3002","title":"\u8003\u524d\u51c6\u5907\u5de5\u4f5c"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#_2","text":"","title":"\u82f1\u6587\u8003\u8bd5\u8bd5\u9898"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#1","text":"**Set configuration context $kubectl config use-context k8s. Monitor the logs of Pod foobar and Extract log lines corresponding to error unable-to-access-website . Write them to /opt/KULM00612/foobar.** \u7ffb\u8bd1\uff1a\u8bbe\u7f6e\u914d\u7f6e\u4e0a\u4e0b\u6587 $kubectl config use context k8s\uff0c\u76d1\u63a7Pod foobar\u7684\u65e5\u5fd7\uff0c\u5e76\u63d0\u53d6\u9519\u8bef\u201cunable-to-access-website\u201d\u5bf9\u5e94\u7684\u65e5\u5fd7\u884c\u3002\u628a\u5b83\u4eec\u5199\u5230/opt/KULM00612/foobar\u3002 \u89e3\u6790\uff1a\u5c31\u662f\u770b\u4e0b\u4e00\u4e2apod\u4e2d\u7684\u65e5\u5fd7\uff0c\u628a\u6ee1\u8db3\u6761\u4ef6\u7684\u65e5\u5fd7\u884c\u4fdd\u5b58\u5728\u67d0\u4e00\u6587\u4ef6\u4e2d \u89e3\u7b54\uff1a \u9996\u5148\u5207\u6362\u4e0b\u4e0a\u4e0b\u6587k8s\u73af\u5883 kubectl config use context k8s [ root@k8s-node1 ~ ] # kubectl logs nginx|grep \"unable-to-access-website\" > /tmp/task.txt","title":"\u8bd5\u98981"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#2","text":"** Set configuration context $kubectl config use - context k8s . List all PVs sorted by capacity , saving the full kubectl output to /opt/ KUCC0006 / my_volumes . Use kubectl own functionally for sorting the output , and do not manipulate it any further ** \u7ffb\u8bd1\uff1a\u8bbe\u7f6e\u914d\u7f6e\u4e0a\u4e0b\u6587 $kubectl config use context k8s \u3002\u5217\u51fa\u6309\u5bb9\u91cf\u6392\u5e8f\u7684\u6240\u6709 PV \uff0c\u5c06\u5b8c\u6574\u7684 kubectl\u8f93\u51fa\u4fdd\u5b58\u5230 / opt / KUCC0006 / my_volumes \u3002\u5728\u529f\u80fd\u4e0a\u4f7f\u7528 kubectl \u672c\u8eab\u5bf9\u8f93\u51fa\u8fdb\u884c\u6392\u5e8f\uff0c\u4e0d\u8981\u518d\u5bf9\u5176\u8fdb\u884c\u4efb\u4f55\u64cd\u4f5c \u89e3\u6790\uff1a pv\u6392\u5e8f \u89e3\u7b54\uff1a [ root @k8s - node1 pv ] # kubectl get pv -A --sort-by={.spec.capacity.sotrge} > /opt/cka-tak.txt \u7136\u540e\u5728\u9a8c\u8bc1\u4e0b\u662f\u5426\u6587\u4ef6\u4e2d\u6709\u6570\u636e\u3002 [ root @k8s - node1 pv ] # cat /opt/cka-tak.txt NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfspv1 1 Gi RWO Recycle Available mynfs 4 m12s [ root @k8s - node1 pv ] #","title":"\u8bd5\u98982"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#3","text":"**Set configuration context $kubectl config use-context k8s. Ensure a single instance of Pod nginx is running on each node of the Kubernetes cluster where nginx also represents the image name which has to be used. Do no override any taints currently in place. Use Daemonset to complete this task and use ds.kusc00612 as Daemonset name** \u7ffb\u8bd1\uff1a\u8bbe\u7f6e\u914d\u7f6e\u4e0a\u4e0b\u6587 $kubectl config use context k8s\u3002\u786e\u4fdd\u5728Kubernetes\u96c6\u7fa4\u7684\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884cPod nginx\u7684\u5355\u4e2a\u5b9e\u4f8b\uff0c\u5176\u4e2dnginx\u8fd8\u8868\u793a\u5fc5\u987b\u4f7f\u7528\u7684\u6620\u50cf\u540d\u79f0\u3002\u4e0d\u8981\u8986\u76d6\u5f53\u524d\u5b58\u5728\u7684\u4efb\u4f55\u6c61\u70b9\u3002\u4f7f\u7528Daemonset\u5b8c\u6210\u6b64\u4efb\u52a1\u5e76\u4f7f\u7528ds.kusc00612\u4f5c\u4e3a\u5b88\u62a4\u8fdb\u7a0b\u540d\u79f0 \u89e3\u7b54\uff1a \u53ef\u53c2\u7167\u5b98\u7f51\u7684Daemonset\u8fdb\u884c\u4fee\u6539 https://kubernetes.io/zh/docs/concepts/workloads/controllers/daemonset/ [ root@k8s-node1 ~ ] # vim Daemonset.ayml apiVersion: apps/v1 kind: DaemonSet metadata: name: ds.kusc00612 labels: k8s-app: ds.kusc00612 spec: selector: matchLabels: name: ds.kusc00612 template: metadata: labels: name: ds.kusc00612 spec: containers: - name: nginx image: nginx apply\u52a0\u8f7dyaml\u6587\u4ef6\u9a8c\u8bc1\u662f\u5426\u6b63\u5e38 [ root@k8s-node1 ~ ] # kubectl apply -f Daemonset.ayml daemonset.apps/ds.kusc00612 created [ root@k8s-node1 ~ ] # kubectl get daemonset NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE ds.kusc00612 1 1 1 1 1 <none> 2m14s","title":"\u8bd5\u98983"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#4","text":"Set configuration context $ kubectl config use - context k8s Perform the following tasks : Add an init container to lumpy - koala ( which has been defined in spec file / opt / kucc00100 / pod - specKUCC00612 . yaml ). The init container should create an empty file named / workdir / calm . txt . If / workdir / calm . txt is not detected , the Pod should exit . Once the spec file has been updated with the init container definition , the Pod should be created \u7ffb\u8bd1\uff1a\u6267\u884c\u4ee5\u4e0b\u4efb\u52a1\uff1a\u5c06 init \u5bb9\u5668\u6dfb\u52a0\u5230 lumpy - koala \uff08\u5df2\u5728\u6587\u4ef6 / opt / kucc00100 / pod - specKUCC00612 . yaml\u4e2d\u5b9a\u4e49 \uff09\u3002 init \u5bb9\u5668\u5e94\u8be5\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a / workdir / calm . txt\u7684\u7a7a\u6587\u4ef6 . \u5982\u679c / workdir / calm . txt\u672a\u68c0\u6d4b\u5230 \uff0c Pod \u5e94\u9000\u51fa\u3002\u4e00\u65e6\u7528 init \u5bb9\u5668\u5b9a\u4e49\u66f4\u65b0\u4e86 spec \u6587\u4ef6\uff0c\u5c31\u5e94\u8be5\u521b\u5efa Pod \u89e3\u6790\uff1a\u8fd9\u9053\u9898\u5728 / opt / kucc00100 / pod - specKUCC00612 . yaml\u8def\u5f84\u4e0b\u5df2\u7ecf\u6709\u5199\u597d\u7684Yaml\u4e86 \uff0c\u4f46\u662f\u8fd8\u672a\u5728\u96c6\u7fa4\u4e2d\u521b\u5efa\u8be5\u5bf9\u8c61\u3002\u6240\u4ee5\u4f60\u4e0a\u53bb\u6700\u597d\u5148 kubectl get po | grep pod \u540d\u5b57\u3002\u53d1\u73b0\u96c6\u7fa4\u8fd8\u6ca1\u6709\u8be5 pod \u3002\u6240\u4ee5\u4f60\u5c31\u5148\u6539\u4e0b\u8fd9\u4e2a Yaml , \u7136\u540e apply . \u5148\u521b\u5efaInitcontainer , \u7136\u540e\u5728\u91cc\u9762\u521b\u5efa\u6587\u4ef6\uff0c / workdir \u76ee\u5f55\u660e\u663e\u662f\u4e2a\u6302\u8f7d\u8fdb\u5ea6\u7684\u76ee\u5f55\uff0c\u9898\u76ee\u6ca1\u89c4\u5b9a\uff0c\u4f60\u5c31\u5b9a\u4e49 empDir \u7c7b\u578b\u3002\u8fd9\u8fb9\u8fd8\u8981\u7528\u5230 liveness \u68c0\u67e5 \u89e3\u7b54\uff1a \u53ef\u53c2\u7167\u5b98\u7f51 : https : //kubernetes.io/docs/concepts/workloads/pods/init-containers/#using-init-containers https : //kubernetes.io/docs/concepts/storage/volumes/#emptydir https : //kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ [ root@ k8s - node1 ~] # vim init - pod . yaml apiVersion : v1 kind : Pod metadata : name : myapp - pod labels : app : myapp spec : containers : - name : myapp - container image : busybox : 1.28 command : [ ' sh ' , ' - c ' , ' echo The app is running ! && sleep 3600 ' ] initContainers : - name : init - myservice image : busybox : 1.28 command : [ ' sh ' , ' - c ' , \"touch /workdir/calm.txt\" ] volumeMounts : - mountPath : / workdir name : cache - volume volumes : - name : cache - volume emptyDir : {} [ root@ k8s - node1 ~] # kubectl apply - f init - pod . yaml pod / myapp - pod created","title":"\u8bd5\u98984"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#5","text":"Set configuration context $kubectl config use - context k8s . Create a pod named kucc6 with a single container for each of the following images running inside ( there may be between 1 and 4 images specified ) :nginx + redis + memcached + consul \u3002 \u7ffb\u8bd1\uff1a\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a kucc6\u7684pod \uff0c\u5176\u4e2d\u5305\u542b\u8fd0\u884c\u5728\u5176\u4e2d\u7684\u4ee5\u4e0b\u6620\u50cf\u7684\u5355\u4e2a\u5bb9\u5668\uff08\u53ef\u80fd\u4f1a\u6307\u5b9a 1 \u5230 4 \u4e2a\u6620\u50cf\uff09\uff1a nginx + redis + memcached + consur \u89e3\u7b54\uff1a \u53ef\u53c2\u7167\u5b98\u7f51 :https:/ / kubernetes . io / zh / docs / concepts / scheduling - eviction / assign - pod - node / [ root @k8s - node1 ~] # vim images.yaml apiVersion : v1 kind : Pod metadata : name : kucc6 labels : env : kucc6 spec : containers : - name : nginx image : nginx - name : redis image : redis - name : memcached image : memcached - name : consul image : consul \u9a8c\u8bc1\uff1a [ root @k8s - node1 ~] # kubectl apply -f images.yaml pod / kucc6 created","title":"\u8bd5\u98985"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#6","text":"Set configuration context $kubectl config use - context k8s Schedule a Pod as follows : Name : nginxkusc00612 Image : nginx Node selector : disk = ssd \u7ffb\u8bd1\uff1a\u521b\u5efa Pod \uff0c\u540d\u5b57\u4e3a nginx \uff0c\u955c\u50cf\u4e3a nginx \uff0c\u90e8\u7f72\u5230 label disk = ssd\u7684node\u4e0a \u89e3\u6790\uff1a pod\u8c03\u5ea6\u5230\u6307\u5b9a\u8282\u70b9 \uff0c Nodeselector \u89e3\u7b54\uff1a \u5b98\u7f51\u641c\u7d22 nodeselector\u627eyaml\u6587\u4ef6 , \u53c2\u8003\u6587\u6863 https : // kubernetes . io / zh / docs / concepts / scheduling - eviction / assign - pod - node / #\u9996\u5148\u67e5\u8be2\u4e0bnode\u8282\u70b9\u4e0a\u9762\u7684\u6807\u7b7e\uff0c\u662f\u5426\u6709\u5bf9\u5e94\u5f97lable\u6807\u7b7e\u7136\u540e\u5728\u64cd\u4f5c [ root @k8s - node1 ~] # kubectl get nodes --show-labels [ root @k8s - node1 ~] # vim nodeSelector.yaml apiVersion : v1 kind : Pod metadata : name : nginxkusc00612 labels : env : test spec : containers : - name : nginx image : nginx imagePullPolicy : IfNotPresent nodeSelector : disktype : ssd [ root @k8s - node1 ~] # kubectl apply -f nodeSelector.yaml pod / nginxkusc00612 created [ root @k8s - node1 ~] # kubectl get pods nginxkusc00612 NAME READY STATUS RESTARTS AGE nginxkusc00612 1 / 1 Running 0 3 m","title":"\u8bd5\u98986"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#7","text":"Set configuration context $kubectl config use - context k8s . Create a deployment as follows : Name : nginxapp Using container nginx with version 1 . 11 . 9 - alpine . The deployment should contain 3 replicas . Next , deploy the app with new version 1 . 12 . 0 - alpine by performing a rolling update and record that update . Finally , rollback that update to the previous version 1 . 11 . 9 - alpine . \u89e3\u6790\uff1a\u90e8\u7f72 deploy , \u7136\u540e\u4fee\u6539\u8fdb\u955c\u50cf\uff08\u6eda\u52a8\u66f4\u65b0\uff09\uff0c\u7136\u540e\u56de\u6eda\u4e0a\u4e00\u7248\u672c \u5b98\u7f51\u641c\u7d22 Deployment \u89e3\u7b54\uff1a https : // kubernetes . io / zh / docs / concepts / workloads / controllers / deployment / [ root @k8s - node1 ~] # kubectl create deployment nginxapp --image=nginx:1.11.9-alpine deployment . apps / nginxapp created [ root @k8s - node1 ~] # kubectl scale deployment nginxapp --replicas=3 deployment . apps / nginxapp scaled [ root @k8s - node1 ~] # kubectl set image deployment/nginxapp nginx=nginx:1.12.0-alpine --record=true deployment . apps / nginxapp image updated [ root @k8s - node1 ~] # kubectl rollout undo deployment.apps/nginxapp","title":"\u8bd5\u98987"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#8","text":"Set configuration context $kubectl config use - context k8s Create and configure the service front - endservice so it \u2019 s accessible through NodePort / ClusterIp and routes to the existing pod named nginxkusc00612 \u89e3\u6790\uff1a\u521b\u5efa service \uff0c\u6307\u5b9a\u540e\u7aef\u5230\u5df2\u6709 pod : nginxkusc00612 \u89e3\u7b54\uff1a https : // kubernetes . io / zh / docs / tasks / access - application - cluster / connecting - frontend - backend / #\u7531\u4e8e\u6ca1\u6709\u9a8c\u8bc1\u6210\u529f\uff0c\u5f85\u540e\u7eed\u5728\u9a8c\u8bc1\u3002\u627e\u4e0d\u5230Endpoints [ root @k8s - node1 ~] # cat service.yaml apiVersion : v1 kind : Service metadata : name : pod - service spec : selector : app : front - end type : NodePort ports : - protocol : TCP port : 80 targetPort : http","title":"\u8bd5\u98988"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#9","text":"Set configuration context $kubectl config use-context k8s Create a Pod as follows: Name: jenkins Using image: jenkins In a new Kubernetes namespace named pro-test \u89e3\u6790\uff1a\u5728\u65b0\u7684\u547d\u540d\u7a7a\u95f4\u4e2d\u521b\u5efajenkins\u7684pod \u89e3\u7b54\uff1a \u9996\u5148\u67e5\u8be2\u662f\u5426\u6709pro-test\u540d\u79f0\u7a7a\u95f4 [root@k8s-node1 ~]# kubectl get namespaces pro-test \u7136\u540e\u5728\u521b\u5efa [root@k8s-node1 ~]# kubectl run jenkins --image=jenkins --namespace=pro-test \u7136\u540e\u5728\u9a8c\u8bc1\uff1a [root@k8s-node1 ~]# kubectl get pods -n pro-test","title":"\u8bd5\u98989"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#10","text":"Set configuration context $kubectl config use-context k8s Create a deployment spec file that will: Launch 7 replicas of the redis image with the label : app_enb_stage=dev Deployment name: kual00612 Save a copy of this spec file to /opt/KUAL00612/deploy_spec.yaml (or .json) When you are done,clean up(delete) any new k8s API objects that you produced during this task \u7ffb\u8bd1\uff1a\u8bbe\u7f6e\u914d\u7f6e\u4e0a\u4e0b\u6587$kubectl config use context k8s\u521b\u5efa\u4e00\u4e2a\u90e8\u7f72\u89c4\u8303\u6587\u4ef6\uff0c\u8be5\u6587\u4ef6\u5c06\uff1a\u542f\u52a8redis\u6620\u50cf\u76847\u4e2a\u526f\u672c\uff0c\u6807\u7b7e\u4e3a\uff1aapp_enb_stage=dev deployment name:kual00612\u5c06\u6b64\u89c4\u8303\u6587\u4ef6\u7684\u526f\u672c\u4fdd\u5b58\u5230/opt/kual00612/deploy_spec.yaml\uff08\u6216.json\uff09\u5b8c\u6210\u540e\uff0c\u6e05\u7406\uff08\u5220\u9664\uff09\u60a8\u5728\u6b64\u4efb\u52a1\u671f\u95f4\u751f\u6210\u7684\u4efb\u4f55\u65b0\u7684k8sapi\u5bf9\u8c61 \u89e3\u6790\uff1a\u521b\u5efa7\u526f\u672c\u7684redis\u7684deploy\uff0c\u6307\u660e\u6807\u7b7e\uff0c\u7136\u540e\u628ayaml\u4fdd\u5b58\u5728\u6307\u5b9a\u4f4d\u7f6e \u89e3\u7b54\uff1a https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ [root@k8s-node1 ~]# vim ReplicaSet.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kual00612 labels: app_enb_stage: dev spec: replicas: 7 selector: matchLabels: app: kual00612 template: metadata: labels: app: kual00612 spec: containers: - name: redis image: redis \u9a8c\u8bc1\u662f\u5426\u6b63\u5e38\uff1a [root@k8s-node1 ~]# kubectl get pods |grep kual00612|grep Running|wc -l \u7136\u540e\u5728\u628ayaml\u590d\u5236\u5230/opt/KUAL00612/deploy_spec.yaml cat ReplicaSet.yaml > /opt/KUAL00612/deploy_spec.yaml","title":"\u8bd5\u989810"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#11","text":"Set configuration context $kubectl config use - context k8s Create a file / opt / KUCC00612 / kucc00612 . txt that lists all pods that implement Service foo in Namespace production . The format of the file should be one pod name per line . \u89e3\u6790\uff1a\u6ee1\u8db3 foo service\u9009\u62e9\u89c4\u5219\u7684pod \uff0c\u5e76\u628a\u540d\u5b57\u5199\u5165\u67d0\u4e2a\u6587\u4ef6 kubecet get svc - n production -- show - lables | grep foo kubectl get pods - nccod45 - l name = foo | grep - v NAME | awk '{print $1}' >> / opt / KUCC00302 / kucc00302 . txt","title":"\u8bd5\u989811"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#12","text":"Set configuration context $kubectl config use - context k8s Create a Kubernetes Secret as follows: Name: super - secret credential: blob , Create a Pod named pod - secrets - via - file using the redis image which mounts a secret named super - secret at / secrets . Create a second Pod named pod - secretsvia - env using the redis image , which exports credential as \u7ffb\u8bd1\uff1a\u521b\u5efa\u4e00\u4e2a Kubernetes Secret \uff0c\u5982\u4e0b\u6240\u793a\uff1a Name: super Secret credential: blob \uff0c\u4f7f\u7528 redis\u6620\u50cf\u521b\u5efa\u4e00\u4e2a\u540d\u4e3aPod secrets\u7684Pod \uff0c\u8be5\u6620\u50cf\u5728 / secrets\u5904\u6302\u8f7d\u4e00\u4e2a\u540d\u4e3asuper Secret\u7684\u673a\u5bc6 \u3002\u4f7f\u7528 redis\u6620\u50cf\u521b\u5efa\u7b2c\u4e8c\u4e2a\u540d\u4e3aPod secretsvia env\u7684Pod \uff0c\u5b83\u5c06\u51ed\u8bc1\u5bfc\u51fa\u4e3a\u51ed\u8bc1 \u89e3\u6790\uff1a\u521b\u5efa secret \uff0c\u5e76\u5728 pod\u4e2d\u901a\u8fc7Volume\u548c\u73af\u5883\u53d8\u91cf\u4f7f\u7528\u8be5secret \u89e3\u7b54\uff1a https: //kubernetes.io/zh/docs/concepts/configuration/secret/ [ root @k8s - node1 ~ ] # echo blob | base64 YmxvYgo =","title":"\u8bd5\u989812"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#13","text":"Set configuration context $kubectl config use-context k8s Create a pod as follows: Name: nonpersistent-redis Container image: redis Named-volume with name: cache-control Mount path : /data/redis It should launch in the pre-prod namespace and the volume MUST NOT be persistent. \u89e3\u6790\uff1a\u521b\u5efa\u4e00\u4e2apod\uff0c\u5e76\u6302\u8f7dvolume \u89e3\u7b54\uff1a https://kubernetes.io/zh/docs/concepts/storage/volumes/ [root@k8s-node1 ~]# cat volumes.yaml apiVersion: v1 kind: Pod metadata: name: nonpersistent-redis spec: containers: - image: redis name: nonpersistent-redis volumeMounts: - mountPath: /data/redis name: cache-control volumes: - name: cache-control emptyDir: {}","title":"\u8bd5\u989813"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#14","text":"Set configuration context $kubectl config use - context k8s Scale the deployment webserver to 6 pods \u89e3\u6790\uff1a\u6269\u7f29\u5bb9 \u89e3\u7b54\uff1a https : // kubernetes . io / docs / reference / generated / kubectl / kubectl - commands #scale [ root @k8s - node1 ~] # kubectl scale --replicas=6 webserver","title":"\u8bd5\u989814"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#15","text":"Set configuration context $kubectl config use - context k8s Check to see how many nodes are ready ( not including nodes tainted NoSchedule ) and write the number to / opt / nodenum . \u89e3\u6790\uff1a\u6709\u591a\u5c11\u8282\u70b9\u662f ready\u72b6\u6001\u7684 \uff0c\u4e0d\u5305\u542b\u88ab\u6253\u4e86 NoSchedule\u6c61\u70b9\u7684\u8282\u70b9 kubectl describe nodes ` kubectl get nodes | grep Ready | awk '{print $1}' ` | grep Taints | grep - vc NoSchedule > / opt / nodenum","title":"\u8bd5\u989815"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#16","text":"Set configuration context $kubectl config use - context k8s Create a deployment as follows : Name : nginxdns Exposed via a service : nginx - dns Ensure that the service & pod are accessible via their respective DNS records The container ( s ) within any Pod ( s ) running as a part of this deployment should use the nginx image . Next , use the utility nslookup to look up the DNS records of the service & pod and write the output to /opt/se rvice . dns and /opt/ pod . dns respectively . Ensure you use the busybox : 1 . 28 image ( or earlier ) for any testing , an the latest release has an upstream bug which impacts the use of nslookup \u89e3\u6790\uff1a\u521b\u5efa service\u548cdeployment \uff0c\u7136\u540e\u89e3\u6790 service\u7684dns\u548cpod\u7684dns \uff0c\u5e76\u628a\u89e3\u6790\u8bb0\u5f55\u4fdd\u5b58\u5230\u6307\u5b9a\u6587\u4ef6 \u89e3\u7b54\uff1a \u53c2\u8003\u7f51\u5740 :https:/ / kubernetes . io / docs / tasks / access - application - cluster / connecting - frontend - backend / https : // kubernetes . io / zh / docs / concepts / workloads / pods / init - containers / [ root @k8s - node1 dns ] # cat deployment.yaml apiVersion : apps / v1 kind : Deployment metadata : name : backend spec : selector : matchLabels : app : nginxdns replicas : 1 template : metadata : labels : app : nginxdns spec : containers : - name : nginx image : nginx ports : - name : http containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginxdns spec : selector : app : nginxdns ports : - protocol : TCP port : 80 targetPort : http --- apiVersion : v1 kind : Pod metadata : name : busybox - test labels : app : busybox - test spec : containers : - name : myapp - container image : busybox : 1 . 28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] [ root @k8s - node1 dns ] # kubectl exec -ti busybox-test -- nslookup nginxdns #\u89e3\u6790svc\u5730\u5740 >/opt/service.dns kubectl exec - ti busybox - test -- nslookup 10 . 244 . 1 . 52 > /opt/ pod . dns #Pod ip\u89e3\u6790","title":"\u8bd5\u989816"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#17","text":"No configuration context change required for this item Create a snapshot of the etcd instance running at https : // 127 . 0 . 0 . 1 : 2379 saving the snapshot to the file path /data/ backup / etcd - snapshot . db The etcd instance is running etcd version 3 . 2 . 18 The following TLS certificates / key are supplied for connecting to the server with etcdctl CA certificate : /opt/ KUCM0612 / ca . crt Client certificate : /opt/ KUCM00612 / etcdclient . crt Client key : /opt/ KUCM00612 / etcd - client . key \u89e3\u7b54\uff1a https : // kubernetes . io / zh / docs / tasks / administer - cluster / configure - upgrade - etcd / \u5907\u4efd etcd ETCDCTL_API = 3 etcdctl -- endpoints = https : // 127 . 0 . 0 . 1 : 2379 \\ -- cacert = /etc/ kubernetes / pki / etcd / ca . crt -- cert = /etc/ kubernetes / pki / etcd / peer . crt -- key = /etc/ kubernetes / pki / etcd / peer . key \\ snapshot save /etc/ kubernetes / pki / etcd / etcd - snapshot - test . db \u9a8c\u8bc1 etcd TCDCTL_API = 3 etcdctl -- write - out = table snapshot status /etc/ kubernetes / pki / etcd / etcd - snapshot - test . db \u8fd8\u539f etcd ETCDCTL_API = 3 etcdctl -- endpoints = https : // 127 . 0 . 0 . 1 : 2379 snapshot restore \\ -- cacert = /etc/ kubernetes / pki / etcd / ca . crt -- cert = /etc/ kubernetes / pki / etcd / peer . crt -- key = /etc/ kubernetes / pki / etcd / peer . key \\ /etc/ kubernetes / pki / etcd / etcd - snapshot - test . db \u7136\u540e\u5728\u91cd\u542f\u4e0b etcd\u4e4b\u540e \uff0c\u5728\u770b\u4e0b nodes\u548cpods","title":"\u8bd5\u989817"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#18","text":"Set configuration context $kubectl config use - context ek8s Set the node labelled with name = ek8s - node - 1 as unavailable and reschedule all the pods running on it . \u89e3\u6790\uff1a\u5c06\u6807\u7b7e\u672a name = ek8s - node - 1 \u8bbe\u7f6e\u6210\u4e0d\u53ef\u7528\u4e14\u628a\u8fd9\u4e2a\u8282\u70b9\u4e0a\u9762\u7684 pod\u8c03\u5ea6\u5230\u5176\u4ed6\u8282\u70b9\u4e0a\u53bb \u3002\u5176\u5b9e\u5c31\u662f\u4f7f\u7528 kubectl drain\u547d\u4ee4 kubectl cordon ek8s - node - 1 #\u5148\u9694\u79bb kubectl drain ek8s - node - 1 -- ignore - daemonsets -- delete - local - data -- force \u5b8c\u6210\u540e\u4e00\u5b9a\u8981\u901a\u8fc7 get nodes \u52a0\u4ee5\u786e\u8ba4","title":"\u8bd5\u989818"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#19","text":"Set configuration context $kubectl config use - context wk8s A Kubernetes worker node , labelled with name = wk8s - node - 0 is in state NotReady . Investigate why this is the case , and perform any appropriate steps to bring the node to a Ready state , Ensuring that any changes are made permanent . Hints: You can ssh to the failed node using $ssh wk8s - node - 0. You can assume elevated privileges on the node with the following command $sudo - i \u9898\u76ee\u89e3\u6790\uff1a wk8s - node - 0 \u662f NotReady\u72b6\u6001 \uff0c\u4f60\u9700\u8981\u5904\u7406\u4e0b\uff0c\u4f7f\u5176\u53d8\u4e3a ready \uff0c\u522b\u66f4\u6539\u9700\u8981\u6c38\u4e45\u6027","title":"\u8bd5\u989819"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#20","text":"21 \u3001 Set configuration context $kubectl config use - context wk8s Configure the kubelet system managed service , on the node labelled with name = wk8s - node - 1 , to Launch a Pod containing a single container of image nginx named myservice automatically . Any spec files required should be placed in the / etc / kubernetes / manifests directory on the node . Hints: You can ssh to the failed node using $ssh wk8snode - 1. You can assume elevated privileges on the node with the following command $sudo - i","title":"\u8bd5\u989820"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#21","text":"\u8fd9\u9898\u662f\u7ed9\u4f60\u4e24\u4e2a\u8282\u70b9 \uff0c master1\u548cnode1 \uff0c \u548c\u4e00\u4e2aadmin . conf\u6587\u4ef6 \uff0c \u7136\u540e\u8ba9\u4f60\u5728\u8fd9\u4e24\u4e2a\u8282\u70b9\u4e0a\u90e8\u7f72\u96c6\u7fa4 \u3002 \u89e3\u6790 \uff1a","title":"\u8bd5\u989821"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#22","text":"Set configuration context $kubectl configuse - context bk8s Given a partially - functioning Kubernetes cluser , identify symptoms of failure on the cluter . Determine the node , the failing service and take actions to bring up the failed service and restore the health of the cluser . Ensure that any changes are made permanently . The worker node in this cluster is labelled with name = bk8s - node - 0 Hints: You can ssh to the relevant nodes using $ssh $ ( NODE ) where $ ( NODE ) is one of bk8s - master - 0 or bk8s - node - 0. You can assume elevated privileges on any node in the cluster with the following command: $ sudo - i . \u89e3\u6790\uff1a\u8fd9\u9898\u7684\u610f\u601d\u662f\uff0c\u6709\u4e2a\u96c6\u7fa4\u90e8\u5206\u529f\u80fd\u51fa\u73b0\u95ee\u9898\uff0c\u9700\u8981\u4f60\u53bb\u4fee\u4e00\u4e0b\uff0c\u9700\u8981\u662f\u6c38\u4e45\u6027\u7684\u4fee\u590d\u3002","title":"\u8bd5\u989822"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#23","text":"Set configuration context $kubectl config use - context hk8s Create a persistent volume with name appconfig of capacity 1 Gi and access mode ReadWriteMany . The type of volume is hostPath and its locationis / srv / app - config","title":"\u8bd5\u989823"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#cka","text":"","title":"CKA\u6a21\u62df\u9898"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#1_1","text":"Task weight: 1 % You have access to multiple clusters from your main terminal through kubectl contexts. Write all those context names into /opt/course/1/contexts. Next write a command to display the current context into /opt/course/1/context_default_kubectl.sh, the command should use kubectl. Finally write a second command doing the same thing into /opt/course/1/context_default_no_kubectl.sh, but without the use of kubectl.","title":"\u8bd5\u98981"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#2_1","text":"Task weight: 3 % Use context: kubectl config use-context k8s-c1-H Create a single Pod of image httpd:2.4.41-alpine in Namespace default. The Pod should be named pod1 and the container should be named pod1-container. This Pod should only be scheduled on a master node, do not add new labels any nodes. Shortly write the reason on why Pods are by default not scheduled on master nodes into /opt/course/2/master_schedule_reason .","title":"\u8bd5\u98982"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#3_1","text":"Use context: kubectl config use-context k8s-c1-H There are two Pods named o3db-* in Namespace project-c13. C13 management asked you to scale the Pods down to one replica to save resources. Record the action.","title":"\u8bd5\u98983"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#4_1","text":"Task weight: 1% Use context: kubectl config use-context k8s-c1-H There are two Pods named o3db-* in Namespace project-c13. C13 management asked you to scale the Pods down to one replica to save resources. Record the action.","title":"\u8bd5\u98984"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#5_1","text":"Task weight: 4 % Use context: kubectl config use - context k8s - c1 - H Do the following in Namespace default . Create a single Pod named ready - if - service - ready of image nginx: 1.16 . 1 - alpine . Configure a LivenessProbe which simply runs true . Also configure a ReadinessProbe which does check if the url http: //service-am-i-ready:80 is reachable, you can use wget -T2 -O- http://service-am-i-ready:80 for this. Start the Pod and confirm it isn't ready because of the ReadinessProbe. Create a second Pod named am - i - ready of image nginx: 1.16 . 1 - alpine with label id: cross - server - ready . The already existing Service service - am - i - ready should now have that second Pod as endpoint . Now the first Pod should be in ready state , confirm that .","title":"\u8bd5\u98985"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#6_1","text":"Task weight: 1 % Use context: kubectl config use-context k8s-c1-H There are various Pods in all namespaces. Write a command into /opt/course/5/find_pods.sh which lists all Pods sorted by their AGE ( metadata.creationTimestamp ) . Write a second command into /opt/course/5/find_pods_uid.sh which lists all Pods sorted by field metadata.uid. Use kubectl sorting for both commands.","title":"\u8bd5\u98986"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#7_1","text":"Task weight : 8 % Use context : kubectl config use - context k8s - c1 - H Create a new PersistentVolume named safari - pv . It should have a capacity of 2 Gi , accessMode ReadWriteOnce , hostPath / Volumes / Data and no storageClassName defined . Next create a new PersistentVolumeClaim in Namespace project - tiger named safari - pvc . It should request 2 Gi storage , accessMode ReadWriteOnce and should not define a storageClassName . The PVC should bound to the PV correctly . Finally create a new Deployment safari in Namespace project - tiger which mounts that volume at / tmp / safari - data . The Pods of that Deployment should be of image httpd : 2.4.41 - alpine .","title":"\u8bd5\u98987"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#8_1","text":"Task weight : 2 % Use context : kubectl config use - context k8s - c1 - H Ssh into the master node with ssh cluster1 - master1 . Check how the master components kubelet , kube - apiserver , kube - scheduler , kube - controller - manager and etcd are started / installed on the master node . Also find out the name of the DNS application and how it ' s started / installed on the master node . Write your findings into file / opt / course / 8 / master - components . txt . The file should be structured like : # / opt / course / 8 / master - components . txt kubelet : [ TYPE ] kube - apiserver : [ TYPE ] kube - scheduler : [ TYPE ] kube - controller - manager : [ TYPE ] etcd : [ TYPE ] dns : [ TYPE ] [ NAME ] Choices of [ TYPE ] are : not - installed , process , static - pod , pod","title":"\u8bd5\u98988"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#9_1","text":"Task weight : 5 % Use context : kubectl config use - context k8s - c2 - AC Ssh into the master node with ssh cluster2 - master1 . Temporarily stop the kube - scheduler , this means in a way that you can start it again afterwards . Create a single Pod named manual - schedule of image httpd : 2.4 - alpine , confirm its started but not scheduled on any node . Now you ' re the scheduler and have all its power , manually schedule that Pod on node cluster2 - master1 . Make sure it ' s running . Start the kube - scheduler again and confirm its running correctly by creating a second Pod named manual - schedule2 of image httpd : 2.4 - alpine and check if it ' s running on cluster2 - worker1 .","title":"\u8bd5\u98989"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#10_1","text":"Use context: kubectl config use-context k8s-c1-H Create a new ServiceAccount processor in Namespace project-hamster. Create a Role and RoleBinding, both named processor as well. These should allow the new SA to only create Secrets and ConfigMaps in that Namespace.","title":"\u8bd5\u989810"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#11_1","text":"Use context: kubectl config use - context k8s - c1 - H Use Namespace project - tiger for the following . Create a DaemonSet named ds - important with image httpd: 2.4 - alpine and labels id = ds - important and uuid = 18426 a0b - 5 f59 - 4e10 - 923 f - c0e078e82462 . The Pods it creates should request 10 millicore cpu and 10 megabytes memory . The Pods of that DaemonSet should run on all nodes .","title":"\u8bd5\u989811"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#12_1","text":"Use context : kubectl config use - context k8s - c1 - H Use Namespace project - tiger for the following . Create a Deployment named deploy - important with label id = very - important ( the pods should also have this label ) and 3 replicas . It should contain two containers , the first named container1 with image nginx : 1.17.6 - alpine and the second one named container2 with image kubernetes / pause . There should be only ever one Pod of that Deployment running on one worker node . We have two worker nodes : cluster1 - worker1 and cluster1 - worker2 . Because the Deployment has three replicas the result should be that on both nodes one Pod is running . The third Pod won ' t be scheduled , unless a new worker node will be added . In a way we kind of simulate the behaviour of a DaemonSet here , but using a Deployment and a fixed number of replicas .","title":"\u8bd5\u989812"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#13_1","text":"Create a Pod named multi - container - playground in Namespace default with three containers , named c1 , c2 and c3 . There should be a volume attached to that Pod and mounted into every container , but the volume shouldn ' t be persisted or shared with other Pods . Container c1 should be of image nginx : 1.17.6 - alpine and have the name of the node where its Pod is running on value available as environment variable MY_NODE_NAME . Container c2 should be of image busybox : 1.31.1 and write the output of the date command every second in the shared volume into file date . log . You can use while true ; do date >> / your / vol / path / date . log ; sleep 1 ; done for this . Container c3 should be of image busybox : 1.31.1 and constantly write the content of file date . log from the shared volume to stdout . You can use tail - f / your / vol / path / date . log for this . Check the logs of container c3 to confirm correct setup .","title":"\u8bd5\u989813"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#14_1","text":"Use context : kubectl config use - context k8s - c1 - H You ' re ask to find out following information about the cluster k8s - c1 - H : How many master nodes are available ? How many worker nodes are available ? What is the Service CIDR ? Which Networking ( or CNI Plugin ) is configured and where is its config file ? Which suffix will static pods have that run on cluster1 - worker1 ? Write your answers into file / opt / course / 14 / cluster - info , structured like this : # / opt / course / 14 / cluster - info 1 : [ ANSWER ] 2 : [ ANSWER ] 3 : [ ANSWER ] 4 : [ ANSWER ] 5 : [ ANSWER ]","title":"\u8bd5\u989814"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#15_1","text":"Use context: kubectl config use-context k8s-c2-AC Write a command into /opt/course/15/cluster_events.sh which shows the latest events in the whole cluster, ordered by time. Use kubectl for it. Now kill the kube-proxy Pod running on node cluster2-worker1 and write the events this caused into /opt/course/15/pod_kill.log. Finally kill the containerd container of the kube-proxy Pod on node cluster2-worker1 and write the events into /opt/course/15/container_kill.log. Do you notice differences in the events both actions caused?","title":"\u8bd5\u989815"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#16_1","text":"Use context: kubectl config use-context k8s-c1-H Create a new Namespace called cka-master. Write the names of all namespaced Kubernetes resources (like Pod, Secret, ConfigMap...) into /opt/course/16/resources.txt. Find the project-* Namespace with the highest number of Roles defined in it and write its name and amount of Roles into /opt/course/16/crowded-namespace.txt.","title":"\u8bd5\u989816"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#17_1","text":"Use context: kubectl config use-context k8s-c1-H In Namespace project-tiger create a Pod named tigers-reunite of image httpd:2.4.41-alpine with labels pod = container and container = pod. Find out on which node the Pod is scheduled. Ssh into that node and find the containerd container belonging to that Pod. Using command crictl: Write the ID of the container and the info.runtimeType into /opt/course/17/pod-container.txt Write the logs of the container into /opt/course/17/pod-container.log","title":"\u8bd5\u989817"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#18_1","text":"Use context: kubectl config use-context k8s-c3-CCC There seems to be an issue with the kubelet not running on cluster3-worker1. Fix it and confirm that cluster3 has node cluster3-worker1 available in Ready state afterwards. Schedule a Pod on cluster3-worker1. Write the reason of the is issue into /opt/course/18/reason.txt.","title":"\u8bd5\u989818"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#19_1","text":"this task can only be solved if questions 18 or 20 have been successfully implemented and the k8s - c3 - CCC cluster has a functioning worker node Use context : kubectl config use - context k8s - c3 - CCC Do the following in a new Namespace secret . Create a Pod named secret - pod of image busybox : 1.31.1 which should keep running for some time , it should be able to run on master nodes as well . There is an existing Secret located at / opt / course / 19 / secret1 . yaml , create it in the secret Namespace and mount it readonly into the Pod at / tmp / secret1 . Create a new Secret in Namespace secret called secret2 which should contain user = user1 and pass = 1234. These entries should be available inside the Pod ' s container as environment variables APP_USER and APP_PASS . Confirm everything is working .","title":"\u8bd5\u989819"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#20_1","text":"Your coworker said node cluster3 - worker2 is running an older Kubernetes version and is not even part of the cluster . Update kubectl and kubeadm to the version that ' s running on cluster3 - master1 . Then add this node to the cluster , you can use kubeadm for this .","title":"\u8bd5\u989820"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#21_1","text":"Use context: kubectl config use-context k8s-c3-CCC Create a Static Pod named my-static-pod in Namespace default on cluster3-master1. It should be of image nginx:1.16-alpine and have resource requests for 10m CPU and 20Mi memory. Then create a NodePort Service named static-pod-service which exposes that static Pod on port 80 and check if it has Endpoints and if its reachable through the cluster3-master1 internal IP address. You can connect to the internal node IPs from your main terminal.","title":"\u8bd5\u989821"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#22_1","text":"Use context: kubectl config use-context k8s-c2-AC Check how long the kube-apiserver server certificate is valid on cluster2-master1. Do this with openssl or cfssl. Write the exipiration date into /opt/course/22/expiration. Also run the correct kubeadm command to list the expiration dates and confirm both methods show the same date. Write the correct kubeadm command that would renew the apiserver server certificate into /opt/course/22/kubeadm-renew-certs.sh.","title":"\u8bd5\u989822"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#23_1","text":"Use context: kubectl config use-context k8s-c2-AC Node cluster2-worker1 has been added to the cluster using kubeadm and TLS bootstrapping. Find the \"Issuer\" and \"Extended Key Usage\" values of the cluster2-worker1: kubelet client certificate, the one used for outgoing connections to the kube-apiserver. kubelet server certificate, the one used for incoming connections from the kube-apiserver. Write the information into file /opt/course/23/certificate-info.txt. Compare the \"Issuer\" and \"Extended Key Usage\" fields of both certificates and make sense of these.","title":"\u8bd5\u989823"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#24","text":"Use context: kubectl config use-context k8s-c1-H There was a security incident where an intruder was able to access the whole cluster from a single hacked backend Pod. To prevent this create a NetworkPolicy called np-backend in Namespace project-snake. It should allow the backend-* Pods only to: connect to db1-* Pods on port 1111 connect to db2-* Pods on port 2222 Use the app label of Pods in your policy. After implementation, connections from backend-* Pods to vault-* Pods on port 3333 should for example no longer work.","title":"\u8bd5\u989824"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#25","text":"Use context: kubectl config use - context k8s - c3 - CCC Make a backup of etcd running on cluster3 - master1 and save it on the master node at / tmp / etcd - backup . db . Then create a Pod of your kind in the cluster . Finally restore the backup , confirm the cluster is still working and that the created Pod is no longer with us .","title":"\u8bd5\u989825"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#26","text":"Use context: kubectl config use-context k8s-c1-H Check all available Pods in the Namespace project-c13 and find the names of those that would probably be terminated first if the Nodes run out of resources ( cpu or memory ) to schedule all Pods. Write the Pod names into /opt/course/e1/pods-not-stable.txt.","title":"\u8bd5\u989826"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#27","text":"Use context: kubectl config use-context k8s-c1-H There is an existing ServiceAccount secret-reader in Namespace project-hamster. Create a Pod of image curlimages/curl:7.65.3 named tmp-api-contact which uses this ServiceAccount. Make sure the container keeps running. Exec into the Pod and use curl to access the Kubernetes Api of that cluster manually, listing all available secrets. You can ignore insecure https connection. Write the command ( s ) for this into file /opt/course/e4/list-secrets.","title":"\u8bd5\u989827"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#26_1","text":"Preview Question 1 Use context : kubectl config use - context k8s - c2 - AC The cluster admin asked you to find out the following information about etcd running on cluster2 - master1 : Server private key location Server certificate expiration date Is client certificate authentication endabled Write these information into / opt / course / p1 / etcd - info . txt Finally you ' re asked to save an etcd snapshot at / etc / etcd - snapshot . db on cluster2 - master1 and display its status .","title":"\u8bd5\u989826"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#27_1","text":"Preview Question 2 Use context : kubectl config use - context k8s - c1 - H You 're asked to confirm that kube-proxy is running correctly on all nodes. For this perform the following in Namespace project-hamster: Create a new Pod named p2 - pod with two containers , one of image nginx : 1.21.3 - alpine and one of image busybox : 1.31 . Make sure the busybox container keeps running for some time . Create a new Service named p2 - service which exposes that Pod internally in the cluster on port 3000 -> 80. Find the kube - proxy container on all nodes cluster1 - master1 , cluster1 - worker1 and cluster1 - worker2 and make sure that it 's using iptables. Use command crictl for this. Write the iptables rules of all nodes belonging the created Service p2 - service into file / opt / course / p2 / iptables . txt . Finally delete the Service and confirm that the iptables rules are gone from all nodes .","title":"\u8bd5\u989827"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#26_2","text":"Preview Question 3 Use context: kubectl config use-context k8s-c2-AC Create a Pod named check-ip in Namespace default using image httpd:2.4.41-alpine. Expose it on port 80 as a ClusterIP Service named check-ip-service. Remember/output the IP of that Service. Change the Service CIDR to 11 .96.0.0/12 for the cluster. Then create a second Service named check-ip-service2 pointing to the same Pod to check if your settings did take effect. Finally check if the IP of the first Service has changed.","title":"\u8bd5\u989826"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#cka_1","text":"\u6ce8\u610f\u70b9 \u5982\u679c\u9047\u5230etcd\u8fd8\u539f\u7684\u540e\u548c\u5347\u7ea7k8s\u6700\u597d\u8fd9\u9898\u6700\u540e\u5728\u505a\u5427 \u5347\u7ea7k8s\u4e5f\u6700\u540e\u5728\u505a\u5427\uff0c\u907f\u514d\u6574\u4e2ak8s\u5f02\u5e38\uff0c\u5bfc\u81f4\u5176\u4ed6\u7684\u9898\u76ee\u65e0\u6cd5\u8fdb\u884c\u4e86\uff0c\u6211\u7b2c\u4e00\u6b21\u8003\u6302\u4e86\uff0c\u5bfc\u81f4\u540e\u9762\u7684\u9898\u76ee\u65e0\u6cd5\u505a\u3002 \u53c2\u8003\u6587\u7ae0: https://blog.csdn.net/u011127242/category_10823035.html?spm=1001.2014.3001.5482 \u9898\u76ee1 \u9700\u8981\u91cd\u70b9\u8bb0\u5f55 \u521b\u5efa\u540d\u79f0 deployment-clusterrole \u7684 ClusterRole \u8be5\u89d2\u8272\u5177\u5907\u521b\u5efa Deployment\u3001Statefulset\u3001Daemonset \u7684\u6743\u9650 \u5728\u547d\u540d\u7a7a\u95f4 app-team1 \u4e2d\u521b\u5efa\u540d\u79f0\u4e3a cicd-token \u7684 ServiceAccount \u7ed1\u5b9a ClusterRole \u5230 ServiceAccount\uff0c\u4e14\u9650\u5b9a\u547d\u540d\u7a7a\u95f4\u4e3a app-team1 kubectl create ns app-team1 kubectl create serviceaccount cicd-token -n app-team1 kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployment,statefulset,daemonset kubectl -n app-team1 create rolebinding cicd-clusterrole --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token \u9898\u76ee2 \u9700\u8981\u91cd\u70b9\u8bb0\u5f55 \u8bbe\u7f6e ek8s - node -1 \u8282\u70b9\u4e3a\u4e0d\u53ef\u7528 \u91cd\u65b0\u8c03\u5ea6\u8be5\u8282\u70b9\u4e0a\u7684\u6240\u6709 pod kubectl cordon ek8s - node -1 kubectl drain ek8s - node -1 -- ignore - daemonsets -- delete - local - data -- force \u9898\u76ee3 \u5347\u7ea7 master \u8282\u70b9\u4e3a1.20.1 \u5347\u7ea7\u524d\u786e\u4fdddrain master \u8282\u70b9 \u4e0d\u8981\u5347\u7ea7worker node \u3001\u5bb9\u5668 manager\u3001 etcd\u3001 CNI\u63d2\u4ef6\u3001DNS \u7b49\u5185\u5bb9 https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ kubectl get nodes ssh mk8s-master-0 kubectl cordon mk8s-master-0 kubectl drain mk8s-master-0 --ignore-daemonsets apt-mark unhold kubeadm kubectl kubelet apt-get update && apt-get install -y kubeadm = 1 .20.1-00 kubelet = 1 .20.1-00 kubectl = 1 .20.1-00 apt-mark hold kubeadm kubectl kubelet kubeadm upgrade plan kubeadm upgrade apply v1.20.1 --etcd-upgrade = false // kubectl rollout undo deployment coredns -n kube-system ,\u6709\u4e9b\u5927\u4f6c\u5efa\u8baerollout coredns\uff0c\u7b14\u8005\u8003\u8bd5\u7684\u65f6\u5019\u6ca1\u6709rollover kubectl uncordon mk8s-master-0 \u9898\u76ee4 \u5907\u4efd https://127.0.0.1:2379 \u4e0a\u7684 etcd \u6570\u636e\u5230 /var/lib/backup/etcd-snapshot.db \u4f7f\u7528\u4e4b\u524d\u7684\u6587\u4ef6 /data/backup/etcd-snapshot-previous.db \u8fd8\u539f etcd \u4f7f\u7528\u6307\u5b9a\u7684 ca.crt \u3001 etcd-client.crt \u3001etcd-client.key \u5907\u4efdetc,\u9700\u8981\u6307\u5b9a\u8bc1\u4e66 ETCDCTL_API = 3 etcdctl --endpoints = https://127.0.0.1:2379 --cacert = /etc/kubernetes/pki/etcd/ca.crt --cert = /etc/kubernetes/pki/etcd/peer.crt --key = /etc/kubernetes/pki/etcd/peer.key snapshot save /var/lib/bacp/etcd-snapshot.db \u9a8c\u8bc1etcd\u5907\u4efd\u662f\u5426\u6b63\u786e ETCDCTL_API = 3 etcdctl --write-out = table snapshot status etcd-snapshot.db --cacert = /etc/kubernetes/pki/etcd/ca.crt --cert = /etc/kubernetes/pki/etcd/peer.crt --key = /etc/kubernetes/pki/etcd/peer.key \u6062\u590dectd\u96c6\u7fa4 ETCDCTL_API = 3 etcdctl --write-out = table snapshot restore etcd-snapshot.db --cacert = /etc/kubernetes/pki/etcd/ca.crt --cert = /etc/kubernetes/pki/etcd/peer.crt --key = /etc/kubernetes/pki/etcd/peer.key \u9898\u76ee5 \u9700\u8981\u91cd\u70b9\u8bb0\u5f55 \u62f7\u8d1d services-networking/network-policies \u4e2d\u7684\u6848\u4f8b\uff0c\u5220\u6389\u4e0d\u5fc5\u8981\u7684\u90e8\u5206 \u8bbe\u7f6e\u7f51\u7edc\u7b56\u7565\u6240\u5c5e\u7684 ns \u4e3a fubar\uff0c\u7aef\u53e3\u4e3a 80 \u8bbe\u7f6e namespaceSelector \u4e3a\u6e90ns my-app \u7684labels https://kubernetes.io/docs/concepts/services-networking/network-policies/ [ root@k8s-node1 ~ ] # vim NetworkPolicy.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-port-from-namespace namespace: fubar spec: podSelector: matchLabels: {} policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: my-app-key: my-app-value - podSelector: matchLabels: {} ports: - protocol: TCP port: 80 \u9898\u76ee6 <\u9700\u8981\u91cd\u70b9\u6ce8\u610f\u70b9> \u91cd\u65b0\u914d\u7f6e\u5df2\u6709\u7684 deployment front - end \uff0c\u6dfb\u52a0\u4e00\u4e2a\u540d\u79f0\u4e3a http \u7684\u7aef\u53e3\uff0c\u66b4\u9732 80 / TCP \u521b\u5efa\u540d\u79f0\u4e3a front - end - svc \u7684 service \uff0c\u66b4\u9732\u5bb9\u5668\u7684 http \u7aef\u53e3 \u914d\u7f6e service \u7684\u7c7b\u522b\u4e3a NodePort 1 \uff09 edit front - end \uff0c\u5728 containers \u4e2d\u6dfb\u52a0\u5982\u4e0b\u5185\u5bb9 kubectl edit deployment front - end ports : - name : http protocol : TCP containerPort : 80 https : // kubernetes . io / docs / reference / generated / kubectl / kubectl - commands #expose \u53c2\u8003 kubectl expose deployment\uff0c\u7136\u540e\u5728\u4f7f\u7528-h\u67e5\u770b\u5e2e\u52a9\u4fe1\u606f\u3002 2 \uff09 [ root @k8s - node1 ~] # kubectl expose deployment kual00612 --port=80 --target-port=80 --type=NodePort --name=front-end-svc \u9898\u76ee7 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Ingress \u8d44\u6e90\uff0c\u540d\u79f0 ping \uff0c\u547d\u540d\u7a7a\u95f4 ing - internal \u4f7f\u7528 / hello \u8def\u5f84\u66b4\u9732\u670d\u52a1 hello \u7684 5678 \u7aef\u53e3 https : //kubernetes.io/docs/concepts/services-networking/ingress/ [ root@ k8s - node1 ~] # vim ingress . yaml apiVersion : networking . k8s . io / v1 kind : Ingress metadata : name : ping namespace : ing - internal annotations : nginx . ingress . kubernetes . io / rewrite - target : / spec : rules : - http : paths : - path : / hello pathType : Prefix backend : service : name : test port : number : 5678 \u9898\u76ee8 \u6269\u5bb9 deployment guestbook \u4e3a 6\u4e2apod kubectl scale deployment --replicas = 6 guestbook \u6216\u8005\u4f7f\u7528 kubectl edit deployment guestbook #\u627e\u5230replicas \u8bbe\u7f6e\u4e3a6\uff0c\u5728\u4fdd\u5b58 \u9898\u76ee9 \u521b\u5efa pod \u540d\u79f0 nginx - kusc0041 \uff0c\u955c\u50cf nginx \u8c03\u5ea6\u8be5 pod \u5230 disk = ssd \u7684\u8282\u70b9\u4e0a https : // kubernetes . io / docs / concepts / scheduling - eviction / assign - pod - node / apiVersion : v1 kind : Pod metadata : name : nginx - kusc0041 spec : containers : - name : nginx image : nginx nodeSelector : disk : ssd \u9898\u76ee10 \u68c0\u67e5\u6709\u591a\u5c11\u8282\u70b9\u5df2\u51c6\u5907\u5c31\u7eea\uff08\u4e0d\u5305\u62ec nodes tainted Noschedule \uff09\uff0c\u5e76\u5c06\u7f16\u53f7\u5199\u5165 / opt / kusco0402 / kusco0402 . txt [ root @k8s - node1 ~] # kubectl get nodes|grep -v NAME|wc -l 2 [ root @k8s - node1 ~] # kubectl describe nodes |grep NoSchedule|wc -l 0 pods\u6570\u51cf\u53bbNoSchedule\u67e5\u8be2\u7684\u6570 \uff0c\u5199\u5165\u5230\u6307\u5b9a\u6587\u4ef6\u4e2d echo 2 > /opt/ kusco0402 / kusco0402 . txt \u9898\u76ee11 \u521b\u5efa\u540d\u79f0\u4e3a kucc1 \u7684 pod pod \u4e2d\u8fd0\u884c nginx \u548c redis \u4e24\u4e2a\u793a\u4f8b [ root @k8s - node1 ~] # more create-pods.yaml apiVersion : v1 kind : Pod metadata : name : nginx - to - redis spec : containers : - name : redis image : redis - name : nginx image : nginx \u9898\u76ee12 \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a app - config \u7684 PV \uff0c PV\u7684\u5bb9\u91cf\u4e3a2Gi \uff0c\u8bbf\u95ee\u6a21\u5f0f\u4e3a ReadWriteMany \uff0c volume \u7684\u7c7b\u578b\u4e3a hostPath pv \u6620\u5c04\u7684 hostPath \u4e3a /srv/ app - config \u76ee\u5f55 \u76f4\u63a5\u4ece\u5b98\u65b9\u62f7\u8d1d\u5408\u9002\u7684\u6848\u4f8b\uff0c\u4fee\u6539\u53c2\u6570\uff0c\u7136\u540e\u8bbe\u7f6e hostPath \u4e3a /srv/ app - config \u5373\u53ef https : // kubernetes . io / zh / docs / tasks / configure - pod - container / configure - persistent - volume - storage / #create-a-persistentvolume [ root @k8s - node1 ~] # more create-pv.yaml apiVersion : v1 kind : PersistentVolume metadata : name : app - config - 1 labels : type : local spec : capacity : storage : 2 Gi accessModes : - ReadWriteOnce hostPath : path : \"/srv/app-config\" \u9898\u76ee13 \u4f7f\u7528\u6307\u5b9a storageclass csi-hostpath-sc \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a pv-volume \u7684 pvc\uff0c\u5bb9\u91cf\u4e3a 10Mi \u521b\u5efa\u540d\u79f0\u4e3a web-server \u7684pod\uff0c\u5c06 nginx \u5bb9\u5668\u7684 /usr/share/nginx/html \u76ee\u5f55\u4f7f\u7528\u8be5 pvc \u6302\u8f7d \u5c06\u4e0a\u8ff0 pvc \u7684\u5927\u5c0f\u4ece 10Mi \u66f4\u65b0\u4e3a 70Mi\uff0c\u5e76\u8bb0\u5f55\u672c\u6b21\u53d8\u66f4 https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/ [ root@k8s-node1 ~ ] # cat create-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pv-volume spec: storageClassName: csi-hostpath-sc accessModes: - ReadWriteOnce resources: requests: storage: 3Mi [ root@k8s-node1 ~ ] # cat pvc-mount-pods.yaml apiVersion: v1 kind: Pod metadata: name: task-pv-pod spec: volumes: - name: my-pvc persistentVolumeClaim: claimName: pv-volume containers: - name: web-server image: nginx ports: - containerPort: 80 name: \"http-server\" volumeMounts: - mountPath: \"/usr/share/nginx/html\" name: my-pvc kubectl edit pvc pv-volume --record #--record\u662f\u8bb0\u5f55\u5728\u6ce8\u89e3\u4e2d \u9898\u76ee15 \u6dfb\u52a0 sidecar \u5bb9\u5668\u5e76\u8f93\u51fa\u65e5\u5fd7 \u6dfb\u52a0\u4e00\u4e2a sidecar \u5bb9\u5668 ( \u4f7f\u7528busybox \u955c\u50cf ) \u5230\u5df2\u6709\u7684 pod 11 - factor - app \u4e2d \u786e\u4fdd sidecar \u5bb9\u5668\u80fd\u591f\u8f93\u51fa / var / log / 11 - factor - app . log \u7684\u4fe1\u606f \u4f7f\u7528 volume \u6302\u8f7d / var / log \u76ee\u5f55 \uff0c \u786e\u4fdd sidecar \u80fd\u8bbf\u95ee 11 - factor - app . log \u6587\u4ef6 \u901a\u8fc7 kubectl get pod - o yaml \u7684\u65b9\u6cd5\u5907\u4efd\u539f\u59cb pod \u4fe1\u606f \uff0c \u5220\u9664\u65e7\u7684pod 11 - factor - app copy \u4e00\u4efd\u65b0 yaml \u6587\u4ef6 \uff0c \u6dfb\u52a0 \u4e00\u4e2a\u540d\u79f0\u4e3a sidecar \u7684\u5bb9\u5668 \u65b0\u5efa emptyDir \u7684\u5377 \uff0c \u786e\u4fdd\u4e24\u4e2a\u5bb9\u5668\u90fd\u6302\u8f7d\u4e86 / var / log \u76ee\u5f55 \u65b0\u5efa\u542b\u6709 sidecar \u7684 pod \uff0c \u5e76\u901a\u8fc7 kubectl logs \u9a8c\u8bc1 https : //kubernetes.io/zh/docs/concepts/cluster-administration/logging/ \u9898\u76ee16 \u8282\u70b9 wk8s-node-0 \u72b6\u6001\u4e3a NotReady\uff0c\u67e5\u770b\u539f\u56e0\u5e76\u6062\u590d\u5176\u72b6\u6001\u4e3a Ready \u786e\u4fdd\u64cd\u4f5c\u4e3a\u6301\u4e45\u7684 \u89e3\u6790\uff1a \u901a\u8fc7 get nodes \u67e5\u770b\u5f02\u5e38\u8282\u70b9\uff0c\u767b\u5f55\u8282\u70b9\u67e5\u770b kubelet \u7b49\u7ec4\u4ef6\u7684 status \u5e76\u5224\u65ad\u539f\u56e0 \u542f\u52a8 kubelet \u5e76 enable kubelet \u5373\u53ef kubectl get nodes ssh wk8s-node-0 sudo -i systemctl status kubelet systemctl enable kubelet systemctl restart kubelet systemctl status kubelet \u518d\u6b21 get nodes\uff0c \u786e\u4fdd\u8282\u70b9\u6062\u590d Ready \u72b6\u6001 \u9898\u76ee17 \u67e5\u627e label \u4e3a name = cpu - loader \u7684 pod , \u7b5b\u9009\u51fa cpu \u8d1f\u8f7d\u6700\u9ad8\u7684\u90a3\u4e2a pod \uff0c\u5e76\u5c06\u540d\u79f0 \u8ffd\u52a0 \u5230 / opt / KUTR00401 / KUTR00401 . txt \u89e3\u6790\uff1a \u4f7f\u7528 top\u547d\u4ee4 \uff0c\u7ed3\u5408 - l label_key = label_value \u548c -- sort = cpu \u8fc7\u6ee4\u51fa\u76ee\u6807\u5373\u53ef kubectl top pod - l name = cpu - loader - A -- sort - by = cpu echo podName >> / opt / KUTR00401 / KUTR00401 . txt","title":"CKA\u771f\u547d\u9898"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#_3","text":"\u6dfb\u52a0 sidecar \u5bb9\u5668\u5e76\u8f93\u51fa\u65e5\u5fd7 \u6dfb\u52a0\u4e00\u4e2a sidecar \u5bb9\u5668 ( \u4f7f\u7528busybox \u955c\u50cf ) \u5230\u5df2\u6709\u7684 pod 11 -factor-app \u4e2d \u786e\u4fdd sidecar \u5bb9\u5668\u80fd\u591f\u8f93\u51fa /var/log/11-factor-app.log \u7684\u4fe1\u606f \u4f7f\u7528 volume \u6302\u8f7d /var/log \u76ee\u5f55\uff0c\u786e\u4fdd sidecar \u80fd\u8bbf\u95ee 11 -factor-app.log \u6587\u4ef6 \u89e3\u7b54\uff1a https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/ \u5148\u7528kubectl get podname -o yaml > podname.yaml \u83b7\u53d6\u5230yaml\u6587\u4ef6\uff0c\u7136\u540e\u5220\u9664\u65e7\u7684 pod\uff1b \u518d\u91cd\u65b0 copy \u4e00\u4e2a\u65b0\u7684 yaml \u6dfb\u52a0 sidecar \u5bb9\u5668\uff0c\u5e76\u5728\u4e24\u4e2a\u5bb9\u5668\u4e2d\u90fd\u6302\u8f7d emtpyDir \u5230 /var/log\u76ee\u5f55\uff0c\u6700\u540e\u901a\u8fc7apply \u751f\u6210\u5e26 sidecar \u7684 pod\uff1b pod \u6b63\u5e38\u62c9\u8d77\u540e,\u901a\u8fc7 kubectl logs 11 -factor-app sidecar \u786e\u8ba4\u80fd\u6b63\u5e38\u8f93\u5165\u65e5\u5fd7\u5373\u53ef #\u5148\u5907\u4efd [ root@k8s-node1 ~ ] # kubectl get pods 11-factor-app -o yaml > 11-factor-app.ayml #\u7136\u540e\u5728\u5220\u9664\uff0c\u4f7f\u7528\u5b98\u65b9\u7684\u6a21\u677f\u8fdb\u884c\u4fee\u6539\u5373\u53ef\u3002 [ root@k8s-node1 ~ ] # more sidecar.yaml apiVersion: v1 kind: Pod metadata: name: 11 -factor-app spec: containers: - name: 11 -factor-app image: busybox args: - /bin/sh - -c - > i = 0 ; while true ; do echo \" $( date ) INFO $i \" >> /var/log/11-factor-app.log ; i = $(( i+1 )) ; sleep 1 ; done volumeMounts: - name: varlog mountPath: /var/log - name: sidecar image: busybox args: [ /bin/sh, -c, 'tail -n+1 -f /var/log/11-factor-app.log' ] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: {} \u521b\u5efa\u548c\u4f7f\u7528 PVC \u4f7f\u7528\u6307\u5b9a storageclass csi - hostpath - sc \u521b\u5efa\u4e00\u4e2a\u540d\u79f0\u4e3a pv - volume \u7684 pvc \uff0c\u5bb9\u91cf\u4e3a 10 Mi \u521b\u5efa\u540d\u79f0\u4e3a web - server \u7684 pod \uff0c\u5c06 nginx \u5bb9\u5668\u7684 /usr/s hare / nginx / html \u76ee\u5f55\u4f7f\u7528\u8be5 pvc \u6302\u8f7d \u5c06\u4e0a\u8ff0 pvc \u7684\u5927\u5c0f\u4ece 10 Mi \u66f4\u65b0\u4e3a 70 Mi \uff0c\u5e76\u8bb0\u5f55\u672c\u6b21\u53d8\u66f4 \u9996\u5148\u8981\u786e\u8ba4\u662f\u5426\u8fdb\u9519\u4e86 k8s\u96c6\u7fa4\u4e2d \uff0c\u7136\u540e\u5728\u67e5\u770b StorageClass \u662f\u5426\u6709\u5bf9\u5e94\u7684 [ root @k8s - node1 newpvc ] # more createpvc.yaml apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pv - volume - to2 spec : accessModes : - ReadWriteOnce volumeMode : Filesystem resources : requests : storage : 10 Mi storageClassName : cis - hostpath - cs - to2 [ root @k8s - node1 newpvc ] # kubectl get pvc #\u5982\u679c\u6ca1\u7ed1\u5b9a\u7684\u8bdd\uff0c\u770b\u4e0b NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pv - volume - to2 Bound task - pv - volume - to2 1 Gi RWO cis - hostpath - cs - to2 31 m [ root @k8s - node1 newpvc ] # more newpvcmount.yaml apiVersion : v1 kind : Pod metadata : name : web - server - to2 spec : containers : - name : nginx image : nginx volumeMounts : - mountPath : \"/usr/share/nginx/html\" name : mypd volumes : - name : mypd persistentVolumeClaim : claimName : pv - volume - to2 kubectl edit pvc pv - volume - to2 -- record #\u7531\u4e8enfs\u548c\u672c\u5730\u5377\u4e0d\u652f\u6301\u52a8\u6001\u8c03\u6574\uff0c\u6545\u5c31\u6ca1\u505a\u3002 \u6309\u8981\u6c42\u521b\u5efa Ingress \u8d44\u6e90 \u521b\u5efa\u4e00\u4e2a\u65b0\u7684 Ingress \u8d44\u6e90\uff0c\u540d\u79f0 ping\uff0c\u547d\u540d\u7a7a\u95f4 ing-internal \u4f7f\u7528 /hello \u8def\u5f84\u66b4\u9732\u670d\u52a1 hello \u7684 5678 \u7aef\u53e3\uff0c #\u89e3\u7b54\u521b\u5efaingress\u4e4b\u540e\u65e0\u6cd5\u83b7\u53d6ip\u5730\u5740","title":"\u9700\u8981\u91cd\u70b9\u6ce8\u610f\u7684\u9898"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/01-CAK%E8%AF%95%E9%A2%98%E6%94%B6%E9%9B%86%2001/#cka-k8s","text":"\u4f7f\u7528\u8bf4\u660e \u4f7f\u7528\u590d\u5236\u4e4b\u540e\u5199\u5165\u6587\u4ef6txt\u6587\u4ef6\u4e2d\uff0c\u540e\u7f00\u4fee\u6539\u6210html,\u7136\u540e\u5728\u5bfc\u5165\u4e66\u7b7e\u4e2d\u5373\u53ef \u89c1 https://www.jianshu.com/p/a743860b13fe \u53c2\u8003\u6587\u6863\uff1a https://www.cloudcared.cn/3138.html","title":"CKA\u8003\u8bd5 K8s \u5b98\u65b9\u9875\u7b7e"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/02-CKA%E5%A4%A7%E7%BA%B2/","text":"CKA \u5907\u8003\u5927\u7eb2 \u00b6 Kubernetes\u5165\u95e8 \u00b6 Kubernetes\u57fa\u672c\u6982\u5ff5\u4e0e\u5e94\u7528\u573a\u666f Kubernetes\u4e3b\u8981\u529f\u80fd\u7279\u6027\u3001\u96c6\u7fa4\u67b6\u6784\u4e0e\u7ec4\u4ef6 \u4f7f\u7528kubeadm\u5b89\u88c5\u96c6\u7fa4\u4e0e\u7248\u672c\u5347\u7ea7 etcd\u6570\u636e\u5907\u4efd\u4e0e\u8fd8\u539f kubectl\u4f7f\u7528\u3001shell\u81ea\u52a8\u8865 Kubernetes\u5de5\u4f5c\u8d1f\u8f7d\u3001\u8c03\u5ea6\u4e0eHelm \u00b6 Pod\u57fa\u672c\u64cd\u4f5c\u3001\u751f\u547d\u5468\u671f\u3001\u56de\u8c03\u4e0e\u63a2\u9488 \u521d\u59cb\u5316\u4e0e\u4e34\u65f6\u5bb9\u5668 \u4f7f\u7528Deployment\u90e8\u7f72\u81ea\u4fee\u590d\u65e0\u72b6\u6001\u670d\u52a1 \u4f7f\u7528Deployment\u6eda\u52a8\u66f4\u65b0/\u56de\u6eda/\u6269\u7f29\u65e0\u72b6\u6001\u670d\u52a1 \u4f7f\u7528StatefulSet\u90e8\u7f72\u6709\u72b6\u6001\u670d\u52a1 \u4f7f\u7528DaemonSet\u90e8\u7f72\u5b88\u62a4\u8fdb\u7a0b \u6df1\u5165\u7406\u89e3\u63a7\u5236\u5668\u5de5\u4f5c\u539f\u7406 \u4f7f\u7528ConfigMaps\u548cSecrets\u914d\u7f6e\u5e94\u7528\u7a0b\u5e8f Kubernetes\u8c03\u5ea6\u7b56\u7565\u5b9e\u8df5 \u8d44\u6e90\u9650\u5236\u5982\u4f55\u5f71\u54cdPod\u8c03\u5ea6 \u7406\u89e3\u8c03\u5ea6\u5668\u5de5\u4f5c\u539f\u7406 \u5404\u79cd\u8c03\u5ea6\u7b56\u7565\u4f7f\u7528\u573a\u666f\u603b\u7ed3 \u4f7f\u7528Helm\u90e8\u7f72/\u5347\u7ea7/\u56de\u6eda/\u4e0b\u7ebf\u670d\u52a1 Helm\u56de\u8c03\u4e0eChart\u7f16\u5199 Kubernetes\u670d\u52a1\u4e0e\u7f51\u7edc \u00b6 \u5b9a\u4e49Service\u4e0eEndpoint Service Iptables\u4e0eIPVS\u4ee3\u7406\u6a21\u5f0f \u901a\u8fc7Service\u540d\u79f0\u4e0eClusterIP\u96c6\u7fa4\u5185\u4e92\u8bbf \u901a\u8fc7NodePort\u3001Ingress\u3001LoadBalancer\u96c6\u7fa4\u5916\u8bbf\u95ee CoreDNS\u539f\u7406\u4ecb\u7ecd \u914d\u7f6e\u548c\u4f7f\u7528CoreDNS \u540cPod/\u540cNode/\u8de8Node/\u8de8\u96c6\u7fa4\u4e92\u901a\u6027 \u5e38\u89c1\u7f51\u7edc\u63a5\u53e3\u63d2\u4ef6\u5de5\u4f5c\u539f\u7406\u4e0e\u9002\u7528\u573a\u666f \u5e38\u89c1\u7f51\u7edc\u6545\u969c\u6392\u67e5 Kubernetes\u5b58\u50a8\u4e0e\u5b89\u5168 \u00b6 Volume\u3001PV\u3001PVC\u3001StorageClass \u5377\u6a21\u5f0f\u3001\u8bbf\u95ee\u6a21\u5f0f\u548c\u5377\u56de\u6536\u7b56\u7565 \u7406\u89e3\u6301\u4e45\u5bb9\u91cf\u58f0\u660e\u539f\u8bed \u4e86\u89e3\u5982\u4f55\u914d\u7f6e\u5177\u6709\u6301\u4e45\u6027\u5b58\u50a8\u7684\u5e94\u7528\u7a0b\u5e8f \u8ba4\u8bc1\u3001\u6388\u6743\u4e0e\u9274\u6743 \u7ba1\u7406\u57fa\u4e8e\u89d2\u8272\u7684\u8bbf\u95ee\u63a7\u5236\uff08RBAC\uff09 Pod\u548c\u5bb9\u5668\u64cd\u4f5c\u6743\u9650\u5b89\u5168\u7b56\u7565 Network Policy Kubernetes\u76d1\u63a7\u65e5\u5fd7\u3001\u6545\u969c\u6392\u67e5 \u00b6 \u5982\u4f55\u76d1\u63a7\u4e00\u4e2aKubenetes\u5e94\u7528 \u67e5\u770b\u4e0e\u7ba1\u7406\u96c6\u7fa4\u548c\u8282\u70b9\u65e5\u5fd7 \u7ba1\u7406\u5bb9\u5668\u6807\u51c6\u8f93\u51fa\u548c\u6807\u51c6\u9519\u8bef\u65e5\u5fd7 \u5982\u4f55\u89e3\u51b3\u5e94\u7528\u7a0b\u5e8f\u6545\u969c \u5bf9\u7fa4\u96c6\u7ec4\u4ef6\u6545\u969c\u8fdb\u884c\u6545\u969c\u6392\u9664 Kubernetes\u5176\u4ed6\u5e38\u89c1\u95ee\u9898\u5b9a\u4f4d","title":"CKA \u5907\u8003\u5927\u7eb2"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/02-CKA%E5%A4%A7%E7%BA%B2/#cka","text":"","title":"CKA \u5907\u8003\u5927\u7eb2"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/02-CKA%E5%A4%A7%E7%BA%B2/#kubernetes","text":"Kubernetes\u57fa\u672c\u6982\u5ff5\u4e0e\u5e94\u7528\u573a\u666f Kubernetes\u4e3b\u8981\u529f\u80fd\u7279\u6027\u3001\u96c6\u7fa4\u67b6\u6784\u4e0e\u7ec4\u4ef6 \u4f7f\u7528kubeadm\u5b89\u88c5\u96c6\u7fa4\u4e0e\u7248\u672c\u5347\u7ea7 etcd\u6570\u636e\u5907\u4efd\u4e0e\u8fd8\u539f kubectl\u4f7f\u7528\u3001shell\u81ea\u52a8\u8865","title":"Kubernetes\u5165\u95e8"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/02-CKA%E5%A4%A7%E7%BA%B2/#kuberneteshelm","text":"Pod\u57fa\u672c\u64cd\u4f5c\u3001\u751f\u547d\u5468\u671f\u3001\u56de\u8c03\u4e0e\u63a2\u9488 \u521d\u59cb\u5316\u4e0e\u4e34\u65f6\u5bb9\u5668 \u4f7f\u7528Deployment\u90e8\u7f72\u81ea\u4fee\u590d\u65e0\u72b6\u6001\u670d\u52a1 \u4f7f\u7528Deployment\u6eda\u52a8\u66f4\u65b0/\u56de\u6eda/\u6269\u7f29\u65e0\u72b6\u6001\u670d\u52a1 \u4f7f\u7528StatefulSet\u90e8\u7f72\u6709\u72b6\u6001\u670d\u52a1 \u4f7f\u7528DaemonSet\u90e8\u7f72\u5b88\u62a4\u8fdb\u7a0b \u6df1\u5165\u7406\u89e3\u63a7\u5236\u5668\u5de5\u4f5c\u539f\u7406 \u4f7f\u7528ConfigMaps\u548cSecrets\u914d\u7f6e\u5e94\u7528\u7a0b\u5e8f Kubernetes\u8c03\u5ea6\u7b56\u7565\u5b9e\u8df5 \u8d44\u6e90\u9650\u5236\u5982\u4f55\u5f71\u54cdPod\u8c03\u5ea6 \u7406\u89e3\u8c03\u5ea6\u5668\u5de5\u4f5c\u539f\u7406 \u5404\u79cd\u8c03\u5ea6\u7b56\u7565\u4f7f\u7528\u573a\u666f\u603b\u7ed3 \u4f7f\u7528Helm\u90e8\u7f72/\u5347\u7ea7/\u56de\u6eda/\u4e0b\u7ebf\u670d\u52a1 Helm\u56de\u8c03\u4e0eChart\u7f16\u5199","title":"Kubernetes\u5de5\u4f5c\u8d1f\u8f7d\u3001\u8c03\u5ea6\u4e0eHelm"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/02-CKA%E5%A4%A7%E7%BA%B2/#kubernetes_1","text":"\u5b9a\u4e49Service\u4e0eEndpoint Service Iptables\u4e0eIPVS\u4ee3\u7406\u6a21\u5f0f \u901a\u8fc7Service\u540d\u79f0\u4e0eClusterIP\u96c6\u7fa4\u5185\u4e92\u8bbf \u901a\u8fc7NodePort\u3001Ingress\u3001LoadBalancer\u96c6\u7fa4\u5916\u8bbf\u95ee CoreDNS\u539f\u7406\u4ecb\u7ecd \u914d\u7f6e\u548c\u4f7f\u7528CoreDNS \u540cPod/\u540cNode/\u8de8Node/\u8de8\u96c6\u7fa4\u4e92\u901a\u6027 \u5e38\u89c1\u7f51\u7edc\u63a5\u53e3\u63d2\u4ef6\u5de5\u4f5c\u539f\u7406\u4e0e\u9002\u7528\u573a\u666f \u5e38\u89c1\u7f51\u7edc\u6545\u969c\u6392\u67e5","title":"Kubernetes\u670d\u52a1\u4e0e\u7f51\u7edc"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/02-CKA%E5%A4%A7%E7%BA%B2/#kubernetes_2","text":"Volume\u3001PV\u3001PVC\u3001StorageClass \u5377\u6a21\u5f0f\u3001\u8bbf\u95ee\u6a21\u5f0f\u548c\u5377\u56de\u6536\u7b56\u7565 \u7406\u89e3\u6301\u4e45\u5bb9\u91cf\u58f0\u660e\u539f\u8bed \u4e86\u89e3\u5982\u4f55\u914d\u7f6e\u5177\u6709\u6301\u4e45\u6027\u5b58\u50a8\u7684\u5e94\u7528\u7a0b\u5e8f \u8ba4\u8bc1\u3001\u6388\u6743\u4e0e\u9274\u6743 \u7ba1\u7406\u57fa\u4e8e\u89d2\u8272\u7684\u8bbf\u95ee\u63a7\u5236\uff08RBAC\uff09 Pod\u548c\u5bb9\u5668\u64cd\u4f5c\u6743\u9650\u5b89\u5168\u7b56\u7565 Network Policy","title":"Kubernetes\u5b58\u50a8\u4e0e\u5b89\u5168"},{"location":"support/CKA%20%E8%80%83%E8%AF%95/02-CKA%E5%A4%A7%E7%BA%B2/#kubernetes_3","text":"\u5982\u4f55\u76d1\u63a7\u4e00\u4e2aKubenetes\u5e94\u7528 \u67e5\u770b\u4e0e\u7ba1\u7406\u96c6\u7fa4\u548c\u8282\u70b9\u65e5\u5fd7 \u7ba1\u7406\u5bb9\u5668\u6807\u51c6\u8f93\u51fa\u548c\u6807\u51c6\u9519\u8bef\u65e5\u5fd7 \u5982\u4f55\u89e3\u51b3\u5e94\u7528\u7a0b\u5e8f\u6545\u969c \u5bf9\u7fa4\u96c6\u7ec4\u4ef6\u6545\u969c\u8fdb\u884c\u6545\u969c\u6392\u9664 Kubernetes\u5176\u4ed6\u5e38\u89c1\u95ee\u9898\u5b9a\u4f4d","title":"Kubernetes\u76d1\u63a7\u65e5\u5fd7\u3001\u6545\u969c\u6392\u67e5"},{"location":"survey/","text":"\u5e02\u573a\u8c03\u7814 \u00b6 \u8c03\u7814\u6c47\u603b\uff1ahttps://dwiki.daocloud.io/pages/viewpage.action?pageId=87828082","title":"\u5e02\u573a\u8c03\u7814"},{"location":"survey/#_1","text":"\u8c03\u7814\u6c47\u603b\uff1ahttps://dwiki.daocloud.io/pages/viewpage.action?pageId=87828082","title":"\u5e02\u573a\u8c03\u7814"},{"location":"survey/cce/","text":"CCE \u00b6 https://dwiki.daocloud.io/pages/viewpage.action?pageId=89577648","title":"CCE"},{"location":"survey/cce/#cce","text":"https://dwiki.daocloud.io/pages/viewpage.action?pageId=89577648","title":"CCE"},{"location":"survey/cilium/","text":"Cilium \u00b6","title":"Cilium"},{"location":"survey/cilium/#cilium","text":"","title":"Cilium"},{"location":"survey/harbor/","text":"Harbor \u00b6 \u955c\u50cf\u4ed3\u5e93 harbor \u8c03\u7814\uff1a https://dwiki.daocloud.io/pages/viewpage.action?pageId=87842865","title":"Harbor"},{"location":"survey/harbor/#harbor","text":"\u955c\u50cf\u4ed3\u5e93 harbor \u8c03\u7814\uff1a https://dwiki.daocloud.io/pages/viewpage.action?pageId=87842865","title":"Harbor"},{"location":"survey/karmada/","text":"Karmada \u00b6 \u591a\u4e91\u7ba1\u7406\u3002 https://dwiki.daocloud.io/display/Enterprise/karmada","title":"Karmada"},{"location":"survey/karmada/#karmada","text":"\u591a\u4e91\u7ba1\u7406\u3002 https://dwiki.daocloud.io/display/Enterprise/karmada","title":"Karmada"},{"location":"survey/kubesphere/","text":"KubeSphere \u00b6 https://dwiki.daocloud.io/pages/viewpage.action?pageId=113380566","title":"KubeSphere"},{"location":"survey/kubesphere/#kubesphere","text":"https://dwiki.daocloud.io/pages/viewpage.action?pageId=113380566","title":"KubeSphere"},{"location":"survey/rancher/","text":"Rancher \u00b6 https://dwiki.daocloud.io/pages/viewpage.action?pageId=89556983","title":"Rancher"},{"location":"survey/rancher/#rancher","text":"https://dwiki.daocloud.io/pages/viewpage.action?pageId=89556983","title":"Rancher"},{"location":"survey/storage/","text":"\u5b58\u50a8\u8c03\u7814 \u00b6","title":"\u5b58\u50a8\u8c03\u7814"},{"location":"survey/storage/#_1","text":"","title":"\u5b58\u50a8\u8c03\u7814"},{"location":"survey/tanzu/","text":"Tanzu \u00b6 https://dwiki.daocloud.io/pages/viewpage.action?pageId=86277083","title":"Tanzu"},{"location":"survey/tanzu/#tanzu","text":"https://dwiki.daocloud.io/pages/viewpage.action?pageId=86277083","title":"Tanzu"},{"location":"scaffolds/tags/","text":"Tags \u00b6 Following is a list of relevant tags: HowTo \u00b6 \u6587\u6863\u7ad9\u4f7f\u7528\u8bf4\u660e","title":"Tags"},{"location":"scaffolds/tags/#tags","text":"Following is a list of relevant tags:","title":"Tags"},{"location":"scaffolds/tags/#howto","text":"\u6587\u6863\u7ad9\u4f7f\u7528\u8bf4\u660e","title":"HowTo"}]}